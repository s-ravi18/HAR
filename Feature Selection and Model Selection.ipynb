{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various Feature selection methods that can be used:\n",
    "1. ANOVA - FILTER Method of FS\n",
    "2. CHI2 - FILTER Method of FS\n",
    "3. RFE - WRAPPER Method of FS\n",
    "4. Mutual Info - FILTER Method of FS\n",
    "\n",
    "There are other methods of FS like:\n",
    "1. Feature Importance of individual models - EMBEDDED Method of FS\n",
    "2. Pearson's Correlation\n",
    "3. Spearman's Rank Correlation\n",
    "4. PCA - FILTER Method of FS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Combining individual zip files:\n",
    "## Used Dataset:after-tsfresh-imu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining all the files:\n",
    "dir=\"../input/after-tsfresh-imu\"\n",
    "\n",
    "### Combining all the EMG Files\n",
    "emg=pd.DataFrame()\n",
    "for i in os.listdir(dir):\n",
    "    for j in os.listdir(os.path.join(dir,i)):\n",
    "        if \"_EMG_\" in j:\n",
    "            emg=pd.concat([emg,pd.read_csv(os.path.join(dir,i,j))],axis=0)\n",
    "\n",
    "### Combining all the Non-EMG Files            \n",
    "nemg=pd.DataFrame()\n",
    "for i in os.listdir(dir):\n",
    "    for j in os.listdir(os.path.join(dir,i)):\n",
    "        if \"_NonEMG_\" in j:\n",
    "            nemg=pd.concat([nemg,pd.read_csv(os.path.join(dir,i,j))],axis=0)\n",
    "            \n",
    "emg.to_csv(\"Combined_EMG.csv\",index=False)\n",
    "nemg.to_csv(\"Combined_NEMG.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created a dataset called imu-for-feature-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../input/imu-for-feature-selection/Combined_EMG.csv\")\n",
    "imu=pd.read_csv(\"../input/imu-for-feature-selection/Combined_NEMG.csv\")\n",
    "\n",
    "def initial(df):\n",
    "    \n",
    "    #### Label Encoding the Target Variable\n",
    "\n",
    "    X=df.drop([\"label\"],axis=1)\n",
    "    y=df[\"label\"]\n",
    "    if df.label.dtype==str:   ### Will apply label encoding if needed\n",
    "        le=LabelEncoder()\n",
    "        y=le.fit_transform(y)\n",
    "        y=pd.Series(y)\n",
    "\n",
    "    #### Removing Features having zero variance.\n",
    "\n",
    "    Var=X[X.columns].std()\n",
    "    col=Var[Var==0].index\n",
    "    X=X.drop(col,axis=1)\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_curve(train_sizes, train_scores, test_scores):\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # box-like grid\n",
    "    plt.grid()\n",
    "    \n",
    "    # plot the std deviation as a transparent range at each training set size\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    # plot the average training and test score lines at each training set size\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    # sizes the window for readability and displays the plot\n",
    "    # shows error from 0 to 1.1\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.ylim(-.1,1.1)\n",
    "    plt.show()\n",
    "       \n",
    "\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "\n",
    "    print(classification_report(y_true, y_pred)) # print classification report\n",
    "    cm=confusion_matrix(y_true,y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # For normalising the Matrix for better visualisation.\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.rc(\"font\",size=10)\n",
    "    sns.heatmap(cm,annot=True,fmt=\".2f\",cmap=\"viridis\")\n",
    "    plt.show()\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score\n",
    "\n",
    "def fun_best(X,y):\n",
    "    \n",
    "    X.rename({\"emg6__value_count__value_-1\":\"emg6__value_count__value_2\"},axis=1,inplace=True)\n",
    "\n",
    "    #To remove JSON characters from column names because LGBM fails to execute    \n",
    "    X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    \n",
    "    models={\"XGB\":XGBClassifier(),\"LGBM\":LGBMClassifier(),\"GradientBoost\":GradientBoostingClassifier(),\"LDA\":LinearDiscriminantAnalysis(),\"RandomForest\":RandomForestClassifier()}\n",
    "    mean_score=[]\n",
    "    \n",
    "    for i,j in models.items():\n",
    "        try:\n",
    "            \n",
    "            model=j\n",
    "            score_model=cross_val_score(model,X,y,cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "            mean_score.append(score_model.mean())\n",
    "            train_sizes, train_scores, test_scores = learning_curve(model, X, y, n_jobs=-1, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
    "            draw_curve(train_sizes, train_scores, test_scores)\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            model=j\n",
    "            score_model=cross_val_score(model,X,y,cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "            mean_score.append(score_model.mean())\n",
    "            train_sizes, train_scores, test_scores = learning_curve(model, X, y, n_jobs=-1, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
    "            draw_curve(train_sizes, train_scores, test_scores)            \n",
    "        \n",
    "    result=dict(zip(models.keys(),mean_score))\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for Implementing Various Feature Selection Methods\n",
    "def funct_FS_bestmodel(ds,fs_name,n):\n",
    "    \n",
    "    df=pd.read_csv(f\"../input/imu-for-feature-selection/Combined_{ds}.csv\")\n",
    "    \n",
    "    print(\"Read the Dataset\")\n",
    "    \n",
    "    X,y=initial(df)\n",
    "\n",
    "    print(\"Passed initial function\")\n",
    "    \n",
    "    fs_dic={\"MI\":SelectKBest(mutual_info_classif, k=n),\"CHI2\":SelectKBest(chi2, k=n),\"ANOVA\":SelectKBest(f_classif, k=n),\"RFE\":RFE(estimator=CatBoostClassifier(), n_features_to_select=n)}\n",
    "    \n",
    "    ### for Chi2 feature selection, data points must be strictly positive.\n",
    "    \n",
    "    if fs_name==\"CHI2\":\n",
    "        pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', fs_dic[fs_name])])\n",
    "    \n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('selector', fs_dic[fs_name])])\n",
    "    \n",
    "    print(\"Pipeline set up\")\n",
    "    \n",
    "    pipe.fit(X, y)\n",
    "    # Get columns to keep and create new dataframe with those only\n",
    "    cols = pipe.named_steps['selector'].get_support(indices=True) ### Note the format\n",
    "    X_fs= X.iloc[:,cols]\n",
    "\n",
    "    df_fs=pd.concat([X_fs,y],axis=1)\n",
    "    df_fs.rename({\"0\":\"label\"},axis=1,inplace=True)\n",
    "    df_fs.to_csv(f\"{ds}_{fs_name}_{n}_features.csv\",index=False)\n",
    "    \n",
    "    print(\"Done Feature Selection\")\n",
    "    \n",
    "    result=fun_best(X_fs,y)\n",
    "    print(f\"Top {n} features using {fs_name} technique:\")\n",
    "    print(result)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To check the Corresponding Letter -> Number Encoding\n",
    "target=pd.DataFrame(np.hstack([y.values.reshape((-1,1)),y_label.reshape((-1,1))]),columns=[\"label\",\"label_encoded\"])\n",
    "target.drop_duplicates().sort_values(\"label\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For Trial:\n",
    "### Params are (\"EMG\" or \"NEMG\"/ Selection Method Name / Number of features)\n",
    "funct_FS_bestmodel(\"EMG\",\"CHI2\",200) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3eff57f1b29dfdd788faddb90ebd9222db114d636e87b8fde724656933d5975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
