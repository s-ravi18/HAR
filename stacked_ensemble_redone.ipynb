{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##TSFresh\n",
    "import tsfresh\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "##ML scikit learn classes for data preprocessing:\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##ML scikit learn classes for feature selection:\n",
    "from sklearn.feature_selection import chi2,mutual_info_classif  ### for chi2 and mutual info\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif  ### for ANOVA\n",
    "from sklearn.feature_selection import RFE  ### for RFE\n",
    "\n",
    "##ML scikit learn classes for model selection:  \n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "##ML scikit learn classes for evaluating model:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report,accuracy_score,make_scorer,confusion_matrix,precision_recall_fscore_support\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "##ML scikit learn classes for creating Pipeline:\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "## Autokeras Library:\n",
    "import autokeras as ak\n",
    "from autokeras import StructuredDataClassifier\n",
    "\n",
    "## Deep Learning Libraries:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model,Model\n",
    "from tensorflow.keras.layers import concatenate,Input,Dense,ReLU,BatchNormalization,Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Ensemble Redone::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have used the dataset which corresponds to Top 200 Features selected on the basis of ANOVA for EMG and IMU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data:\n",
    "- This time the data was split into train and test (test_size=0.2)  {Two ways were tried, Random Sampling and Stratified Sampling}\n",
    "- Standardised the test data on the basis of train data \n",
    "- Saved the UnScaled train data and the Scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label Encoding, Removing Zero variance Features and Scaling the test data::\n",
    "def initial(df):\n",
    "    \n",
    "    #### Label Encoding the Target Variable\n",
    "\n",
    "    X=df.drop([\"label\"],axis=1)\n",
    "    y=df[\"label\"]\n",
    "    if df.label.dtype==str:   ### Will apply label encoding if needed\n",
    "        le=LabelEncoder()\n",
    "        y=le.fit_transform(y)\n",
    "        y=pd.Series(y)\n",
    "\n",
    "    #### Removing Features having zero variance.\n",
    "\n",
    "    Var=X[X.columns].std()\n",
    "    col=Var[Var==0].index\n",
    "    X=X.drop(col,axis=1)\n",
    "    \n",
    "    X, X_val, y, y_val = train_test_split(X, y,stratify=y,test_size=0.80, random_state=None)  ### For stratified sampling\n",
    "    #X, X_val, y, y_val = train_test_split(X, y,stratify=y,test_size=0.80, random_state=None)  ### For random sampling\n",
    "    \n",
    "    ### Scaling the Data:\n",
    "    sc=StandardScaler()\n",
    "    X_sc=sc.fit_transform(X)\n",
    "    X_val=sc.transform(X_val)  \n",
    "    \n",
    "    return X.reset_index(drop=True), pd.DataFrame(X_val,columns=X.columns), y.reset_index(drop=True), y_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For the best model using EMG and NEMG Features (EMG+NEMG Combined)\n",
    "df_emg=pd.read_csv(\"EMG_ANOVA_200_features.csv\").rename({'0':'label'},axis=1)\n",
    "df_nemg=pd.read_csv(\"NEMG_ANOVA_200_features.csv\").rename({\"0\":\"label\"},axis=1)\n",
    "\n",
    "temp=pd.DataFrame()\n",
    "for i in range(26):\n",
    "    n=df_nemg[df_nemg[\"label\"]==i].reset_index(drop=True)\n",
    "    e=df_emg[df_emg[\"label\"]==i].reset_index(drop=True).drop([\"label\"],axis=1)[0:n.shape[0]]\n",
    "    k=pd.concat([e,n],axis=1)\n",
    "    temp=pd.concat([temp,k],axis=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_val, y, y_val=initial(temp)   ### Remains Fixed\n",
    "\n",
    "## X,y ----> Training data\n",
    "## X_val,y_val ----> Testing data   Which is being stored for training and evaluating the other level 0 models\n",
    "X.shape,X_val.shape\n",
    "# ((4736, 400), (18944, 400))\n",
    "y.shape,y_val.shape\n",
    "# ((4736,), (18944,))\n",
    "\n",
    "\n",
    "train=pd.concat([X,y],axis=1)\n",
    "test=pd.concat([X_val,y_val],axis=1)\n",
    "\n",
    "train.shape,test.shape\n",
    "# ((4736, 401), (18944, 401))\n",
    "\n",
    "test.to_csv(\"Test_data_str.csv\",index=False)    ### Scaled Test Data\n",
    "train.to_csv(\"Train_data_str.csv\",index=False)  ### Unscaled Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Reading the saved Unscaled training data and used autokeras with max_iterations=28 to reach to the best model::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label Encoding and Removing Zero variance Features\n",
    "def initial1(df):\n",
    "    \n",
    "    #### Label Encoding the Target Variable\n",
    "\n",
    "    X=df.drop([\"label\"],axis=1)\n",
    "    y=df[\"label\"]\n",
    "    if df.label.dtype==str:   ### Will apply label encoding if needed\n",
    "        le=LabelEncoder()\n",
    "        y=le.fit_transform(y)\n",
    "        y=pd.Series(y)\n",
    "\n",
    "    #### Removing Features having zero variance.\n",
    "\n",
    "    Var=X[X.columns].std()\n",
    "    col=Var[Var==0].index\n",
    "    X=X.drop(col,axis=1)\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to find out best model using autokeras library.\n",
    "\n",
    "Parameters description: feature_count - no of features to be selected\n",
    "\n",
    "                    max_trials - for the autokeras\n",
    "                        \n",
    "                    data -  EMG/NEMG                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funct_autokeras_accuracy(X_train, X_test, y_train, y_test,max_trials,data):\n",
    "    \n",
    "    search = StructuredDataClassifier(max_trials=max_trials)\n",
    "    \n",
    "    # perform the search\n",
    "    search.fit(x=X_train, y=y_train, verbose=0)\n",
    "    \n",
    "    # evaluate the model\n",
    "    loss, acc = search.evaluate(X_test, y_test, verbose=0)\n",
    "    accuracy[f\"{data}_{max_trials}\"]=round(acc, 3)    \n",
    "    \n",
    "    # get the best performing model\n",
    "    model = search.export_model()   \n",
    "    \n",
    "    # save the best performing model to file\n",
    "    try:\n",
    "        model.save(f\"{data}_{max_trials}_strat\", save_format=\"tf\")\n",
    "    except Exception:\n",
    "        model.save(f\"{data}_{max_trials}_strat.h5\")\n",
    "\n",
    "\n",
    "def funct_autokeras(max_trials,data,X,y):    ### data=\"EMG\"/\"NEMG\"/\"Both\" ### feature_count=NULL in case of \"Both\"\n",
    "    X=X\n",
    "    y=y\n",
    "\n",
    "    ### Splitting the data:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.3, random_state=1)\n",
    "    \n",
    "    ### Scaling the Data:\n",
    "    sc=StandardScaler()\n",
    "    X_train_scaled=sc.fit_transform(X_train)\n",
    "    X_test_scaled=sc.transform(X_test)    \n",
    "    \n",
    "    funct_autokeras_accuracy(X_train_scaled, X_test_scaled, y_train, y_test,max_trials,data)\n",
    "\n",
    "\n",
    "accuracy={}\n",
    "df=pd.read_csv(\"Train_data_random.csv\").rename({'0':'label'},axis=1)\n",
    "X,y=initial1(df)   ### Remains Fixed\n",
    "### Iterating through various values:\n",
    "funct_autokeras(28,\"Both\",X,y) \n",
    "    \n",
    "with open('accuracy_results_stacked_28_strat.json', 'w') as fp:\n",
    "    json.dump(accuracy, fp,  indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Architecture of Best Model on Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = load_model(\"Both_28_strat\", custom_objects=ak.CUSTOM_OBJECTS)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,number,l1,l2):\n",
    "        self.number=number\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(tf.keras.layers.InputLayer(input_shape=(400)))\n",
    "        self.model.add(tf.keras.layers.Normalization(axis=-1))\n",
    "        self.model.add(tf.keras.layers.Dense(units=l1,activation=None))   ###\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "        self.model.add(tf.keras.layers.ReLU())\n",
    "        self.model.add(tf.keras.layers.Dropout(0.2))\n",
    "        self.model.add(tf.keras.layers.Dense(units=l2,activation=None))  ###\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "        self.model.add(tf.keras.layers.ReLU())\n",
    "        self.model.add(tf.keras.layers.Dropout(0.2))\n",
    "        #self.model.add(tf.keras.layers.Dropout(0.3))\n",
    "        self.model.add(tf.keras.layers.Dense(units=26,activation=\"softmax\"))\n",
    "        self.model.compile(optimizer=\"adam\",loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=\"accuracy\")\n",
    "    \n",
    "    def funct_fit(self,X_train,X_test,y_train,y_test,count,n_split):\n",
    "        self.model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=50,batch_size=50)\n",
    "        if count==n_split:\n",
    "            self.model.save(f\"{os.getcwd()}/model_{self.number}.h5\")\n",
    "        return self.model.history.history[\"accuracy\"],self.model.history.history[\"val_accuracy\"] \n",
    "\n",
    "def funct_avg(d):\n",
    "    temp={}\n",
    "    for i,j in d.items():\n",
    "        temp[i]=np.mean(j)\n",
    "        \n",
    "    return temp\n",
    "\n",
    "### cross validation:\n",
    "def funct_cv(m,n_split,X,y,l1,l2):\n",
    "    dic_results={\"accuracy\":[],\"val_accuracy\":[]}\n",
    "    n_split=n_split\n",
    "    count=0\n",
    "    for train_index,test_index in StratifiedKFold(n_split).split(X,y):\n",
    "        X_train,X_test=X[train_index],X[test_index]\n",
    "        y_train,y_test=y[train_index],y[test_index]\n",
    "\n",
    "        sc=StandardScaler()\n",
    "        X_train_scaled=sc.fit_transform(X_train)\n",
    "        X_test_scaled=sc.transform(X_test)  \n",
    "\n",
    "        model=Model(m,l1,l2)   ### creating the model object \n",
    "        count=count+1\n",
    "        acc,val_acc=model.funct_fit(X_train_scaled,X_test_scaled, y_train,y_test,count,n_split)        \n",
    "\n",
    "        dic_results[\"accuracy\"].append(max(acc))\n",
    "        dic_results[\"val_accuracy\"].append(max(val_acc))\n",
    "\n",
    "    dic_results=funct_avg(dic_results)\n",
    "\n",
    "    final_result[m]=dic_results\n",
    "\n",
    "final_result={}\n",
    "funct_cv(1,5,X.values,y.values,60,60)\n",
    "funct_cv(2,5,X.values,y.values,90,90)\n",
    "funct_cv(3,5,X.values,y.values,150,150)\n",
    "\n",
    "with open('accuracy_results_StratifiedCV.json', 'w') as fp:\n",
    "    json.dump(final_result, fp,  indent=4)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Base Models for final Prediction::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmodels=[base_model]\n",
    "\n",
    "#### Loading other models:\n",
    "def funct_load(number):\n",
    "    for i in range(1,number+1):\n",
    "        # load model from file\n",
    "        model = load_model(f'model_{i}.h5')\n",
    "        model.summary()\n",
    "        # add to list of members\n",
    "        allmodels.append(model)\n",
    "            \n",
    "funct_load(3)    \n",
    "\n",
    "df=pd.read_csv(\"/home/sudharshan/Sudharshan/Test_data_str.csv\").rename({'0':'label'},axis=1)\n",
    "X_test,y_test=initial1(df)   ### Remains Fixed\n",
    "\n",
    "\n",
    "# create stacked model input dataset as outputs from the individual ensemble models\n",
    "def stacked_dataset(allmodels, X_test):\n",
    "    stackX = None\n",
    "    for model in allmodels:\n",
    "        # make prediction\n",
    "        yhat = model.predict(X_test, verbose=0)\n",
    "        # stack predictions into [rows, members, class]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = np.dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members x class]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    return stackX\n",
    "\n",
    "stackedX = stacked_dataset(allmodels, X_test)\n",
    "\n",
    "results={}\n",
    "\n",
    "# evaluate standalone models on test dataset\n",
    "for model in allmodels:\n",
    "    y_hat=model.predict(X_test)\n",
    "    y_hat=np.argmax(y_hat,axis=1)\n",
    "    acc = accuracy_score(y_test, y_hat)\n",
    "    print('Model Accuracy: %.6f' % acc)\n",
    "\n",
    "\n",
    "def funct_report_csv(y,y_hat):\n",
    "    clf_rep = precision_recall_fscore_support(y, y_hat)\n",
    "    out_dict = {\n",
    "                 \"precision\" :clf_rep[0].round(2)\n",
    "                ,\"recall\" : clf_rep[1].round(2)\n",
    "                ,\"f1-score\" : clf_rep[2].round(2)\n",
    "                ,\"support\" : clf_rep[3]\n",
    "                }\n",
    "    out_df = pd.DataFrame(out_dict)\n",
    "    avg_tot = (out_df.apply(lambda x: round(x.mean(), 2) if x.name!=\"support\" else  round(x.sum(), 2)).to_frame().T)\n",
    "    avg_tot.index = [\"avg/total\"]\n",
    "    out_df = out_df.append(avg_tot)\n",
    "    return out_df\n",
    "\n",
    "def classification_report_with_accuracy_score(y, y_hat,model_name):\n",
    "    #report=classification_report(y, y_hat,output_dict=True) # print classification report\n",
    "    report=funct_report_csv(y, y_hat) # print classification report\n",
    "    report.to_csv(f\"Classification_Report_{model_name}.csv\",index=False)\n",
    "\n",
    "    cm=confusion_matrix(y,y_hat)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.rc(\"font\",size=10)\n",
    "    sns.heatmap(cm,annot=True,fmt=\".2f\",cmap=\"viridis\")\n",
    "    plt.savefig(f\"Confusion_Matrix_{model_name}_simple.png\")\n",
    "\n",
    "    cm_1 = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # For normalising the Matrix for better visualisation.\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.rc(\"font\",size=10)\n",
    "    sns.heatmap(cm_1,annot=True,fmt=\".2f\",cmap=\"viridis\")\n",
    "    plt.savefig(f\"Confusion_Matrix_{model_name}_axis=1.png\")\n",
    "    \n",
    "    return accuracy_score(y, y_hat) # return accuracy score\n",
    "\n",
    "    \n",
    "def stacked_model_test(allmodels,X,y):\n",
    "    # dictionary of all models\n",
    "    sample={\"XGB\":XGBClassifier(),\"LGBM\":LGBMClassifier(),\"RandomForest\":RandomForestClassifier(),\"LR\":LogisticRegression(),\"CatBoost\":CatBoostClassifier(),\"Naive Bayes\":GaussianNB(),\"SVC\":SVC(),\"KNN_3\":KNeighborsClassifier(n_neighbors=3),\"KNN_5\":KNeighborsClassifier(n_neighbors=5)}\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(allmodels, X)\n",
    "    \n",
    "    for i,j in sample.items():\n",
    "        arg={\"model_name\":i}\n",
    "        sc=make_scorer(classification_report_with_accuracy_score,**arg)\n",
    "        score_=cross_val_score(j,stackedX,y,cv=5,scoring=sc)\n",
    "        acc = score_.mean()\n",
    "       \n",
    "        results[i]=round(acc,5)\n",
    "           \n",
    "\n",
    "stacked_model_test(allmodels,X_test,y_test)\n",
    "#### To store the results in the form of json that is prettified:\n",
    "with open('accuracy_results_final_stacked_random-sampling.json', 'w') as fp:\n",
    "    json.dump(results, fp,  indent=4)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3eff57f1b29dfdd788faddb90ebd9222db114d636e87b8fde724656933d5975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
