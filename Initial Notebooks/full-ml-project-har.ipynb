{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing:","metadata":{}},{"cell_type":"markdown","source":"### Done on Local Jupyter Notebook","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\n\nimport re\n\nalpha=[\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n\nextension = 'csv'\n\n# Set 008 and 009\n\nfor n in [\"08\",\"09\"]:\n    for i in alpha:\n        os.chdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right\")\n    \n        all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n        \n        if len(all_filenames)==0:\n            continue\n        \n        elif len(all_filenames)>5:\n            a1=[i for i in all_filenames if \"accelerometer\" in i ]  \n            a2=[i for i in all_filenames if \"emg\" in i ]\n            a3=[i for i in all_filenames if \"gyro\" in i ]\n            a4=re.findall(r\"orientation-\\S+\",\" \".join(all_filenames))\n            a5=re.findall(r\"orientation\\w+\\S+\",\" \".join(all_filenames))\n            \n            combined_csv_a1=pd.concat([pd.read_csv(f) for f in a1],axis=0)\n            combined_csv_a2=pd.concat([pd.read_csv(f) for f in a2],axis=0)\n            combined_csv_a3=pd.concat([pd.read_csv(f) for f in a3],axis=0)\n            combined_csv_a4=pd.concat([pd.read_csv(f) for f in a4],axis=0)\n            combined_csv_a5=pd.concat([pd.read_csv(f) for f in a5],axis=0)\n            \n            #export combined csv of each feature:\n            \n            combined_csv_a1.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right/\" + f\"combined_csv_a1.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a2.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right/\" + f\"combined_csv_emg.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a3.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right/\" + f\"combined_csv_a3.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a4.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right/\" + f\"combined_csv_a4.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a5.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right/\" + f\"combined_csv_a5.csv\", index=False, encoding='utf-8-sig')\n            \n            #export combined csv of each feature:\n            \n            all_filenames = [\"combined_csv_a1.csv\",\"combined_csv_a3.csv\",\"combined_csv_a4.csv\",\"combined_csv_a5.csv\"]\n    \n            #combine all files in the list\n            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in all_filenames],axis=1)\n            combined_csv_a2=pd.read_csv(\"combined_csv_emg.csv\")\n                        \n            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n    \n            #export to csv\n            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg{n}.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a2.to_csv(destination + f\"combined_csv_{i}_emg{n}.csv\", index=False, encoding='utf-8-sig')            \n            \n        elif len(all_filenames)<=5:\n            \n            a2=[i for i in all_filenames if \"emg\" in i ]\n            a1=list(set(all_filenames)-set(a2))\n            \n            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n    \n            #combine all files in the list\n            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in a1],axis=1)\n            a2=pd.read_csv(a2[0])\n    \n            #export to csv\n            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg{n}.csv\", index=False, encoding='utf-8-sig')\n            a2.to_csv(destination + f\"combined_csv_{i}_emg{n}.csv\", index=False, encoding='utf-8-sig')\n            \n\n# Set 10,11,13\n\nfor n in [\"10\",\"11\",\"13\"]:\n    for i in alpha:\n        os.chdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}\")\n    \n        all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n        \n        if len(all_filenames)==0:\n            continue\n        \n        elif len(all_filenames)>5:\n            a1=[i for i in all_filenames if \"accelerometer\" in i ]  \n            a2=[i for i in all_filenames if \"emg\" in i ]\n            a3=[i for i in all_filenames if \"gyro\" in i ]\n            a4=re.findall(r\"orientation-\\S+\",\" \".join(all_filenames))\n            a5=re.findall(r\"orientation\\w+\\S+\",\" \".join(all_filenames))\n            \n            combined_csv_a1=pd.concat([pd.read_csv(f) for f in a1],axis=0)\n            combined_csv_a2=pd.concat([pd.read_csv(f) for f in a2],axis=0)\n            combined_csv_a3=pd.concat([pd.read_csv(f) for f in a3],axis=0)\n            combined_csv_a4=pd.concat([pd.read_csv(f) for f in a4],axis=0)\n            combined_csv_a5=pd.concat([pd.read_csv(f) for f in a5],axis=0)\n            \n            #export combined csv of each feature:\n            \n            combined_csv_a1.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/\" + f\"combined_csv_a1.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a2.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/\" + f\"combined_csv_emg.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a3.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/\" + f\"combined_csv_a3.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a4.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/\" + f\"combined_csv_a4.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a5.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/\" + f\"combined_csv_a5.csv\", index=False, encoding='utf-8-sig')\n            \n            #export combined csv of each feature:\n            \n            all_filenames = [\"combined_csv_a1.csv\",\"combined_csv_a3.csv\",\"combined_csv_a4.csv\",\"combined_csv_a5.csv\"]\n    \n            #combine all files in the list\n            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in all_filenames],axis=1)\n            combined_csv_a2=pd.read_csv(\"combined_csv_emg.csv\")\n                        \n            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n    \n            #export to csv\n            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg{n}.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a2.to_csv(destination + f\"combined_csv_{i}_emg{n}.csv\", index=False, encoding='utf-8-sig')            \n            \n        elif len(all_filenames)<=5:\n            \n            a2=[i for i in all_filenames if \"emg\" in i ]\n            a1=list(set(all_filenames)-set(a2))\n            \n            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n    \n            #combine all files in the list\n            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in a1],axis=1)\n            a2=pd.read_csv(a2[0])\n    \n            #export to csv\n            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg{n}.csv\", index=False, encoding='utf-8-sig')\n            a2.to_csv(destination + f\"combined_csv_{i}_emg{n}.csv\", index=False, encoding='utf-8-sig')\n            \n\n# Set 12\n\nfor n in [\"12\"]:\n    for i in alpha:\n        os.chdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left\")\n    \n        all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n        \n        if len(all_filenames)==0:\n            continue\n        \n        elif len(all_filenames)>5:\n            a1=[i for i in all_filenames if \"accelerometer\" in i ]  \n            a2=[i for i in all_filenames if \"emg\" in i ]\n            a3=[i for i in all_filenames if \"gyro\" in i ]\n            a4=re.findall(r\"orientation-\\S+\",\" \".join(all_filenames))\n            a5=re.findall(r\"orientation\\w+\\S+\",\" \".join(all_filenames))\n            \n            combined_csv_a1=pd.concat([pd.read_csv(f) for f in a1],axis=0)\n            combined_csv_a2=pd.concat([pd.read_csv(f) for f in a2],axis=0)\n            combined_csv_a3=pd.concat([pd.read_csv(f) for f in a3],axis=0)\n            combined_csv_a4=pd.concat([pd.read_csv(f) for f in a4],axis=0)\n            combined_csv_a5=pd.concat([pd.read_csv(f) for f in a5],axis=0)\n            \n            #export combined csv of each feature:\n            \n            combined_csv_a1.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left/\" + f\"combined_csv_a1.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a2.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left/\" + f\"combined_csv_emg.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a3.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left/\" + f\"combined_csv_a3.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a4.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left/\" + f\"combined_csv_a4.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a5.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left/\" + f\"combined_csv_a5.csv\", index=False, encoding='utf-8-sig')\n            \n            #export combined csv of each feature:\n            \n            all_filenames = [\"combined_csv_a1.csv\",\"combined_csv_a3.csv\",\"combined_csv_a4.csv\",\"combined_csv_a5.csv\"]\n    \n            #combine all files in the list\n            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in all_filenames],axis=1)\n            combined_csv_a2=pd.read_csv(\"combined_csv_emg.csv\")\n                        \n            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n    \n            #export to csv\n            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg12.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a2.to_csv(destination + f\"combined_csv_{i}_emg12.csv\", index=False, encoding='utf-8-sig')            \n            \n        elif len(all_filenames)<=5:\n            \n            a2=[i for i in all_filenames if \"emg\" in i ]\n            a1=list(set(all_filenames)-set(a2))\n            \n            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n    \n            #combine all files in the list\n            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in a1],axis=1)\n            a2=pd.read_csv(a2[0])\n    \n            #export to csv\n            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg12.csv\", index=False, encoding='utf-8-sig')\n            a2.to_csv(destination + f\"combined_csv_{i}_emg12.csv\", index=False, encoding='utf-8-sig')\n            \n\n# Set 15\n\n    for i in alpha:\n        os.chdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}\")\n    \n        all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n        \n        if len(all_filenames)==0:\n            continue\n        \n        elif len(all_filenames)>5:\n            a1=[i for i in all_filenames if \"accelerometer\" in i ]  \n            a2=[i for i in all_filenames if \"emg\" in i ]\n            a3=[i for i in all_filenames if \"gyro\" in i ]\n            a4=re.findall(r\"orientation-\\S+\",\" \".join(all_filenames))\n            a5=re.findall(r\"orientation\\w+\\S+\",\" \".join(all_filenames))\n            \n            combined_csv_a1=pd.concat([pd.read_csv(f) for f in a1],axis=0)\n            combined_csv_a2=pd.concat([pd.read_csv(f) for f in a2],axis=0)\n            combined_csv_a3=pd.concat([pd.read_csv(f) for f in a3],axis=0)\n            combined_csv_a4=pd.concat([pd.read_csv(f) for f in a4],axis=0)\n            combined_csv_a5=pd.concat([pd.read_csv(f) for f in a5],axis=0)\n            \n            #export combined csv of each feature:\n            \n            combined_csv_a1.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}/\" + f\"combined_csv_a1.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a2.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}/\" + f\"combined_csv_emg.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a3.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}/\" + f\"combined_csv_a3.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a4.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}/\" + f\"combined_csv_a4.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a5.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}/\" + f\"combined_csv_a5.csv\", index=False, encoding='utf-8-sig')\n            \n            #export combined csv of each feature:\n            \n            all_filenames = [\"combined_csv_a1.csv\",\"combined_csv_a3.csv\",\"combined_csv_a4.csv\",\"combined_csv_a5.csv\"]\n    \n            #combine all files in the list\n            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in all_filenames],axis=1)\n            combined_csv_a2=pd.read_csv(\"combined_csv_emg.csv\")\n                        \n            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/\"\n    \n            #export to csv\n            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg15.csv\", index=False, encoding='utf-8-sig')\n            combined_csv_a2.to_csv(destination + f\"combined_csv_{i}_emg15.csv\", index=False, encoding='utf-8-sig')            \n            \n        elif len(all_filenames)<=5:\n            \n            a2=[i for i in all_filenames if \"emg\" in i ]\n            a1=list(set(all_filenames)-set(a2))\n            \n            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/\"\n    \n            #combine all files in the list\n            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in a1],axis=1)\n            a2=pd.read_csv(a2[0])\n    \n            #export to csv\n            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg15.csv\", index=False, encoding='utf-8-sig')\n            a2.to_csv(destination + f\"combined_csv_{i}_emg15.csv\", index=False, encoding='utf-8-sig')\n            \n\n\n\nos.chdir(\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2\")\n\nDir=\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2\"\n\nDir\n\nsets=os.listdir()\n\nsets\n\nfor i in sets:\n    l_nemg=[i for i in os.listdir(os.path.join(Dir,i)) if \"_nemg\" in i]\n    l_emg=[i for i in os.listdir(os.path.join(Dir,i)) if \"_emg\" in i]\n    #print(f\"Set {i} has {len(l)} files\")\n    #print(l)\n    if len(l)!=0:\n        combined_emg=pd.concat([pd.read_csv(os.path.join(Dir,i,j)) for j in l_emg],axis=0)\n        combined_nemg=pd.concat([pd.read_csv(os.path.join(Dir,i,j)) for j in l_nemg],axis=0)\n        combined.to_csv(f\"{i}.csv\",index=False)\n    else:\n        pass\n        \n\n\ntarget2=\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/NonEMG\"\ntarget1=\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/EMG\"\n\nfor i in sets:\n    if len(os.listdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/{i}\"))!=0:\n        q=os.listdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/{i}\")\n        q_emg=[f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/{i}/{j}\" for j in q if \"_emg\" in j]\n        q_nemg=[f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/{i}/{j}\" for j in q if \"_nemg\" in j]\n        for j in alpha:\n            w_emg=[x for x in q_emg if f\"_{j}_\" in x]\n            w_nemg=[x for x in q_nemg if f\"_{j}_\" in x]\n            \n            if len(w_emg)!=0:\n                df_emg=pd.read_csv(w_emg[0])\n                df_nemg=pd.read_csv(w_nemg[0])\n                #df_emg=pd.concat([pd.read_csv(l) for l in w_emg],axis=0)\n                #df_nemg=pd.concat([pd.read_csv(l) for l in w_nemg],axis=0)\n                df_emg.to_csv(target1+f\"/{i}{j}.csv\",index=False)\n                df_nemg.to_csv(target2+f\"/{i}{j}.csv\",index=False)\n                \n            else:\n                pass\n            \n    else:\n        pass\n            \n            \n            \n\n\n\nfile=[\"EMG\",\"NonEMG\"]\n\nfor i in file:\n    os.chdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/{i}\")\n    for j in alpha:\n        l=[k for k in os.listdir() if f\"{j}.csv\" in k]\n        df=pd.concat([pd.read_csv(x) for x in l],axis=0)\n        df.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/Final{i}_{j}.csv\")\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combining all Files:","metadata":{}},{"cell_type":"code","source":"DIR=\"../input/nemg-nonimputed/Non-EMG-Non Imputed\"\ntext=\"abcdefghijklmnopqrstuvwxyz\"\nj=[i for i in text]\ndf_mega=pd.DataFrame()\ndf_label=pd.DataFrame()\nl=os.listdir(DIR)\n\nfor i in l:\n    if len(df_mega)==0:\n        df=pd.read_csv(os.path.join(DIR,i))\n        df_mega=df\n        \n        df_label[\"Label\"]=np.array([i[-5] for u in range(len(df_mega))])\n        \n    else:\n        x=pd.read_csv(os.path.join(DIR,i))\n        arr=pd.DataFrame()\n        arr[\"Label\"]=np.array([i[-5] for u in range(len(x))])\n        \n        df_mega=pd.concat([df_mega,x])\n        df_label=pd.concat([df_label,arr])\n\ndf_combined=pd.concat([df_mega,df_label],axis=1)\ndf_combined=df_combined.reset_index(drop=True)\ndel df_combined[\"index\"]\ndf_combined.to_csv(\"Combined_NonEMG_nonimputed.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TSFresh ","metadata":{}},{"cell_type":"markdown","source":"No Overlap:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-09-11T10:46:01.677003Z","iopub.execute_input":"2021-09-11T10:46:01.67808Z","iopub.status.idle":"2021-09-11T10:46:01.682828Z","shell.execute_reply.started":"2021-09-11T10:46:01.677995Z","shell.execute_reply":"2021-09-11T10:46:01.681651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=pd.read_csv(\"../input/final-combined-har/Version 2_Dataset 2/Non EMG/FinalNonEMG_a.csv\")\nx.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T10:46:06.63739Z","iopub.execute_input":"2021-09-11T10:46:06.637711Z","iopub.status.idle":"2021-09-11T10:46:07.066252Z","shell.execute_reply.started":"2021-09-11T10:46:06.637675Z","shell.execute_reply":"2021-09-11T10:46:07.065258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tsfresh\nfrom tsfresh import extract_features\nfrom tsfresh.utilities.dataframe_functions import impute\ndirect=\"../input/har-dataset/Version 2/Non EMG\"\nl=sorted(os.listdir(direct))\nl\ndef funct_preprocessing(df):\n    \n    #Removing unwanted columns.\n    df.drop(columns=[i for i in df.columns if \"timestamp\" in i],inplace=True)\n    del df[\"Unnamed: 0\"]\n    \n    #Creating two new columns and initialising them with zeroes.\n    df1=pd.DataFrame({\"id\":np.zeros(len(df)),\"time\":np.zeros(len(df))})\n    df=pd.concat([df1,df],axis=1)\n    df[\"id\"]=df[\"id\"].astype(\"int\")\n    df[\"time\"]=df[\"time\"].astype(\"int\")\n    \n    l=np.arange(0,df.shape[0],step=400)\n    l=np.append(l,df.shape[0])\n    \n    #Sliding Window:    \n    for i in range(len(l)-1):   (0,400)(400,800)\n        k=1\n        for j in range(l[i],l[i+1]):\n            df.loc[j,\"time\"]=k\n            df.loc[j,\"id\"]=i\n            k=k+1\n    \n    return df\n\ndef funct_tsfresh(df):\n    #Extracting Features\n    df_extracted = extract_features(df, column_id=\"id\", column_sort=\"time\")\n    \n    #Imputing the NaNs\n    df_extracted=impute(df_extracted)\n    \n    return df_extracted\n\nfor i in l[0]:\n    df=pd.read_csv(os.path.join(direct,i))\n    df.dropna(inplace=True)\n    df.reset_index(drop=True,inplace=True)\n    df=funct_preprocessing(df)\n    df_extracted=funct_tsfresh(df)\n    df_extracted.to_csv(f\"{i}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"50% Overlap:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport tsfresh\nfrom tsfresh import extract_features\nfrom tsfresh.utilities.dataframe_functions import impute\ndirect=\"../input/final-combined-har/Version 2_Dataset 2/EMG\"\nl=sorted(os.listdir(direct))\n\n\ndef funct_preprocessing(df):\n    \n    #Removing unwanted columns.\n    df.drop(columns=[i for i in df.columns if \"timestamp\" in i],inplace=True)\n    del df[\"Unnamed: 0\"]\n    \n    #Sliding Window with 50% overlap:\n    w,t=300,150\n    r = np.arange(len(df))  # creating an array\n    s = r[::t]              #selecting elements with a step of t, i.e. half of SL\n    z = list(zip(s, s + w)) #Creating a list of tuples, each tuple holding the starting and ending row numbers.\n    g = lambda t: df.iloc[t[0]:t[1]]\n    df=pd.concat(map(g, z))\n    \n    #Given Dataframe is new1\n    df[\"id\"]=np.zeros(((len(df)),1))\n    df[\"time\"]=np.zeros(((len(df)),1))  \n    \n    df.reset_index(drop=True,inplace=True)\n    \n    l=np.arange(0,len(df),step=300)\n    l=np.append(l,len(df))\n    \n    #Sliding Window:    \n    for i in range(len(l)-1):\n        k=1\n        for j in range(l[i],l[i+1]):\n            df.loc[j,\"time\"]=k\n            df.loc[j,\"id\"]=i\n            k=k+1\n                \n    return df\n\ndef funct_tsfresh(df):\n    #Extracting Features\n    df_extracted = extract_features(df, column_id=\"id\", column_sort=\"time\")\n    \n    #Imputing the NaNs\n    df_extracted=impute(df_extracted)\n    \n    return df_extracted\n\n\nfor i in l[14:20]:\n    df=pd.read_csv(os.path.join(direct,i))\n    df.dropna(inplace=True)\n    df.reset_index(drop=True,inplace=True)\n    df=funct_preprocessing(df)\n    df_extracted=funct_tsfresh(df)\n    df_extracted.to_csv(f\"{i}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"markdown","source":"For Separate Files","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import chi2,mutual_info_classif\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.preprocessing import StandardScaler\n\ndfe=pd.read_csv(\"../input/combined-sl200/Combined_EMG.csv\")\ndfne=pd.read_csv(\"../input/combined-sl200/Combined_NonEMG.csv\")\n\ndfe.shape\ndfne.shape\ntext=\"abcdefghijklmnopqrstuvwxyz\"\nj=[i for i in text]\n\ncol=[i for i in dfe.columns if \"Unnamed: 0\" in i]\ndfe.drop(col,inplace=True,axis=1)\ncol=[i for i in dfne.columns if \"Unnamed: 0\" in i]\ndfne.drop(col,inplace=True,axis=1)\n\ndfe.reset_index(drop=True,inplace=True)\ndfne.reset_index(drop=True,inplace=True)\n\nXe=dfe.drop([\"Label\"],axis=1)\nye=dfe[\"Label\"]\nye=ye.apply(lambda x:j.index(x))\n\nXne=dfne.drop([\"Label\"],axis=1)\nyne=dfne[\"Label\"]\nyne=yne.apply(lambda x:j.index(x))\n#Optional and Important to know\n#Xe=Xe.mask(Xe==0).fillna(Xe.mean())\n#Xne=Xne.mask(Xne==0).fillna(Xne.mean())\nVare=Xe[Xe.columns].std()\ncol_e=Vare[Vare==0].index\nXe=Xe.drop(col_e,axis=1)\nXe.shape\n\nVarne=Xne[Xne.columns].std()\ncol_ne=Varne[Varne==0].index\nXne=Xne.drop(col_ne,axis=1)\nXne.shape\n\n---------------------------------------------\n\nl = [  61,  832, 1585, 2337, 3090, 3841, 4593, 5346] #List of indices to remove\ncols=[list(Xe.columns)[i] for i in l]\nXe=Xe.drop(cols,axis=1)\n\n# Create and fit selector\nselector = SelectKBest(f_classif, k=200)    \nselector.fit(Xe, ye)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\nX_e_fs= Xe.iloc[:,cols]\n\n---------------------------------------------\ns=\"   60   825  1575  2326  3078  3830  4581  5333  6092  6852  7611  8368 \\\n  9129  9889 10650 11410 12171 12932 13691 14448 15207\"\nl=list(map(int,s.split()))\ncols=[list(Xe.columns)[i] for i in l]\nXe=Xe.drop(cols,axis=1)\n\n\n# Create and fit selector\nselector = SelectKBest(f_classif, k=200)\nselector.fit(Xne, yne)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\nX_ne_fs= Xne.iloc[:,cols]\n\ndf_e_fs=pd.concat([X_e_fs,ye],axis=1)\ndf_ne_fs=pd.concat([X_ne_fs,yne],axis=1)\n(df_e_fs.shape,df_ne_fs.shape)\n\ndf_e_fs.to_csv(\"EMG_ANOVA_FS_200.csv\",index=False)\ndf_ne_fs.to_csv(\"NEMG_ANOVA_FS_200.csv\",index=False)\n\n---------------------------------------------\n\nXe=dfe.drop([\"Label\"],axis=1)\nye=dfe[\"Label\"]\nye=ye.apply(lambda x:j.index(x))\n\nXne=dfne.drop([\"Label\"],axis=1)\nyne=dfne[\"Label\"]\nyne=yne.apply(lambda x:j.index(x))\n\nmn=MinMaxScaler()\nX_m_e=mn.fit_transform(Xe)\n\nmn=MinMaxScaler()\nX_m_ne=mn.fit_transform(Xne)\n\n# Create and fit selector\nselector = SelectKBest(chi2, k=200)\nselector.fit(X_m_e, ye)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\nX_e_fs= Xe.iloc[:,cols]\n\n# Create and fit selector\nselector = SelectKBest(chi2, k=200)\nselector.fit(X_m_ne, yne)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\nX_ne_fs= Xne.iloc[:,cols]\n\ndf_e_fs=pd.concat([X_e_fs,ye],axis=1)\ndf_ne_fs=pd.concat([X_ne_fs,yne],axis=1)\n(df_e_fs.shape,df_ne_fs.shape)\n\ndf_e_fs.to_csv(\"EMG_CHI2_FS_200.csv\",index=False)\ndf_ne_fs.to_csv(\"NEMG_CHI2_FS_200.csv\",index=False)\n\n---------------------------------------------\n\nXe=dfe.drop([\"Label\"],axis=1)\nye=dfe[\"Label\"]\nye=ye.apply(lambda x:j.index(x))\n\nXne=dfne.drop([\"Label\"],axis=1)\nyne=dfne[\"Label\"]\nyne=yne.apply(lambda x:j.index(x))\n\n# Create and fit selector\nselector = SelectKBest(mutual_info_classif, k=200)\nselector.fit(Xe, ye)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\nX_e_fs= Xe.iloc[:,cols]\n\n# Create and fit selector\nselector = SelectKBest(mutual_info_classif, k=200)\nselector.fit(Xne, yne)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\nX_ne_fs= Xne.iloc[:,cols]\n\ndf_e_fs=pd.concat([X_e_fs,ye],axis=1)\ndf_ne_fs=pd.concat([X_ne_fs,yne],axis=1)\n(df_e_fs.shape,df_ne_fs.shape)\n\ndf_e_fs.to_csv(\"EMG_MI_FS_200.csv\",index=False)\ndf_ne_fs.to_csv(\"NEMG_MI_FS_200.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For Combined","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif,chi2,mutual_info_classif\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\ndfe=pd.read_csv(\"../input/combined-sl200/Combined_EMG.csv\")\ndfne=pd.read_csv(\"../input/combined-sl200/Combined_NonEMG.csv\")\ntext=\"abcdefghijklmnopqrstuvwxyz\"\nj=[i for i in text]\n\ndfe[\"Label\"]=dfe[\"Label\"].apply(lambda x:j.index(x))\ndfne[\"Label\"]=dfne[\"Label\"].apply(lambda x:j.index(x))\n\ncol=[i for i in dfe.columns if \"Unnamed: 0\" in i]\ndfe.drop(col,inplace=True,axis=1)\ncol=[i for i in dfne.columns if \"Unnamed: 0\" in i]\ndfne.drop(col,inplace=True,axis=1)\n\ndfe[\"Label\"]=dfe[\"Label\"].astype(int)\ndfne[\"Label\"]=dfne[\"Label\"].astype(int)\nd1=dfe.copy()#More rows\nd2=dfne.copy()#Less rows\n\ndf_final=pd.DataFrame()\n\nfor i in np.arange(0,26):\n    if len(df_final)==0:\n        df2=(d2[d2[\"Label\"]==i]).drop([\"Label\"],axis=1).reset_index(drop=True)\n        df1=(d1[d1[\"Label\"]==i].head(len(df2))).drop([\"Label\"],axis=1).reset_index(drop=True)\n        label=pd.DataFrame(np.array([i for j in range(len(df2))]).reshape(-1,1),columns=[\"Label\"])\n        df12=pd.concat([df1,df2,label],axis=1)\n        df_final=df12\n        df_final.reset_index(drop=True,inplace=True)\n    else:\n        df2=(d2[d2[\"Label\"]==i]).drop([\"Label\"],axis=1).reset_index(drop=True)\n        df1=(d1[d1[\"Label\"]==i].head(len(df2))).drop([\"Label\"],axis=1).reset_index(drop=True)\n        label=pd.DataFrame(np.array([i for j in range(len(df2))]).reshape(-1,1),columns=[\"Label\"])\n        df12=pd.concat([df1,df2,label],axis=1)\n        df_final=pd.concat([df_final,df12],axis=0)\n        df_final.reset_index(drop=True,inplace=True)\n\n        \ndf_final.dropna(inplace=True)\nX=df_final.drop([\"Label\"],axis=1)\ny=df_final[\"Label\"]\n\n---------------------------------------------\n#Var=X.var()\n#cols=Var[Var==0].index\n#X=X.drop(cols,axis=1)\ns=\"   60   825  1575  2326  3078  3830  4581  5333  6092  6852  7611  8368 \\\n  9129  9889 10650 11410 12171 12932 13691 14448 15207\"\nl=list(map(int,s.split()))\ncols=[list(Xe.columns)[i] for i in l]\nXe=Xe.drop(cols,axis=1)\n\n\n# Create and fit selector\nselector = SelectKBest(f_classif, k=200)\nselector.fit(X, y)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\nX_fs= X.iloc[:,cols]\n\ndf_fs=pd.concat([X_fs,y],axis=1)\ndf_fs.to_csv(\"EMG+NEMG_ANOVA_FS_200.csv\",index=False)\n\n---------------------------------------------\n\nX=df_final.drop([\"Label\"],axis=1)\ny=df_final[\"Label\"]\n\nmn=MinMaxScaler()\nX_m=mn.fit_transform(X)\n\n# Create and fit selector\nselector = SelectKBest(chi2, k=200)\nselector.fit(X_m, y)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\nX_fs= X.iloc[:,cols]\n\ndf_fs=pd.concat([X_fs,y],axis=1)\ndf_fs.to_csv(\"EMG+NEMG_CHI2_FS_200.csv\",index=False)\n#mn=MinMaxScaler()\n#X_m=mn.fit_transform(X)\n\n---------------------------------------------\n\n# Create and fit selector\nselector = SelectKBest(mutual_info_classif, k=200)\nselector.fit(X_m, y)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\nX_fs= X.iloc[:,cols]\n\ndf_fs=pd.concat([X_fs,y],axis=1)\ndf_fs.to_csv(\"EMG+NEMG_MI_FS_200.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report,accuracy_score,make_scorer,confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import StandardScaler\n\ndf=pd.read_csv(\"../input/200dataset/EMGNEMG_ANOVA_FS_200.csv\")\n\nX=df.drop([\"Label\"],axis=1)\ny=df[\"Label\"]\n\nX=X.values\ny=y.values\n\nsc=StandardScaler()\nX=sc.fit_transform(X)\n\ndef classification_report_with_accuracy_score(y_true, y_pred):\n\n    print(classification_report(y_true, y_pred)) # print classification report\n    cm=confusion_matrix(y_true,y_pred)\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # For normalising the MAtrix for better visualisation.\n    plt.figure(figsize=(15,15))\n    plt.rc(\"font\",size=7)\n    sns.heatmap(cm,annot=True,fmt=\".2f\",cmap=\"viridis\")\n    plt.show()\n    return accuracy_score(y_true, y_pred) # return accuracy score\n\ndef fun_best(X,y):\n    models=[\"XGB\",\"LGBM\",\"CatBoost\",\"GradientBoost\",\"LDA\"]\n    mean_score=[]\n    \n    xgbr=XGBClassifier()\n    score_xg=cross_val_score(xgbr,X,y,cv=5,scoring=make_scorer(classification_report_with_accuracy_score))\n    mean_score.append(score_xg.mean())\n\n    lgbm=LGBMClassifier()\n    score_lg=cross_val_score(lgbm,X,y,cv=5,scoring=make_scorer(classification_report_with_accuracy_score))\n    mean_score.append(score_lg.mean())\n    \n    cb=CatBoostClassifier()\n    score_cb=cross_val_score(cb,X,y,cv=5,scoring=make_scorer(classification_report_with_accuracy_score))\n    mean_score.append(score_cb.mean())\n\n    gb=GradientBoostClassifier()\n    score_gb=cross_val_score(gb,X,y,cv=5,scoring=make_scorer(classification_report_with_accuracy_score))\n    mean_score.append(score_gb.mean())\n    \n    ld = LinearDiscriminantAnalysis()\n    score_ld=cross_val_score(ld,X,y,cv=5,scoring=make_scorer(classification_report_with_accuracy_score))\n    mean_score.append(score_ld.mean())\n    \n'''\n    Use of Stratified Sampling:\n    xgbr=XGBClassifier()\n    score_xg=cross_val_score(xgbr,X,y,cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),scoring=make_scorer(classification_report_with_accuracy_score))\n    mean_score.append(score_xg.mean())\n\n    lgbm=LGBMClassifier()\n    score_lg=cross_val_score(lgbm,X,y,cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),scoring=make_scorer(classification_report_with_accuracy_score))\n    mean_score.append(score_lg.mean())\n\n''' \n\n    return dict(zip(models,mean_score))\n\nresult=fun_best(X,y)\nprint(\"For ___ Feature Selection:\")\nprint(result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting Feature Names:","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"../input/model-selection-sl300-50/50% overlap/EMG+NEMG/EMGNEMG_ANOVA_FS_3 (1).csv\")\nfeatures=np.array(df.columns)\nnp.savetxt(\"ANOVA_50%_SL=300.txt\",features,fmt='%s')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# t-SNE Plots","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#SL=300 with 50% Overlap\ndf=pd.read_csv(\"../input/200dataset/EMGNEMG_ANOVA_FS_200.csv\")\nX=df.drop([\"Label\"],axis=1)\ny=df[\"Label\"]\n\nstr=\".,ov^<>12348pP*hH+xXDd|_\"\nlen(str)\n\nmarker=[i for i in str]\nmarker.append('$y$')\nmarker.append('$z$')\nmarker\n# performs t-sne with different perplexity values and their repective plots..\n\ndef perform_tsne(X_data, y_data, perplexities, n_iter=1000, img_name_prefix='t-sne'):\n        \n    for index,perplexity in enumerate(perplexities):\n        # perform t-sne\n        print('\\nperforming tsne with perplexity {} and with {} iterations at max'.format(perplexity, n_iter))\n        X_reduced = TSNE(verbose=2, perplexity=perplexity).fit_transform(X_data)\n        print('Done..')\n        \n        # prepare the data for seaborn         \n        print('Creating plot for this t-sne visualization..')\n        df = pd.DataFrame({'x':X_reduced[:,0], 'y':X_reduced[:,1] ,'label':y_data})\n        \n        # draw the plot in appropriate place in the grid\n        sns.set_style(\"darkgrid\")\n        sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, height=8,markers=marker,palette=\"Set1\")\n        plt.title(\"Perplexity : {} and max_iter : {}\".format(perplexity, n_iter),size=25,weight=\"bold\")\n        img_name = img_name_prefix + '_perp_{}_iter_{}.png'.format(perplexity, n_iter)\n        print('Saving this plot as image in present working directory...')\n        plt.savefig(img_name)\n        plt.show()\n        print('Done!')\n\n\nperform_tsne(X_data = X,y_data=y, perplexities =[2,5,10,20,50])   ","metadata":{},"execution_count":null,"outputs":[]}]}