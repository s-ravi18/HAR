{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd "]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing:"]},{"cell_type":"markdown","metadata":{},"source":["### Done on Local Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import glob\n","import pandas as pd\n","\n","import re\n","\n","alpha=[\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n","\n","extension = 'csv'\n","\n","# Set 008 and 009\n","\n","for n in [\"08\",\"09\"]:\n","    for i in alpha:\n","        os.chdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right\")\n","    \n","        all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n","        \n","        if len(all_filenames)==0:\n","            continue\n","        \n","        elif len(all_filenames)>5:\n","            a1=[i for i in all_filenames if \"accelerometer\" in i ]  \n","            a2=[i for i in all_filenames if \"emg\" in i ]\n","            a3=[i for i in all_filenames if \"gyro\" in i ]\n","            a4=re.findall(r\"orientation-\\S+\",\" \".join(all_filenames))\n","            a5=re.findall(r\"orientation\\w+\\S+\",\" \".join(all_filenames))\n","            \n","            combined_csv_a1=pd.concat([pd.read_csv(f) for f in a1],axis=0)\n","            combined_csv_a2=pd.concat([pd.read_csv(f) for f in a2],axis=0)\n","            combined_csv_a3=pd.concat([pd.read_csv(f) for f in a3],axis=0)\n","            combined_csv_a4=pd.concat([pd.read_csv(f) for f in a4],axis=0)\n","            combined_csv_a5=pd.concat([pd.read_csv(f) for f in a5],axis=0)\n","            \n","            #export combined csv of each feature:\n","            \n","            combined_csv_a1.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right/\" + f\"combined_csv_a1.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a2.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right/\" + f\"combined_csv_emg.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a3.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right/\" + f\"combined_csv_a3.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a4.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right/\" + f\"combined_csv_a4.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a5.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/right/\" + f\"combined_csv_a5.csv\", index=False, encoding='utf-8-sig')\n","            \n","            #export combined csv of each feature:\n","            \n","            all_filenames = [\"combined_csv_a1.csv\",\"combined_csv_a3.csv\",\"combined_csv_a4.csv\",\"combined_csv_a5.csv\"]\n","    \n","            #combine all files in the list\n","            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in all_filenames],axis=1)\n","            combined_csv_a2=pd.read_csv(\"combined_csv_emg.csv\")\n","                        \n","            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n","    \n","            #export to csv\n","            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg{n}.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a2.to_csv(destination + f\"combined_csv_{i}_emg{n}.csv\", index=False, encoding='utf-8-sig')            \n","            \n","        elif len(all_filenames)<=5:\n","            \n","            a2=[i for i in all_filenames if \"emg\" in i ]\n","            a1=list(set(all_filenames)-set(a2))\n","            \n","            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n","    \n","            #combine all files in the list\n","            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in a1],axis=1)\n","            a2=pd.read_csv(a2[0])\n","    \n","            #export to csv\n","            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg{n}.csv\", index=False, encoding='utf-8-sig')\n","            a2.to_csv(destination + f\"combined_csv_{i}_emg{n}.csv\", index=False, encoding='utf-8-sig')\n","            \n","\n","# Set 10,11,13\n","\n","for n in [\"10\",\"11\",\"13\"]:\n","    for i in alpha:\n","        os.chdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}\")\n","    \n","        all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n","        \n","        if len(all_filenames)==0:\n","            continue\n","        \n","        elif len(all_filenames)>5:\n","            a1=[i for i in all_filenames if \"accelerometer\" in i ]  \n","            a2=[i for i in all_filenames if \"emg\" in i ]\n","            a3=[i for i in all_filenames if \"gyro\" in i ]\n","            a4=re.findall(r\"orientation-\\S+\",\" \".join(all_filenames))\n","            a5=re.findall(r\"orientation\\w+\\S+\",\" \".join(all_filenames))\n","            \n","            combined_csv_a1=pd.concat([pd.read_csv(f) for f in a1],axis=0)\n","            combined_csv_a2=pd.concat([pd.read_csv(f) for f in a2],axis=0)\n","            combined_csv_a3=pd.concat([pd.read_csv(f) for f in a3],axis=0)\n","            combined_csv_a4=pd.concat([pd.read_csv(f) for f in a4],axis=0)\n","            combined_csv_a5=pd.concat([pd.read_csv(f) for f in a5],axis=0)\n","            \n","            #export combined csv of each feature:\n","            \n","            combined_csv_a1.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/\" + f\"combined_csv_a1.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a2.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/\" + f\"combined_csv_emg.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a3.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/\" + f\"combined_csv_a3.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a4.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/\" + f\"combined_csv_a4.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a5.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/\" + f\"combined_csv_a5.csv\", index=False, encoding='utf-8-sig')\n","            \n","            #export combined csv of each feature:\n","            \n","            all_filenames = [\"combined_csv_a1.csv\",\"combined_csv_a3.csv\",\"combined_csv_a4.csv\",\"combined_csv_a5.csv\"]\n","    \n","            #combine all files in the list\n","            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in all_filenames],axis=1)\n","            combined_csv_a2=pd.read_csv(\"combined_csv_emg.csv\")\n","                        \n","            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n","    \n","            #export to csv\n","            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg{n}.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a2.to_csv(destination + f\"combined_csv_{i}_emg{n}.csv\", index=False, encoding='utf-8-sig')            \n","            \n","        elif len(all_filenames)<=5:\n","            \n","            a2=[i for i in all_filenames if \"emg\" in i ]\n","            a1=list(set(all_filenames)-set(a2))\n","            \n","            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n","    \n","            #combine all files in the list\n","            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in a1],axis=1)\n","            a2=pd.read_csv(a2[0])\n","    \n","            #export to csv\n","            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg{n}.csv\", index=False, encoding='utf-8-sig')\n","            a2.to_csv(destination + f\"combined_csv_{i}_emg{n}.csv\", index=False, encoding='utf-8-sig')\n","            \n","\n","# Set 12\n","\n","for n in [\"12\"]:\n","    for i in alpha:\n","        os.chdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left\")\n","    \n","        all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n","        \n","        if len(all_filenames)==0:\n","            continue\n","        \n","        elif len(all_filenames)>5:\n","            a1=[i for i in all_filenames if \"accelerometer\" in i ]  \n","            a2=[i for i in all_filenames if \"emg\" in i ]\n","            a3=[i for i in all_filenames if \"gyro\" in i ]\n","            a4=re.findall(r\"orientation-\\S+\",\" \".join(all_filenames))\n","            a5=re.findall(r\"orientation\\w+\\S+\",\" \".join(all_filenames))\n","            \n","            combined_csv_a1=pd.concat([pd.read_csv(f) for f in a1],axis=0)\n","            combined_csv_a2=pd.concat([pd.read_csv(f) for f in a2],axis=0)\n","            combined_csv_a3=pd.concat([pd.read_csv(f) for f in a3],axis=0)\n","            combined_csv_a4=pd.concat([pd.read_csv(f) for f in a4],axis=0)\n","            combined_csv_a5=pd.concat([pd.read_csv(f) for f in a5],axis=0)\n","            \n","            #export combined csv of each feature:\n","            \n","            combined_csv_a1.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left/\" + f\"combined_csv_a1.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a2.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left/\" + f\"combined_csv_emg.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a3.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left/\" + f\"combined_csv_a3.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a4.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left/\" + f\"combined_csv_a4.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a5.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/{i}/left/\" + f\"combined_csv_a5.csv\", index=False, encoding='utf-8-sig')\n","            \n","            #export combined csv of each feature:\n","            \n","            all_filenames = [\"combined_csv_a1.csv\",\"combined_csv_a3.csv\",\"combined_csv_a4.csv\",\"combined_csv_a5.csv\"]\n","    \n","            #combine all files in the list\n","            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in all_filenames],axis=1)\n","            combined_csv_a2=pd.read_csv(\"combined_csv_emg.csv\")\n","                        \n","            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n","    \n","            #export to csv\n","            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg12.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a2.to_csv(destination + f\"combined_csv_{i}_emg12.csv\", index=False, encoding='utf-8-sig')            \n","            \n","        elif len(all_filenames)<=5:\n","            \n","            a2=[i for i in all_filenames if \"emg\" in i ]\n","            a1=list(set(all_filenames)-set(a2))\n","            \n","            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set0{n}/\"\n","    \n","            #combine all files in the list\n","            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in a1],axis=1)\n","            a2=pd.read_csv(a2[0])\n","    \n","            #export to csv\n","            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg12.csv\", index=False, encoding='utf-8-sig')\n","            a2.to_csv(destination + f\"combined_csv_{i}_emg12.csv\", index=False, encoding='utf-8-sig')\n","            \n","\n","# Set 15\n","\n","    for i in alpha:\n","        os.chdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}\")\n","    \n","        all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n","        \n","        if len(all_filenames)==0:\n","            continue\n","        \n","        elif len(all_filenames)>5:\n","            a1=[i for i in all_filenames if \"accelerometer\" in i ]  \n","            a2=[i for i in all_filenames if \"emg\" in i ]\n","            a3=[i for i in all_filenames if \"gyro\" in i ]\n","            a4=re.findall(r\"orientation-\\S+\",\" \".join(all_filenames))\n","            a5=re.findall(r\"orientation\\w+\\S+\",\" \".join(all_filenames))\n","            \n","            combined_csv_a1=pd.concat([pd.read_csv(f) for f in a1],axis=0)\n","            combined_csv_a2=pd.concat([pd.read_csv(f) for f in a2],axis=0)\n","            combined_csv_a3=pd.concat([pd.read_csv(f) for f in a3],axis=0)\n","            combined_csv_a4=pd.concat([pd.read_csv(f) for f in a4],axis=0)\n","            combined_csv_a5=pd.concat([pd.read_csv(f) for f in a5],axis=0)\n","            \n","            #export combined csv of each feature:\n","            \n","            combined_csv_a1.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}/\" + f\"combined_csv_a1.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a2.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}/\" + f\"combined_csv_emg.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a3.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}/\" + f\"combined_csv_a3.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a4.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}/\" + f\"combined_csv_a4.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a5.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/{i}/\" + f\"combined_csv_a5.csv\", index=False, encoding='utf-8-sig')\n","            \n","            #export combined csv of each feature:\n","            \n","            all_filenames = [\"combined_csv_a1.csv\",\"combined_csv_a3.csv\",\"combined_csv_a4.csv\",\"combined_csv_a5.csv\"]\n","    \n","            #combine all files in the list\n","            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in all_filenames],axis=1)\n","            combined_csv_a2=pd.read_csv(\"combined_csv_emg.csv\")\n","                        \n","            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/\"\n","    \n","            #export to csv\n","            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg15.csv\", index=False, encoding='utf-8-sig')\n","            combined_csv_a2.to_csv(destination + f\"combined_csv_{i}_emg15.csv\", index=False, encoding='utf-8-sig')            \n","            \n","        elif len(all_filenames)<=5:\n","            \n","            a2=[i for i in all_filenames if \"emg\" in i ]\n","            a1=list(set(all_filenames)-set(a2))\n","            \n","            destination=f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/set15_new_remove_few_rows_frpm each_alphabet/\"\n","    \n","            #combine all files in the list\n","            combined_csv_nemg = pd.concat([pd.read_csv(f) for f in a1],axis=1)\n","            a2=pd.read_csv(a2[0])\n","    \n","            #export to csv\n","            combined_csv_nemg.to_csv(destination + f\"combined_csv_{i}_nemg15.csv\", index=False, encoding='utf-8-sig')\n","            a2.to_csv(destination + f\"combined_csv_{i}_emg15.csv\", index=False, encoding='utf-8-sig')\n","            \n","\n","\n","\n","os.chdir(\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2\")\n","\n","Dir=\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2\"\n","\n","Dir\n","\n","sets=os.listdir()\n","\n","sets\n","\n","for i in sets:\n","    l_nemg=[i for i in os.listdir(os.path.join(Dir,i)) if \"_nemg\" in i]\n","    l_emg=[i for i in os.listdir(os.path.join(Dir,i)) if \"_emg\" in i]\n","    #print(f\"Set {i} has {len(l)} files\")\n","    #print(l)\n","    if len(l)!=0:\n","        combined_emg=pd.concat([pd.read_csv(os.path.join(Dir,i,j)) for j in l_emg],axis=0)\n","        combined_nemg=pd.concat([pd.read_csv(os.path.join(Dir,i,j)) for j in l_nemg],axis=0)\n","        combined.to_csv(f\"{i}.csv\",index=False)\n","    else:\n","        pass\n","        \n","\n","\n","target2=\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/NonEMG\"\n","target1=\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/EMG\"\n","\n","for i in sets:\n","    if len(os.listdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/{i}\"))!=0:\n","        q=os.listdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/{i}\")\n","        q_emg=[f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/{i}/{j}\" for j in q if \"_emg\" in j]\n","        q_nemg=[f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/{i}/{j}\" for j in q if \"_nemg\" in j]\n","        for j in alpha:\n","            w_emg=[x for x in q_emg if f\"_{j}_\" in x]\n","            w_nemg=[x for x in q_nemg if f\"_{j}_\" in x]\n","            \n","            if len(w_emg)!=0:\n","                df_emg=pd.read_csv(w_emg[0])\n","                df_nemg=pd.read_csv(w_nemg[0])\n","                #df_emg=pd.concat([pd.read_csv(l) for l in w_emg],axis=0)\n","                #df_nemg=pd.concat([pd.read_csv(l) for l in w_nemg],axis=0)\n","                df_emg.to_csv(target1+f\"/{i}{j}.csv\",index=False)\n","                df_nemg.to_csv(target2+f\"/{i}{j}.csv\",index=False)\n","                \n","            else:\n","                pass\n","            \n","    else:\n","        pass\n","            \n","            \n","            \n","\n","\n","\n","file=[\"EMG\",\"NonEMG\"]\n","\n","for i in file:\n","    os.chdir(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/{i}\")\n","    for j in alpha:\n","        l=[k for k in os.listdir() if f\"{j}.csv\" in k]\n","        df=pd.concat([pd.read_csv(x) for x in l],axis=0)\n","        df.to_csv(f\"C:/Users/Ravikannan/Desktop/DL PROJECT under Shashank sir/hw_dataset_2/Final{i}_{j}.csv\")\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Combining all Files:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DIR=\"../input/nemg-nonimputed/Non-EMG-Non Imputed\"\n","text=\"abcdefghijklmnopqrstuvwxyz\"\n","j=[i for i in text]\n","df_mega=pd.DataFrame()\n","df_label=pd.DataFrame()\n","l=os.listdir(DIR)\n","\n","for i in l:\n","    if len(df_mega)==0:\n","        df=pd.read_csv(os.path.join(DIR,i))\n","        df_mega=df\n","        \n","        df_label[\"Label\"]=np.array([i[-5] for u in range(len(df_mega))])\n","        \n","    else:\n","        x=pd.read_csv(os.path.join(DIR,i))\n","        arr=pd.DataFrame()\n","        arr[\"Label\"]=np.array([i[-5] for u in range(len(x))])\n","        \n","        df_mega=pd.concat([df_mega,x])\n","        df_label=pd.concat([df_label,arr])\n","\n","df_combined=pd.concat([df_mega,df_label],axis=1)\n","df_combined=df_combined.reset_index(drop=True)\n","del df_combined[\"index\"]\n","df_combined.to_csv(\"Combined_NonEMG_nonimputed.csv\",index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# TSFresh "]},{"cell_type":"markdown","metadata":{},"source":["No Overlap:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T10:46:01.67808Z","iopub.status.busy":"2021-09-11T10:46:01.677003Z","iopub.status.idle":"2021-09-11T10:46:01.682828Z","shell.execute_reply":"2021-09-11T10:46:01.681651Z","shell.execute_reply.started":"2021-09-11T10:46:01.677995Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T10:46:06.637711Z","iopub.status.busy":"2021-09-11T10:46:06.63739Z","iopub.status.idle":"2021-09-11T10:46:07.066252Z","shell.execute_reply":"2021-09-11T10:46:07.065258Z","shell.execute_reply.started":"2021-09-11T10:46:06.637675Z"},"trusted":true},"outputs":[],"source":["x=pd.read_csv(\"../input/final-combined-har/Version 2_Dataset 2/Non EMG/FinalNonEMG_a.csv\")\n","x.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import tsfresh\n","from tsfresh import extract_features\n","from tsfresh.utilities.dataframe_functions import impute\n","direct=\"../input/har-dataset/Version 2/Non EMG\"\n","l=sorted(os.listdir(direct))\n","l\n","def funct_preprocessing(df):\n","    \n","    #Removing unwanted columns.\n","    df.drop(columns=[i for i in df.columns if \"timestamp\" in i],inplace=True)\n","    del df[\"Unnamed: 0\"]\n","    \n","    #Creating two new columns and initialising them with zeroes.\n","    df1=pd.DataFrame({\"id\":np.zeros(len(df)),\"time\":np.zeros(len(df))})\n","    df=pd.concat([df1,df],axis=1)\n","    df[\"id\"]=df[\"id\"].astype(\"int\")\n","    df[\"time\"]=df[\"time\"].astype(\"int\")\n","    \n","    l=np.arange(0,df.shape[0],step=400)\n","    l=np.append(l,df.shape[0])\n","    \n","    #Sliding Window:    \n","    for i in range(len(l)-1):   (0,400)(400,800)\n","        k=1\n","        for j in range(l[i],l[i+1]):\n","            df.loc[j,\"time\"]=k\n","            df.loc[j,\"id\"]=i\n","            k=k+1\n","    \n","    return df\n","\n","def funct_tsfresh(df):\n","    #Extracting Features\n","    df_extracted = extract_features(df, column_id=\"id\", column_sort=\"time\")\n","    \n","    #Imputing the NaNs\n","    df_extracted=impute(df_extracted)\n","    \n","    return df_extracted\n","\n","for i in l[0]:\n","    df=pd.read_csv(os.path.join(direct,i))\n","    df.dropna(inplace=True)\n","    df.reset_index(drop=True,inplace=True)\n","    df=funct_preprocessing(df)\n","    df_extracted=funct_tsfresh(df)\n","    df_extracted.to_csv(f\"{i}\")"]},{"cell_type":"markdown","metadata":{},"source":["50% Overlap:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","\n","import tsfresh\n","from tsfresh import extract_features\n","from tsfresh.utilities.dataframe_functions import impute\n","direct=\"../input/final-combined-har/Version 2_Dataset 2/EMG\"\n","l=sorted(os.listdir(direct))\n","\n","\n","def funct_preprocessing(df):\n","    \n","    #Removing unwanted columns.\n","    df.drop(columns=[i for i in df.columns if \"timestamp\" in i],inplace=True)\n","    del df[\"Unnamed: 0\"]\n","    \n","    #Sliding Window with 50% overlap:\n","    w,t=300,150\n","    r = np.arange(len(df))  # creating an array\n","    s = r[::t]              #selecting elements with a step of t, i.e. half of SL\n","    z = list(zip(s, s + w)) #Creating a list of tuples, each tuple holding the starting and ending row numbers.\n","    g = lambda t: df.iloc[t[0]:t[1]]\n","    df=pd.concat(map(g, z))\n","    \n","    #Given Dataframe is new1\n","    df[\"id\"]=np.zeros(((len(df)),1))\n","    df[\"time\"]=np.zeros(((len(df)),1))  \n","    \n","    df.reset_index(drop=True,inplace=True)\n","    \n","    l=np.arange(0,len(df),step=300)\n","    l=np.append(l,len(df))\n","    \n","    #Sliding Window:    \n","    for i in range(len(l)-1):\n","        k=1\n","        for j in range(l[i],l[i+1]):\n","            df.loc[j,\"time\"]=k\n","            df.loc[j,\"id\"]=i\n","            k=k+1\n","                \n","    return df\n","\n","def funct_tsfresh(df):\n","    #Extracting Features\n","    df_extracted = extract_features(df, column_id=\"id\", column_sort=\"time\")\n","    \n","    #Imputing the NaNs\n","    df_extracted=impute(df_extracted)\n","    \n","    return df_extracted\n","\n","\n","for i in l[14:20]:\n","    df=pd.read_csv(os.path.join(direct,i))\n","    df.dropna(inplace=True)\n","    df.reset_index(drop=True,inplace=True)\n","    df=funct_preprocessing(df)\n","    df_extracted=funct_tsfresh(df)\n","    df_extracted.to_csv(f\"{i}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Selection"]},{"cell_type":"markdown","metadata":{},"source":["For Separate Files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.feature_selection import chi2,mutual_info_classif\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif\n","from sklearn.preprocessing import StandardScaler\n","\n","dfe=pd.read_csv(\"../input/combined-sl200/Combined_EMG.csv\")\n","dfne=pd.read_csv(\"../input/combined-sl200/Combined_NonEMG.csv\")\n","\n","dfe.shape\n","dfne.shape\n","text=\"abcdefghijklmnopqrstuvwxyz\"\n","j=[i for i in text]\n","\n","col=[i for i in dfe.columns if \"Unnamed: 0\" in i]\n","dfe.drop(col,inplace=True,axis=1)\n","col=[i for i in dfne.columns if \"Unnamed: 0\" in i]\n","dfne.drop(col,inplace=True,axis=1)\n","\n","dfe.reset_index(drop=True,inplace=True)\n","dfne.reset_index(drop=True,inplace=True)\n","\n","Xe=dfe.drop([\"Label\"],axis=1)\n","ye=dfe[\"Label\"]\n","ye=ye.apply(lambda x:j.index(x))\n","\n","Xne=dfne.drop([\"Label\"],axis=1)\n","yne=dfne[\"Label\"]\n","yne=yne.apply(lambda x:j.index(x))\n","#Optional and Important to know\n","#Xe=Xe.mask(Xe==0).fillna(Xe.mean())\n","#Xne=Xne.mask(Xne==0).fillna(Xne.mean())\n","Vare=Xe[Xe.columns].std()\n","col_e=Vare[Vare==0].index\n","Xe=Xe.drop(col_e,axis=1)\n","Xe.shape\n","\n","Varne=Xne[Xne.columns].std()\n","col_ne=Varne[Varne==0].index\n","Xne=Xne.drop(col_ne,axis=1)\n","Xne.shape\n","\n","---------------------------------------------\n","\n","l = [  61,  832, 1585, 2337, 3090, 3841, 4593, 5346] #List of indices to remove\n","cols=[list(Xe.columns)[i] for i in l]\n","Xe=Xe.drop(cols,axis=1)\n","\n","# Create and fit selector\n","selector = SelectKBest(f_classif, k=200)    \n","selector.fit(Xe, ye)\n","# Get columns to keep and create new dataframe with those only\n","cols = selector.get_support(indices=True)\n","X_e_fs= Xe.iloc[:,cols]\n","\n","---------------------------------------------\n","s=\"   60   825  1575  2326  3078  3830  4581  5333  6092  6852  7611  8368 \\\n","  9129  9889 10650 11410 12171 12932 13691 14448 15207\"\n","l=list(map(int,s.split()))\n","cols=[list(Xe.columns)[i] for i in l]\n","Xe=Xe.drop(cols,axis=1)\n","\n","\n","# Create and fit selector\n","selector = SelectKBest(f_classif, k=200)\n","selector.fit(Xne, yne)\n","# Get columns to keep and create new dataframe with those only\n","cols = selector.get_support(indices=True)\n","X_ne_fs= Xne.iloc[:,cols]\n","\n","df_e_fs=pd.concat([X_e_fs,ye],axis=1)\n","df_ne_fs=pd.concat([X_ne_fs,yne],axis=1)\n","(df_e_fs.shape,df_ne_fs.shape)\n","\n","df_e_fs.to_csv(\"EMG_ANOVA_FS_200.csv\",index=False)\n","df_ne_fs.to_csv(\"NEMG_ANOVA_FS_200.csv\",index=False)\n","\n","---------------------------------------------\n","\n","Xe=dfe.drop([\"Label\"],axis=1)\n","ye=dfe[\"Label\"]\n","ye=ye.apply(lambda x:j.index(x))\n","\n","Xne=dfne.drop([\"Label\"],axis=1)\n","yne=dfne[\"Label\"]\n","yne=yne.apply(lambda x:j.index(x))\n","\n","mn=MinMaxScaler()\n","X_m_e=mn.fit_transform(Xe)\n","\n","mn=MinMaxScaler()\n","X_m_ne=mn.fit_transform(Xne)\n","\n","# Create and fit selector\n","selector = SelectKBest(chi2, k=200)\n","selector.fit(X_m_e, ye)\n","# Get columns to keep and create new dataframe with those only\n","cols = selector.get_support(indices=True)\n","X_e_fs= Xe.iloc[:,cols]\n","\n","# Create and fit selector\n","selector = SelectKBest(chi2, k=200)\n","selector.fit(X_m_ne, yne)\n","# Get columns to keep and create new dataframe with those only\n","cols = selector.get_support(indices=True)\n","X_ne_fs= Xne.iloc[:,cols]\n","\n","df_e_fs=pd.concat([X_e_fs,ye],axis=1)\n","df_ne_fs=pd.concat([X_ne_fs,yne],axis=1)\n","(df_e_fs.shape,df_ne_fs.shape)\n","\n","df_e_fs.to_csv(\"EMG_CHI2_FS_200.csv\",index=False)\n","df_ne_fs.to_csv(\"NEMG_CHI2_FS_200.csv\",index=False)\n","\n","---------------------------------------------\n","\n","Xe=dfe.drop([\"Label\"],axis=1)\n","ye=dfe[\"Label\"]\n","ye=ye.apply(lambda x:j.index(x))\n","\n","Xne=dfne.drop([\"Label\"],axis=1)\n","yne=dfne[\"Label\"]\n","yne=yne.apply(lambda x:j.index(x))\n","\n","# Create and fit selector\n","selector = SelectKBest(mutual_info_classif, k=200)\n","selector.fit(Xe, ye)\n","# Get columns to keep and create new dataframe with those only\n","cols = selector.get_support(indices=True)\n","X_e_fs= Xe.iloc[:,cols]\n","\n","# Create and fit selector\n","selector = SelectKBest(mutual_info_classif, k=200)\n","selector.fit(Xne, yne)\n","# Get columns to keep and create new dataframe with those only\n","cols = selector.get_support(indices=True)\n","X_ne_fs= Xne.iloc[:,cols]\n","\n","df_e_fs=pd.concat([X_e_fs,ye],axis=1)\n","df_ne_fs=pd.concat([X_ne_fs,yne],axis=1)\n","(df_e_fs.shape,df_ne_fs.shape)\n","\n","df_e_fs.to_csv(\"EMG_MI_FS_200.csv\",index=False)\n","df_ne_fs.to_csv(\"NEMG_MI_FS_200.csv\",index=False)"]},{"cell_type":"markdown","metadata":{},"source":["For Combined"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif,chi2,mutual_info_classif\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","\n","dfe=pd.read_csv(\"../input/combined-sl200/Combined_EMG.csv\")\n","dfne=pd.read_csv(\"../input/combined-sl200/Combined_NonEMG.csv\")\n","text=\"abcdefghijklmnopqrstuvwxyz\"\n","j=[i for i in text]\n","\n","dfe[\"Label\"]=dfe[\"Label\"].apply(lambda x:j.index(x))\n","dfne[\"Label\"]=dfne[\"Label\"].apply(lambda x:j.index(x))\n","\n","col=[i for i in dfe.columns if \"Unnamed: 0\" in i]\n","dfe.drop(col,inplace=True,axis=1)\n","col=[i for i in dfne.columns if \"Unnamed: 0\" in i]\n","dfne.drop(col,inplace=True,axis=1)\n","\n","dfe[\"Label\"]=dfe[\"Label\"].astype(int)\n","dfne[\"Label\"]=dfne[\"Label\"].astype(int)\n","d1=dfe.copy()#More rows\n","d2=dfne.copy()#Less rows\n","\n","df_final=pd.DataFrame()\n","\n","for i in np.arange(0,26):\n","    if len(df_final)==0:\n","        df2=(d2[d2[\"Label\"]==i]).drop([\"Label\"],axis=1).reset_index(drop=True)\n","        df1=(d1[d1[\"Label\"]==i].head(len(df2))).drop([\"Label\"],axis=1).reset_index(drop=True)\n","        label=pd.DataFrame(np.array([i for j in range(len(df2))]).reshape(-1,1),columns=[\"Label\"])\n","        df12=pd.concat([df1,df2,label],axis=1)\n","        df_final=df12\n","        df_final.reset_index(drop=True,inplace=True)\n","    else:\n","        df2=(d2[d2[\"Label\"]==i]).drop([\"Label\"],axis=1).reset_index(drop=True)\n","        df1=(d1[d1[\"Label\"]==i].head(len(df2))).drop([\"Label\"],axis=1).reset_index(drop=True)\n","        label=pd.DataFrame(np.array([i for j in range(len(df2))]).reshape(-1,1),columns=[\"Label\"])\n","        df12=pd.concat([df1,df2,label],axis=1)\n","        df_final=pd.concat([df_final,df12],axis=0)\n","        df_final.reset_index(drop=True,inplace=True)\n","\n","        \n","df_final.dropna(inplace=True)\n","X=df_final.drop([\"Label\"],axis=1)\n","y=df_final[\"Label\"]\n","\n","---------------------------------------------\n","#Var=X.var()\n","#cols=Var[Var==0].index\n","#X=X.drop(cols,axis=1)\n","s=\"   60   825  1575  2326  3078  3830  4581  5333  6092  6852  7611  8368 \\\n","  9129  9889 10650 11410 12171 12932 13691 14448 15207\"\n","l=list(map(int,s.split()))\n","cols=[list(Xe.columns)[i] for i in l]\n","Xe=Xe.drop(cols,axis=1)\n","\n","\n","# Create and fit selector\n","selector = SelectKBest(f_classif, k=200)\n","selector.fit(X, y)\n","# Get columns to keep and create new dataframe with those only\n","cols = selector.get_support(indices=True)\n","X_fs= X.iloc[:,cols]\n","\n","df_fs=pd.concat([X_fs,y],axis=1)\n","df_fs.to_csv(\"EMG+NEMG_ANOVA_FS_200.csv\",index=False)\n","\n","---------------------------------------------\n","\n","X=df_final.drop([\"Label\"],axis=1)\n","y=df_final[\"Label\"]\n","\n","mn=MinMaxScaler()\n","X_m=mn.fit_transform(X)\n","\n","# Create and fit selector\n","selector = SelectKBest(chi2, k=200)\n","selector.fit(X_m, y)\n","# Get columns to keep and create new dataframe with those only\n","cols = selector.get_support(indices=True)\n","X_fs= X.iloc[:,cols]\n","\n","df_fs=pd.concat([X_fs,y],axis=1)\n","df_fs.to_csv(\"EMG+NEMG_CHI2_FS_200.csv\",index=False)\n","#mn=MinMaxScaler()\n","#X_m=mn.fit_transform(X)\n","\n","---------------------------------------------\n","\n","# Create and fit selector\n","selector = SelectKBest(mutual_info_classif, k=200)\n","selector.fit(X_m, y)\n","# Get columns to keep and create new dataframe with those only\n","cols = selector.get_support(indices=True)\n","X_fs= X.iloc[:,cols]\n","\n","df_fs=pd.concat([X_fs,y],axis=1)\n","df_fs.to_csv(\"EMG+NEMG_MI_FS_200.csv\",index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Selection"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from xgboost import XGBClassifier\n","from catboost import CatBoostClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import classification_report,accuracy_score,make_scorer,confusion_matrix\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","df=pd.read_csv(\"../input/200dataset/EMGNEMG_ANOVA_FS_200.csv\")\n","\n","X=df.drop([\"Label\"],axis=1)\n","y=df[\"Label\"]\n","\n","X=X.values\n","y=y.values\n","\n","sc=StandardScaler()\n","X=sc.fit_transform(X)\n","\n","def classification_report_with_accuracy_score(y_true, y_pred):\n","\n","    print(classification_report(y_true, y_pred)) # print classification report\n","    cm=confusion_matrix(y_true,y_pred)\n","    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # For normalising the MAtrix for better visualisation.\n","    plt.figure(figsize=(15,15))\n","    plt.rc(\"font\",size=7)\n","    sns.heatmap(cm,annot=True,fmt=\".2f\",cmap=\"viridis\")\n","    plt.show()\n","    return accuracy_score(y_true, y_pred) # return accuracy score\n","\n","def fun_best(X,y):\n","    models=[\"XGB\",\"LGBM\",\"CatBoost\",\"GradientBoost\",\"LDA\"]\n","    mean_score=[]\n","    \n","    xgbr=XGBClassifier()\n","    score_xg=cross_val_score(xgbr,X,y,cv=5,scoring=make_scorer(classification_report_with_accuracy_score))\n","    mean_score.append(score_xg.mean())\n","\n","    lgbm=LGBMClassifier()\n","    score_lg=cross_val_score(lgbm,X,y,cv=5,scoring=make_scorer(classification_report_with_accuracy_score))\n","    mean_score.append(score_lg.mean())\n","    \n","    cb=CatBoostClassifier()\n","    score_cb=cross_val_score(cb,X,y,cv=5,scoring=make_scorer(classification_report_with_accuracy_score))\n","    mean_score.append(score_cb.mean())\n","\n","    gb=GradientBoostClassifier()\n","    score_gb=cross_val_score(gb,X,y,cv=5,scoring=make_scorer(classification_report_with_accuracy_score))\n","    mean_score.append(score_gb.mean())\n","    \n","    ld = LinearDiscriminantAnalysis()\n","    score_ld=cross_val_score(ld,X,y,cv=5,scoring=make_scorer(classification_report_with_accuracy_score))\n","    mean_score.append(score_ld.mean())\n","    \n","'''\n","    Use of Stratified Sampling:\n","    xgbr=XGBClassifier()\n","    score_xg=cross_val_score(xgbr,X,y,cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),scoring=make_scorer(classification_report_with_accuracy_score))\n","    mean_score.append(score_xg.mean())\n","\n","    lgbm=LGBMClassifier()\n","    score_lg=cross_val_score(lgbm,X,y,cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),scoring=make_scorer(classification_report_with_accuracy_score))\n","    mean_score.append(score_lg.mean())\n","\n","''' \n","\n","    return dict(zip(models,mean_score))\n","\n","result=fun_best(X,y)\n","print(\"For ___ Feature Selection:\")\n","print(result)"]},{"cell_type":"markdown","metadata":{},"source":["# Extracting Feature Names:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df=pd.read_csv(\"../input/model-selection-sl300-50/50% overlap/EMG+NEMG/EMGNEMG_ANOVA_FS_3 (1).csv\")\n","features=np.array(df.columns)\n","np.savetxt(\"ANOVA_50%_SL=300.txt\",features,fmt='%s')"]},{"cell_type":"markdown","metadata":{},"source":["# t-SNE Plots"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#SL=300 with 50% Overlap\n","df=pd.read_csv(\"../input/200dataset/EMGNEMG_ANOVA_FS_200.csv\")\n","X=df.drop([\"Label\"],axis=1)\n","y=df[\"Label\"]\n","\n","str=\".,ov^<>12348pP*hH+xXDd|_\"\n","len(str)\n","\n","marker=[i for i in str]\n","marker.append('$y$')\n","marker.append('$z$')\n","marker\n","# performs t-sne with different perplexity values and their repective plots..\n","\n","def perform_tsne(X_data, y_data, perplexities, n_iter=1000, img_name_prefix='t-sne'):\n","        \n","    for index,perplexity in enumerate(perplexities):\n","        # perform t-sne\n","        print('\\nperforming tsne with perplexity {} and with {} iterations at max'.format(perplexity, n_iter))\n","        X_reduced = TSNE(verbose=2, perplexity=perplexity).fit_transform(X_data)\n","        print('Done..')\n","        \n","        # prepare the data for seaborn         \n","        print('Creating plot for this t-sne visualization..')\n","        df = pd.DataFrame({'x':X_reduced[:,0], 'y':X_reduced[:,1] ,'label':y_data})\n","        \n","        # draw the plot in appropriate place in the grid\n","        sns.set_style(\"darkgrid\")\n","        sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, height=8,markers=marker,palette=\"Set1\")\n","        plt.title(\"Perplexity : {} and max_iter : {}\".format(perplexity, n_iter),size=25,weight=\"bold\")\n","        img_name = img_name_prefix + '_perp_{}_iter_{}.png'.format(perplexity, n_iter)\n","        print('Saving this plot as image in present working directory...')\n","        plt.savefig(img_name)\n","        plt.show()\n","        print('Done!')\n","\n","\n","perform_tsne(X_data = X,y_data=y, perplexities =[2,5,10,20,50])   "]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"b3eff57f1b29dfdd788faddb90ebd9222db114d636e87b8fde724656933d5975"}}},"nbformat":4,"nbformat_minor":4}
