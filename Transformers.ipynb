{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import string\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emg-window size is 150\n",
    "\n",
    "Nemg-window size is 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=\"../input/imu-dataset-original/IMU dataset\"\n",
    "\n",
    "#### Data Preprocesing::\n",
    "\n",
    "def funct_preprocessing(l,f,fn):\n",
    "    \n",
    "    emg=pd.DataFrame(columns=['timestamp', 'emg1', 'emg2', 'emg3', 'emg4', 'emg5', 'emg6', 'emg7',\n",
    "       'emg8'])\n",
    "    acc=pd.DataFrame()\n",
    "    gyro=pd.DataFrame()\n",
    "    \n",
    "    for i in sorted(os.listdir(os.path.join(dir,l))):\n",
    "        \n",
    "        if \"emg\" in i:\n",
    "            emg=pd.concat([emg,pd.read_csv(os.path.join(dir,l,i))],axis=0)\n",
    "            \n",
    "        elif \"acc\" in i or \"gyro\" in i:  \n",
    "        #### accelerometer,gyro,orientation have same feature names. Hence changing the column names\n",
    "            if \"acc\" in i:\n",
    "                temp=pd.read_csv(os.path.join(dir,l,i))\n",
    "                acc=pd.concat([acc,temp])\n",
    "\n",
    "            elif \"gyro\" in i:\n",
    "                temp=pd.read_csv(os.path.join(dir,l,i))\n",
    "                gyro=pd.concat([gyro,temp])\n",
    "\n",
    "\n",
    "    acc.columns=[\"timestamp\",\"x_acc\",\"y_acc\",\"z_acc\"]\n",
    "    acc.drop(\"timestamp\",axis=1,inplace=True)\n",
    "    gyro.columns=[\"timestamp\",\"x_gyro\",\"y_gyro\",\"z_gyro\"]\n",
    "    \n",
    "    nemg=pd.concat([acc,gyro],axis=1)\n",
    "    \n",
    "    ### Dropping the timestamp column:\n",
    "    emg.drop(\"timestamp\",axis=1,inplace=True)\n",
    "    nemg.drop(\"timestamp\",axis=1,inplace=True)\n",
    "    \n",
    "    #### preprocess emg and nemg further\n",
    "    ### Emg-window size is 150\n",
    "    ### Nemg-window size is 55\n",
    "\n",
    "    ne=emg.shape[0]//150\n",
    "    nn=nemg.shape[0]//55\n",
    "    \n",
    "    emg=emg.iloc[0:(ne)*150,:]\n",
    "    nemg=nemg.iloc[0:(nn)*55,:]\n",
    "\n",
    "    emg_new=pd.DataFrame(columns=[i for i in range(150)])\n",
    "    ### for emg:\n",
    "    for i in range(ne):   ### points to row number\n",
    "        for j in range(emg.shape[1]):   ### columns number\n",
    "            k=pd.DataFrame((emg.iloc[i*150:(i+1)*150,j].values).reshape(1,-1))\n",
    "            k1=k.to_dict('records')\n",
    "            emg_new=emg_new.append(k1)\n",
    "    emg_new[\"label\"]=l\n",
    "\n",
    "    nemg_new=pd.DataFrame(columns=[i for i in range(55)])\n",
    "    ### for emg:\n",
    "    for i in range(nn):   ### points to row number\n",
    "        for j in range(nemg.shape[1]):   ### columns number\n",
    "            k=pd.DataFrame((nemg.iloc[i*55:(i+1)*55,j].values).reshape(1,-1))\n",
    "            k1=k.to_dict('records')\n",
    "            nemg_new=nemg_new.append(k1)\n",
    "    nemg_new[\"label\"]=l    \n",
    "    \n",
    "    f=pd.concat([f,emg_new],axis=0)\n",
    "    fn=pd.concat([fn,nemg_new],axis=0)\n",
    "    \n",
    "    return f,fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### total time for 5 classes is 14.5 mins:\n",
    "f=pd.DataFrame(columns=[i for i in range(150)]+[\"label\"])\n",
    "fn=pd.DataFrame(columns=[i for i in range(55)]+[\"label\"])\n",
    "for i in list(string.ascii_uppercase)[:5]:\n",
    "    f,fn=funct_preprocessing(i,f,fn)\n",
    "\n",
    "#### Label Encoding the target variable:\n",
    "le=LabelEncoder()\n",
    "f[\"label\"]=le.fit_transform(f[\"label\"])\n",
    "fn[\"label\"]=le.fit_transform(fn[\"label\"])\n",
    "\n",
    "%%time\n",
    "f.head()\n",
    "\n",
    "### Exporting the dataframe into csv::\n",
    "f.to_csv(\"sample_5_classes_emg.csv\",index=False)\n",
    "fn.to_csv(\"sample_5_classes_nemg.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the Time Series Transformers Model::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../input/sample-5-classes-data/sample_5_classes_emg.csv\")\n",
    "X=df.drop(\"label\",axis=1).values\n",
    "y=df[\"label\"].values\n",
    "\n",
    "### Splitting the data\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "### Reshaping it into a 3-D dimension\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "n_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample Data (Default Example in Documentation)\n",
    "\n",
    "# def readucr(filename):\n",
    "#     data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "#     y = data[:, 0]\n",
    "#     x = data[:, 1:]\n",
    "#     return x, y.astype(int)\n",
    "\n",
    "\n",
    "# root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "\n",
    "# X_train, Y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
    "# X_test, Y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
    "\n",
    "# X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "# X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model processes a tensor of shape **(batch size, sequence length, features)**, where sequence length is the number \n",
    "\n",
    "of time steps and features is each input timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "### Training and Validating:::    \n",
    "%%time\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.0,\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3eff57f1b29dfdd788faddb90ebd9222db114d636e87b8fde724656933d5975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
