{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Header Files:"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T06:37:11.969672Z","iopub.status.busy":"2022-06-11T06:37:11.969409Z","iopub.status.idle":"2022-06-11T06:37:18.466543Z","shell.execute_reply":"2022-06-11T06:37:18.465807Z","shell.execute_reply.started":"2022-06-11T06:37:11.969642Z"},"trusted":true},"outputs":[],"source":["##Basics\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import regex as re\n","import os\n","import string\n","import time\n","import warnings\n","import json\n","warnings.filterwarnings(\"ignore\")\n","\n","##TSFresh\n","import tsfresh\n","from tsfresh import extract_features\n","from tsfresh.utilities.dataframe_functions import impute\n","\n","##ML scikit learn classes for data preprocessing:\n","from sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelEncoder\n","from sklearn.model_selection import train_test_split\n","\n","##ML scikit learn classes for feature selection:\n","from sklearn.feature_selection import chi2,mutual_info_classif  ### for chi2 and mutual info\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif  ### for ANOVA\n","from sklearn.feature_selection import RFE  ### for RFE\n","\n","##ML scikit learn classes for model selection:  \n","from xgboost import XGBClassifier\n","from catboost import CatBoostClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.linear_model import LogisticRegression\n","\n","##ML scikit learn classes for evaluating model:\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import classification_report,accuracy_score,make_scorer,confusion_matrix,precision_recall_fscore_support\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import learning_curve\n","\n","##ML scikit learn classes for creating Pipeline:\n","from sklearn.pipeline import Pipeline\n","\n","## Autokeras Library:\n","import autokeras as ak\n","from autokeras import StructuredDataClassifier\n","\n","## Deep Learning Libraries:\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model,Model\n","from tensorflow.keras.layers import concatenate,Input,Dense,ReLU,BatchNormalization,Concatenate\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import plot_model\n"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing:\n","\n","### Used Dataset:imu-dataset-original"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Iterate through the list of alphabets\n","## Create a function that takes in letter for file path, i.e \"K\" \n","## Select each file using file path\n","## Store the csv file name with emg as emg variable(type:Dataframe), file name with other names as nemg(type:Dataframe)\n","## Concatenate it with previous variables and return final eng,nemg\n","## Pass it into the same function with different letter"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dir=\"../input/imu-dataset-original/IMU dataset\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def funct_preprocessing(l,f,fn):\n","    \n","    emg=pd.DataFrame(columns=['timestamp', 'emg1', 'emg2', 'emg3', 'emg4', 'emg5', 'emg6', 'emg7',\n","       'emg8'])\n","    acc=pd.DataFrame()\n","    gyro=pd.DataFrame()\n","    \n","    for i in sorted(os.listdir(os.path.join(dir,l))):\n","        \n","        if \"emg\" in i:\n","            emg=pd.concat([emg,pd.read_csv(os.path.join(dir,l,i))],axis=0)\n","            \n","        elif \"acc\" in i or \"gyro\" in i:  \n","        #### accelerometer,gyro,orientation have same feature names. Hence changing the column names\n","            if \"acc\" in i:\n","                temp=pd.read_csv(os.path.join(dir,l,i))\n","                acc=pd.concat([acc,temp])\n","\n","            elif \"gyro\" in i:\n","                temp=pd.read_csv(os.path.join(dir,l,i))\n","                gyro=pd.concat([gyro,temp])\n","\n","    acc.columns=[\"timestamp\",\"x_acc\",\"y_acc\",\"z_acc\"]\n","    acc.drop(\"timestamp\",axis=1,inplace=True)\n","    gyro.columns=[\"timestamp\",\"x_gyro\",\"y_gyro\",\"z_gyro\"]\n","        \n","    nemg=pd.concat([acc,gyro],axis=1)\n","    \n","    emg[\"label\"]=l\n","    nemg[\"label\"]=l\n","    \n","    f=pd.concat([f,emg],axis=0)\n","    fn=pd.concat([fn,nemg],axis=0)\n","    \n","    return f,fn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["f=pd.DataFrame()\n","fn=pd.DataFrame()\n","for i in list(string.ascii_uppercase):\n","    f,fn=funct_preprocessing(i,f,fn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["f.to_csv(\"IMU_EMG.csv\",index=False)\n","fn.to_csv(\"IMU_NonEMG_acc_and_gyro_only.csv\",index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# TSFresh with 50% Overlap:\n","\n","### for EMG Dataset: WS=200\n","### for Non-EMG Dataset: WS=55\n","\n","### Used Dataset:imu-dataset-original"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Function to include time and id columns in the data:\n","def id_time(df,step):\n","    \n","    #Removing unwanted columns.\n","    df.drop(columns=[i for i in df.columns if \"timestamp\" in i],inplace=True)\n","        \n","    #Sliding Window with 50% overlap:\n","    w,t=step,(step//2)\n","    r = np.arange(len(df))  # creating an array\n","    s = r[::t]              #selecting elements with a step of t, i.e. half of SL\n","    z = list(zip(s, s + w)) #Creating a list of tuples, each tuple holding the starting and ending row numbers.\n","    g = lambda t: df.iloc[t[0]:t[1]]\n","    j=pd.concat(map(g, z))\n","    \n","    j[\"id\"]=0\n","    j[\"time\"]=0 \n","    \n","    l=np.arange(0,len(j),step=step)\n","    l=np.append(l,len(j))\n","    \n","    j.reset_index(drop=True,inplace=True)\n","    \n","    #Sliding Window:    \n","    for i in range(len(l)-1):\n","        time = np.arange(len(j[l[i]:l[i+1]]))\n","        t_id=np.full(len(time),i)\n","        j.iloc[l[i]:l[i+1],list(j.columns).index(\"time\")]=time\n","        j.iloc[l[i]:l[i+1],list(j.columns).index(\"id\")]=t_id\n","                \n","    return j\n","\n","## Function to extract time series features from the data:\n","def funct_tsfresh(df):\n","    #Extracting Features\n","    df_extracted = extract_features(df, column_id=\"id\", column_sort=\"time\")\n","    \n","    #Imputing the NaNs\n","    df_extracted=impute(df_extracted)\n","    \n","    return df_extracted"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Function to complete the preprocessing (combining files) and feature extraction process\n","def funct_preprocessing(l,f,fn):\n","    \n","    emg=pd.DataFrame(columns=['timestamp', 'emg1', 'emg2', 'emg3', 'emg4', 'emg5', 'emg6', 'emg7',\n","       'emg8'])\n","    acc=pd.DataFrame()\n","    gyro=pd.DataFrame()\n","    euler=pd.DataFrame()\n","    ori=pd.DataFrame()\n","    \n","    for i in sorted(os.listdir(os.path.join(dir,l))):\n","        \n","        if \"emg\" in i:\n","            emg=pd.concat([emg,pd.read_csv(os.path.join(dir,l,i))],axis=0)\n","            \n","        elif \"acc\" in i or \"gyro\" in i or \"orientation\" in i:  \n","        #### accelerometer,gyro,orientation have same feature names. Hence changing the column names\n","            if \"acc\" in i:\n","                temp=pd.read_csv(os.path.join(dir,l,i))\n","                acc=pd.concat([acc,temp])\n","\n","            elif \"gyro\" in i:\n","                temp=pd.read_csv(os.path.join(dir,l,i))\n","                gyro=pd.concat([gyro,temp])\n","                \n","            elif \"Euler\" in i:\n","                temp=pd.read_csv(os.path.join(dir,l,i))\n","                euler=pd.concat([euler,temp])\n","                \n","            else:\n","                temp=pd.read_csv(os.path.join(dir,l,i))\n","                ori=pd.concat([ori,temp])\n","\n","    acc.columns=[\"timestamp\",\"x_acc\",\"y_acc\",\"z_acc\"]\n","    acc.drop(\"timestamp\",axis=1,inplace=True)\n","    gyro.columns=[\"timestamp\",\"x_gyro\",\"y_gyro\",\"z_gyro\"]\n","    gyro.drop(\"timestamp\",axis=1,inplace=True)\n","    ori.columns=[\"timestamp\",\"x_ori\",\"y_ori\",\"z_ori\",\"w\"]\n","    ori.drop(\"timestamp\",axis=1,inplace=True)\n","    \n","    nemg=pd.concat([acc,gyro,ori,euler],axis=1)\n","    \n","    emg.reset_index(drop=True,inplace=True)\n","    nemg.reset_index(drop=True,inplace=True)\n","    \n","    type_emg={\"emg1\":float,\n","          \"emg2\":float,\n","          \"emg3\":float,\n","          \"emg4\":float,\n","          \"emg5\":float,\n","          \"emg6\":float,\n","          \"emg7\":float,\n","          \"emg8\":float\n","    }\n","    emg=emg.astype(type_emg)\n","    \n","###Now comes the TSFresh Part(Unsupervised)\n","\n","    emg=id_time(emg,200)\n","    emg=funct_tsfresh(emg)\n","    nemg=id_time(nemg,55)\n","    nemg=funct_tsfresh(nemg)    \n","    \n","    emg[\"label\"]=l\n","    nemg[\"label\"]=l\n","    \n","    f=pd.concat([f,emg],axis=0)\n","    fn=pd.concat([fn,nemg],axis=0)\n","    \n","    return f,fn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## To cross-validate the code:\n","#fn.groupby([\"id\"]).agg(\"count\")  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## To save time:\n","start=X   #### X is the custom value, like 18-21\n","end=X+3\n","f=pd.DataFrame()\n","fn=pd.DataFrame()\n","for i in list(string.ascii_uppercase)[start:end]:\n","    f,fn=funct_preprocessing(i,f,fn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["f.to_csv(f\"IMU_EMG_AfterTSFresh_{start}-{end}.csv\",index=False)\n","fn.to_csv(f\"IMU_NonEMG_AfterTSFresh_{start}-{end}.csv\",index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Selection:"]},{"cell_type":"markdown","metadata":{},"source":["There are various Feature selection methods that can be used:\n","1. ANOVA - FILTER Method of FS\n","2. CHI2 - FILTER Method of FS\n","3. RFE - WRAPPER Method of FS\n","4. Mutual Info - FILTER Method of FS\n","\n","There are other methods of FS like:\n","1. Feature Importance of individual models - EMBEDDED Method of FS\n","2. Pearson's Correlation\n","3. Spearman's Rank Correlation\n","4. PCA - FILTER Method of FS"]},{"cell_type":"markdown","metadata":{},"source":["## For Combining individual zip files:\n","## Used Dataset:after-tsfresh-imu"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:23:02.155233Z","iopub.status.busy":"2022-05-19T09:23:02.154904Z","iopub.status.idle":"2022-05-19T09:23:02.159538Z","shell.execute_reply":"2022-05-19T09:23:02.158376Z","shell.execute_reply.started":"2022-05-19T09:23:02.155203Z"},"trusted":true},"outputs":[],"source":["## Combining all the files:\n","dir=\"../input/after-tsfresh-imu\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:23:02.69262Z","iopub.status.busy":"2022-05-19T09:23:02.691939Z","iopub.status.idle":"2022-05-19T09:23:11.57276Z","shell.execute_reply":"2022-05-19T09:23:11.57147Z","shell.execute_reply.started":"2022-05-19T09:23:02.692582Z"},"trusted":true},"outputs":[],"source":["emg=pd.DataFrame()\n","for i in os.listdir(dir):\n","    for j in os.listdir(os.path.join(dir,i)):\n","        if \"_EMG_\" in j:\n","            emg=pd.concat([emg,pd.read_csv(os.path.join(dir,i,j))],axis=0)\n","            \n","nemg=pd.DataFrame()\n","for i in os.listdir(dir):\n","    for j in os.listdir(os.path.join(dir,i)):\n","        if \"_NonEMG_\" in j:\n","            nemg=pd.concat([nemg,pd.read_csv(os.path.join(dir,i,j))],axis=0)\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["emg.to_csv(\"Combined_EMG.csv\",index=False)\n","nemg.to_csv(\"Combined_NEMG.csv\",index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Created a dataset called imu-for-feature-selection"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-21T17:18:13.96737Z","iopub.status.busy":"2022-04-21T17:18:13.966869Z","iopub.status.idle":"2022-04-21T17:19:43.348749Z","shell.execute_reply":"2022-04-21T17:19:43.346844Z","shell.execute_reply.started":"2022-04-21T17:18:13.967339Z"},"trusted":true},"outputs":[],"source":["df=pd.read_csv(\"../input/imu-for-feature-selection/Combined_EMG.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-21T17:20:42.889342Z","iopub.status.busy":"2022-04-21T17:20:42.888794Z","iopub.status.idle":"2022-04-21T17:23:02.521158Z","shell.execute_reply":"2022-04-21T17:23:02.519948Z","shell.execute_reply.started":"2022-04-21T17:20:42.889311Z"},"trusted":true},"outputs":[],"source":["imu=pd.read_csv(\"../input/imu-for-feature-selection/Combined_NEMG.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-20T06:15:58.099318Z","iopub.status.busy":"2022-04-20T06:15:58.098995Z","iopub.status.idle":"2022-04-20T06:15:58.106947Z","shell.execute_reply":"2022-04-20T06:15:58.105976Z","shell.execute_reply.started":"2022-04-20T06:15:58.099282Z"},"trusted":true},"outputs":[],"source":["### Copied from above section:\n","def initial(df):\n","    \n","    #### Label Encoding the Target Variable\n","\n","    X=df.drop([\"label\"],axis=1)\n","    y=df[\"label\"]\n","    if df.label.dtype==str:   ### Will apply label encoding if needed\n","        le=LabelEncoder()\n","        y=le.fit_transform(y)\n","        y=pd.Series(y)\n","\n","    #### Removing Features having zero variance.\n","\n","    Var=X[X.columns].std()\n","    col=Var[Var==0].index\n","    X=X.drop(col,axis=1)\n","    \n","    return X,y"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-20T16:15:38.464917Z","iopub.status.busy":"2022-04-20T16:15:38.464547Z","iopub.status.idle":"2022-04-20T16:15:38.500216Z","shell.execute_reply":"2022-04-20T16:15:38.499575Z","shell.execute_reply.started":"2022-04-20T16:15:38.46482Z"},"trusted":true},"outputs":[],"source":["def draw_curve(train_sizes, train_scores, test_scores):\n","    train_scores_mean = np.mean(train_scores, axis=1)\n","    train_scores_std = np.std(train_scores, axis=1)\n","    test_scores_mean = np.mean(test_scores, axis=1)\n","    test_scores_std = np.std(test_scores, axis=1)\n","    \n","    plt.figure(figsize=(10,10))\n","    plt.xlabel(\"Training examples\")\n","    plt.ylabel(\"Score\")\n","    plt.gca().invert_yaxis()\n","    \n","    # box-like grid\n","    plt.grid()\n","    \n","    # plot the std deviation as a transparent range at each training set size\n","    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n","    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n","    \n","    # plot the average training and test score lines at each training set size\n","    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n","    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n","    \n","    # sizes the window for readability and displays the plot\n","    # shows error from 0 to 1.1\n","    plt.legend(loc=\"best\")\n","    plt.ylim(-.1,1.1)\n","    plt.show()\n","       \n","\n","def classification_report_with_accuracy_score(y_true, y_pred):\n","\n","    print(classification_report(y_true, y_pred)) # print classification report\n","    cm=confusion_matrix(y_true,y_pred)\n","    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # For normalising the Matrix for better visualisation.\n","    plt.figure(figsize=(20,10))\n","    plt.rc(\"font\",size=10)\n","    sns.heatmap(cm,annot=True,fmt=\".2f\",cmap=\"viridis\")\n","    plt.show()\n","    return accuracy_score(y_true, y_pred) # return accuracy score\n","\n","def fun_best(X,y):\n","    \n","    X.rename({\"emg6__value_count__value_-1\":\"emg6__value_count__value_2\"},axis=1,inplace=True)\n","\n","    #To remove JSON characters from column names because LGBM fails to execute    \n","    X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n","    \n","    models={\"XGB\":XGBClassifier(),\"LGBM\":LGBMClassifier(),\"GradientBoost\":GradientBoostingClassifier(),\"LDA\":LinearDiscriminantAnalysis(),\"RandomForest\":RandomForestClassifier()}\n","    mean_score=[]\n","    \n","    for i,j in models.items():\n","        try:\n","            \n","            model=j\n","            score_model=cross_val_score(model,X,y,cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),scoring=make_scorer(classification_report_with_accuracy_score))\n","            mean_score.append(score_model.mean())\n","            train_sizes, train_scores, test_scores = learning_curve(model, X, y, n_jobs=-1, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n","            draw_curve(train_sizes, train_scores, test_scores)\n","        \n","        except:\n","            \n","            model=j\n","            score_model=cross_val_score(model,X,y,cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),scoring=make_scorer(classification_report_with_accuracy_score))\n","            mean_score.append(score_model.mean())\n","            train_sizes, train_scores, test_scores = learning_curve(model, X, y, n_jobs=-1, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n","            draw_curve(train_sizes, train_scores, test_scores)            \n","        \n","    result=dict(zip(models.keys(),mean_score))\n","   \n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-20T06:16:07.885335Z","iopub.status.busy":"2022-04-20T06:16:07.884632Z","iopub.status.idle":"2022-04-20T06:16:07.89501Z","shell.execute_reply":"2022-04-20T06:16:07.894396Z","shell.execute_reply.started":"2022-04-20T06:16:07.885286Z"},"trusted":true},"outputs":[],"source":["def funct_FS_bestmodel(ds,fs_name,n):\n","    \n","    df=pd.read_csv(f\"../input/imu-for-feature-selection/Combined_{ds}.csv\")\n","    \n","    print(\"Read the Dataset\")\n","    \n","    X,y=initial(df)\n","\n","    print(\"Passed initial function\")\n","    \n","    fs_dic={\"MI\":SelectKBest(mutual_info_classif, k=n),\"CHI2\":SelectKBest(chi2, k=n),\"ANOVA\":SelectKBest(f_classif, k=n),\"RFE\":RFE(estimator=CatBoostClassifier(), n_features_to_select=n)}\n","    \n","    ### for Chi2 feature selection, data points must be strictly positive.\n","    \n","    if fs_name==\"CHI2\":\n","        pipe = Pipeline([('scaler', MinMaxScaler()),\n","                 ('selector', fs_dic[fs_name])])\n","    \n","    else:\n","        pipe = Pipeline([('scaler', StandardScaler()),\n","                 ('selector', fs_dic[fs_name])])\n","    \n","    print(\"Pipeline set up\")\n","    \n","    pipe.fit(X, y)\n","    # Get columns to keep and create new dataframe with those only\n","    cols = pipe.named_steps['selector'].get_support(indices=True) ### Note the format\n","    X_fs= X.iloc[:,cols]\n","\n","    df_fs=pd.concat([X_fs,y],axis=1)\n","    df_fs.rename({\"0\":\"label\"},axis=1,inplace=True)\n","    df_fs.to_csv(f\"{ds}_{fs_name}_{n}_features.csv\",index=False)\n","    \n","    print(\"Done Feature Selection\")\n","    \n","    result=fun_best(X_fs,y)\n","    print(f\"Top {n} features using {fs_name} technique:\")\n","    print(result)    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## To check the Corresponding Letter -> Number Encoding\n","target=pd.DataFrame(np.hstack([y.values.reshape((-1,1)),y_label.reshape((-1,1))]),columns=[\"label\",\"label_encoded\"])\n","target.drop_duplicates().sort_values(\"label\").reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-20T06:16:12.074486Z","iopub.status.busy":"2022-04-20T06:16:12.074172Z"},"trusted":true},"outputs":[],"source":["### For Trial:\n","### Params are (\"EMG\" or \"NEMG\"/ Selection Method Name / Number of features)\n","funct_FS_bestmodel(\"EMG\",\"CHI2\",200) "]},{"cell_type":"markdown","metadata":{},"source":["# Stacked Ensemble Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:23:24.420897Z","iopub.status.busy":"2022-05-19T09:23:24.420575Z","iopub.status.idle":"2022-05-19T09:24:38.307486Z","shell.execute_reply":"2022-05-19T09:24:38.306457Z","shell.execute_reply.started":"2022-05-19T09:23:24.420866Z"},"trusted":true},"outputs":[],"source":["!pip install autokeras\n","import autokeras\n","from autokeras import StructuredDataClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:24:38.309857Z","iopub.status.busy":"2022-05-19T09:24:38.309625Z","iopub.status.idle":"2022-05-19T09:24:38.313629Z","shell.execute_reply":"2022-05-19T09:24:38.312823Z","shell.execute_reply.started":"2022-05-19T09:24:38.30983Z"},"trusted":true},"outputs":[],"source":["### Dictionary for getting Accuracies:\n","accuracy={}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:24:38.315825Z","iopub.status.busy":"2022-05-19T09:24:38.314984Z","iopub.status.idle":"2022-05-19T09:24:38.335185Z","shell.execute_reply":"2022-05-19T09:24:38.334213Z","shell.execute_reply.started":"2022-05-19T09:24:38.315777Z"},"trusted":true},"outputs":[],"source":["### Copied from above section:\n","def initial(df):\n","    \n","    #### Label Encoding the Target Variable\n","\n","    X=df.drop([\"label\"],axis=1)\n","    y=df[\"label\"]\n","    if df.label.dtype==str:   ### Will apply label encoding if needed\n","        le=LabelEncoder()\n","        y=le.fit_transform(y)\n","        y=pd.Series(y)\n","\n","    #### Removing Features having zero variance.\n","\n","    Var=X[X.columns].std()\n","    col=Var[Var==0].index\n","    X=X.drop(col,axis=1)\n","    \n","    return X,y"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:24:38.337899Z","iopub.status.busy":"2022-05-19T09:24:38.337565Z","iopub.status.idle":"2022-05-19T09:24:38.349942Z","shell.execute_reply":"2022-05-19T09:24:38.349321Z","shell.execute_reply.started":"2022-05-19T09:24:38.337858Z"},"trusted":true},"outputs":[],"source":["def funct_FS(ds,fs_name,n,X,y):\n","    \n","    fs_dic={\"MI\":SelectKBest(mutual_info_classif, k=n),\"CHI2\":SelectKBest(chi2, k=n),\"ANOVA\":SelectKBest(f_classif, k=n),\"RFE\":RFE(estimator=CatBoostClassifier(), n_features_to_select=n)}    \n","    ### for Chi2 feature selection, data points must be strictly positive.    \n","    if fs_name==\"CHI2\":\n","        pipe = Pipeline([('scaler', MinMaxScaler()),\n","                 ('selector', fs_dic[fs_name])])\n","    \n","    else:\n","        pipe = Pipeline([('scaler', StandardScaler()),\n","                 ('selector', fs_dic[fs_name])])    \n","    print(\"Pipeline set up\")\n","    \n","    pipe.fit(X, y)\n","    # Get columns to keep and create new dataframe with those only\n","    cols = pipe.named_steps['selector'].get_support(indices=True) ### Note the format\n","    X_fs= X.iloc[:,cols]\n","\n","    df_fs=pd.concat([X_fs,y],axis=1)\n","    df_fs.rename({\"0\":\"label\"},axis=1,inplace=True)\n","    df_fs.to_csv(f\"{ds}_{fs_name}_{n}_features.csv\",index=False)    \n","    return df_fs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:24:38.352176Z","iopub.status.busy":"2022-05-19T09:24:38.351892Z","iopub.status.idle":"2022-05-19T09:24:38.3632Z","shell.execute_reply":"2022-05-19T09:24:38.361997Z","shell.execute_reply.started":"2022-05-19T09:24:38.352147Z"},"trusted":true},"outputs":[],"source":["def funct_autokeras_accuracy(X_train, X_test, y_train, y_test,max_trials,data,feature_count):\n","    \n","    search = StructuredDataClassifier(max_trials=max_trials)\n","    \n","    # perform the search\n","    search.fit(x=X_train, y=y_train, verbose=0)\n","    \n","    # evaluate the model\n","    loss, acc = search.evaluate(X_test, y_test, verbose=0)\n","    accuracy[f\"{data}_{feature_count}_{max_trials}\"]=round(acc, 3)     \n","    \n","    # get the best performing model\n","    model = search.export_model()   \n","    \n","    # save the best performing model to file (Autokeras Model fails to save as h5 file)\n","    try:\n","        model.save(f\"{data}_{feature_count}_{max_trials}\", save_format=\"tf\")\n","    except Exception:\n","        model.save(f\"{data}_{feature_count}_{max_trials}.h5\")"]},{"cell_type":"markdown","metadata":{},"source":["Function to find out best model using autokeras library.\n","\n","Parameters description: feature_count - no of features to be selected\n","\n","                    max_trials - for the autokeras\n","                        \n","                    data -  EMG/NEMG                     "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:28:57.271597Z","iopub.status.busy":"2022-05-19T09:28:57.270693Z","iopub.status.idle":"2022-05-19T09:28:57.280852Z","shell.execute_reply":"2022-05-19T09:28:57.279781Z","shell.execute_reply.started":"2022-05-19T09:28:57.271548Z"},"trusted":true},"outputs":[],"source":["def funct_autokeras(feature_count,max_trials,data,X,y):    ### data=\"EMG\"/\"NEMG\"/\"Both\" ### feature_count=NULL in case of \"Both\"\n","    if feature_count!=\"None\":\n","        df_fs=funct_FS(data,\"ANOVA\",feature_count,X,y)  ### fs_name=\"ANOVA\"  ### (ds,fs_name,n,X,y)\n","        df_fs.rename({0:\"label\"},axis=1,inplace=True)        \n","        X=df_fs.drop([\"label\"],axis=1)\n","        y=df_fs[\"label\"]\n","    else:\n","        X=X\n","        y=y\n","\n","    ### Splitting the data:\n","    X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.3, random_state=1)\n","    \n","    ### Scaling the Data:\n","    sc=StandardScaler()\n","    X_train_scaled=sc.fit_transform(X_train)\n","    X_test_scaled=sc.transform(X_test)    \n","    \n","    funct_autokeras_accuracy(X_train_scaled, X_test_scaled, y_train, y_test,max_trials,data,feature_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:27:00.693585Z","iopub.status.busy":"2022-05-19T09:27:00.693291Z","iopub.status.idle":"2022-05-19T09:27:09.701937Z","shell.execute_reply":"2022-05-19T09:27:09.701033Z","shell.execute_reply.started":"2022-05-19T09:27:00.693556Z"},"trusted":true},"outputs":[],"source":["#Debugging\n","#df_fs=funct_FS(\"EMG\",\"ANOVA\",100,X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:24:38.382785Z","iopub.status.busy":"2022-05-19T09:24:38.381929Z","iopub.status.idle":"2022-05-19T09:25:57.32015Z","shell.execute_reply":"2022-05-19T09:25:57.318792Z","shell.execute_reply.started":"2022-05-19T09:24:38.382751Z"},"trusted":true},"outputs":[],"source":["df_ori=pd.read_csv(\"../input/imu-for-feature-selection/Combined_EMG.csv\")  ### NEMG/EMG"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:25:57.322602Z","iopub.status.busy":"2022-05-19T09:25:57.322322Z","iopub.status.idle":"2022-05-19T09:26:05.017975Z","shell.execute_reply":"2022-05-19T09:26:05.016883Z","shell.execute_reply.started":"2022-05-19T09:25:57.322566Z"},"trusted":true},"outputs":[],"source":["X,y=initial(df_ori)   ### Remains Fixed"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T09:29:01.115127Z","iopub.status.busy":"2022-05-19T09:29:01.11483Z","iopub.status.idle":"2022-05-19T09:29:37.068429Z","shell.execute_reply":"2022-05-19T09:29:37.067016Z","shell.execute_reply.started":"2022-05-19T09:29:01.115097Z"},"trusted":true},"outputs":[],"source":["### Iterating through various values:\n","for feature_count in range(100,301,100): ### No of features to be selected\n","    for max_trials in range(16,21):  ### Trial count for autokeras\n","        funct_autokeras(feature_count,max_trials,\"EMG\",X,y)    \n","    print(f\"Done for {feature_count}!\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#### To store the results in the form of json that is prettified:\n","with open('accuracy_results.json', 'w') as fp:\n","    json.dump(accuracy, fp,  indent=4)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T06:39:38.249559Z","iopub.status.busy":"2022-06-11T06:39:38.249295Z","iopub.status.idle":"2022-06-11T06:39:40.240188Z","shell.execute_reply":"2022-06-11T06:39:40.239450Z","shell.execute_reply.started":"2022-06-11T06:39:38.249530Z"},"trusted":true},"outputs":[],"source":["### For the best model using EMG and NEMG Features\n","df_emg=pd.read_csv(\"../input/imu-top-200-features/EMG_ANOVA_200_features.csv\")\n","df_nemg=pd.read_csv(\"../input/imu-top-200-features/NEMG_ANOVA_200_features.csv\")\n","df_emg.rename({\"0\":\"label\"},axis=1,inplace=True)\n","df_nemg.rename({\"0\":\"label\"},axis=1,inplace=True)\n","\n","## Combining EMG and IMU files:\n","temp=pd.DataFrame()\n","for i in range(26):\n","    n=df_nemg[df_nemg[\"label\"]==i].reset_index(drop=True)\n","    e=df_emg[df_emg[\"label\"]==i].reset_index(drop=True).drop([\"label\"],axis=1)[0:n.shape[0]]\n","    k=pd.concat([e,n],axis=1)\n","    temp=pd.concat([temp,k],axis=0)   "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Trial 1:\n","accuracy={}\n","X,y=initial(temp)   ### Remains Fixed\n","### Iterating through various values:\n","for max_trials in range(16,25):  ### Trial count for autokeras\n","    funct_autokeras(\"None\",max_trials,\"Both\",X,y)  \n","    print(f\"Done for {max_trials}!\")\n","    \n","#### To store the results in the form of json that is prettified:\n","with open('accuracy_results.json', 'w') as fp:\n","    json.dump(accuracy, fp,  indent=4)\n","\n","### End Result:\n","# {\n","#     \"Both_Null_16\": 0.953,\n","#     \"Both_Null_17\": 0.943,\n","#     \"Both_Null_18\": 0.949,\n","#     \"Both_Null_19\": 0.953,\n","#     \"Both_Null_20\": 0.955,\n","#     \"Both_Null_21\": 0.949,\n","#     \"Both_Null_22\": 0.954,\n","#     \"Both_Null_23\": 0.951,\n","#     \"Both_Null_24\": 0.955\n","# }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Trial 2:\n","accuracy={}\n","X,y=initial(temp)   ### Remains Fixed\n","### Iterating through various values:\n","for max_trials in range(20,41,5):  ### Trial count for autokeras\n","    funct_autokeras(\"Null\",max_trials,\"Both\",X,y)  \n","    print(f\"Done for {max_trials}!\")\n","    \n","#### To store the results in the form of json that is prettified:\n","with open('400 Features Combined 5-40.json', 'w') as fp:\n","    json.dump(accuracy, fp,  indent=4)\n","\n","### End Result:\n","# {\n","#     \"Both_Null_20\": 0.075,\n","#     \"Both_Null_25\": 0.913,\n","#     \"Both_Null_30\": 0.932,\n","#     \"Both_Null_35\": 0.948,\n","#     \"Both_Null_40\": 0.276\n","# }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## IMU Best Model Try:\n","accuracy={}\n","X,y=initial(df_nemg)   ### Remains Fixed\n","### Iterating through various values:\n","for max_trials in range(10,30):  ### Trial count for autokeras\n","    funct_autokeras(\"Null\",max_trials,\"IMU\",X,y)  \n","    print(f\"Done for {max_trials}!\")\n","    \n","#### To store the results in the form of json that is prettified:\n","with open('accuracy_results_IMU.json', 'w') as fp:\n","    json.dump(accuracy, fp,  indent=4)\n","\n","### End Result:\n","# {\n","#     \"IMU_Null_10\": 0.695,\n","#     \"IMU_Null_11\": 0.68,\n","#     \"IMU_Null_12\": 0.684,\n","#     \"IMU_Null_13\": 0.702,\n","#     \"IMU_Null_14\": 0.718,\n","#     \"IMU_Null_15\": 0.706,\n","#     \"IMU_Null_16\": 0.75,\n","#     \"IMU_Null_17\": 0.702,\n","#     \"IMU_Null_18\": 0.712,\n","#     \"IMU_Null_19\": 0.72,\n","#     \"IMU_Null_20\": 0.716,\n","#     \"IMU_Null_21\": 0.721,\n","#     \"IMU_Null_22\": 0.7,\n","#     \"IMU_Null_23\": 0.712,\n","#     \"IMU_Null_24\": 0.709,\n","#     \"IMU_Null_25\": 0.707,\n","#     \"IMU_Null_26\": 0.732,\n","#     \"IMU_Null_27\": 0.692,\n","#     \"IMU_Null_28\": 0.711,\n","#     \"IMU_Null_29\": 0.703\n","# }"]},{"cell_type":"markdown","metadata":{},"source":["# Loading the Best Base Learner provided by Autokeras:"]},{"cell_type":"markdown","metadata":{},"source":["### 1st time:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Loading the Best Saved Model - max_iterations=24\n","base_model = load_model(\"Trial 1/Base Model\", custom_objects=ak.CUSTOM_OBJECTS)\n","\n","## To get the Best Model summary(Details about its architecture):\n","base_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model: \"model\"\n","# _________________________________________________________________\n","#  Layer (type)                Output Shape              Param #   \n","# =================================================================\n","#  input_1 (InputLayer)        [(None, 400)]             0         \n","                                                                 \n","#  multi_category_encoding (Mu  (None, 400)              0         \n","#  ltiCategoryEncoding)                                            \n","                                                                 \n","#  normalization (Normalizatio  (None, 400)              801       \n","#  n)                                                              \n","                                                                 \n","#  dense (Dense)               (None, 32)                12832     \n","                                                                 \n","#  re_lu (ReLU)                (None, 32)                0         \n","                                                                 \n","#  dense_1 (Dense)             (None, 128)               4224      \n","                                                                 \n","#  re_lu_1 (ReLU)              (None, 128)               0         \n","                                                                 \n","#  dense_2 (Dense)             (None, 26)                3354      \n","                                                                 \n","#  classification_head_1 (Soft  (None, 26)               0         \n","#  max)                                                            \n","                                                                 \n","# =================================================================\n","# Total params: 21,211\n","# Trainable params: 20,410\n","# Non-trainable params: 801\n","# _________________________________________________________________"]},{"cell_type":"markdown","metadata":{},"source":["### 2nd time:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model: \"model\"\n","# _________________________________________________________________\n","#  Layer (type)                Output Shape              Param #   \n","# =================================================================\n","#  input_1 (InputLayer)        [(None, 400)]             0         \n","                                                                 \n","#  multi_category_encoding (Mu  (None, 400)              0         \n","#  ltiCategoryEncoding)                                            \n","                                                                 \n","#  normalization (Normalizatio  (None, 400)              801       \n","#  n)                                                              \n","                                                                 \n","#  dense (Dense)               (None, 1024)              410624    \n","                                                                 \n","#  re_lu (ReLU)                (None, 1024)              0         \n","                                                                 \n","#  dropout (Dropout)           (None, 1024)              0         \n","                                                                 \n","#  dense_1 (Dense)             (None, 512)               524800    \n","                                                                 \n","#  re_lu_1 (ReLU)              (None, 512)               0         \n","                                                                 \n","#  dropout_1 (Dropout)         (None, 512)               0         \n","                                                                 \n","#  dense_2 (Dense)             (None, 512)               262656    \n","                                                                 \n","#  re_lu_2 (ReLU)              (None, 512)               0         \n","                                                                 \n","#  dropout_2 (Dropout)         (None, 512)               0         \n","                                                                 \n","#  dropout_3 (Dropout)         (None, 512)               0         \n","                                                                 \n","#  dense_3 (Dense)             (None, 26)                13338     \n","                                                                 \n","#  classification_head_1 (Soft  (None, 26)               0         \n","#  max)                                                            \n","                                                                 \n","# =================================================================\n","# Total params: 1,212,219\n","# Trainable params: 1,211,418\n","# Non-trainable params: 801\n","# _________________________________________________________________"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Copied from above section:\n","def initial(df):\n","    \n","    #### Label Encoding the Target Variable\n","\n","    X=df.drop([\"label\"],axis=1)\n","    y=df[\"label\"]\n","    if df.label.dtype==str:   ### Will apply label encoding if needed\n","        le=LabelEncoder()\n","        y=le.fit_transform(y)\n","        y=pd.Series(y)\n","\n","    #### Removing Features having zero variance.\n","\n","    Var=X[X.columns].std()\n","    col=Var[Var==0].index\n","    X=X.drop(col,axis=1)\n","    \n","    X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.3, random_state=1)\n","    \n","    ### Scaling the Data:\n","    sc=StandardScaler()\n","    X_train_scaled=sc.fit_transform(X_train)\n","    X_test_scaled=sc.transform(X_test)  \n","    \n","    return X_train_scaled, X_test_scaled, y_train, y_test   ## Changed the return values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### For the best model using EMG and NEMG Features\n","df_emg=pd.read_csv(\"EMG_ANOVA_200_features.csv\")\n","df_nemg=pd.read_csv(\"NEMG_ANOVA_200_features.csv\")\n","df_emg.rename({\"0\":\"label\"},axis=1,inplace=True)\n","df_nemg.rename({\"0\":\"label\"},axis=1,inplace=True)\n","\n","temp=pd.DataFrame()\n","for i in range(26):\n","    n=df_nemg[df_nemg[\"label\"]==i].reset_index(drop=True)\n","    e=df_emg[df_emg[\"label\"]==i].reset_index(drop=True).drop([\"label\"],axis=1)[0:n.shape[0]]\n","    k=pd.concat([e,n],axis=1)\n","    temp=pd.concat([temp,k],axis=0)   \n","\n","X_train_scaled, X_test_scaled, y_train, y_test=initial(temp)   ### Remains Fixed"]},{"cell_type":"markdown","metadata":{},"source":["ADDING NOISE PART:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Adding noise:\n","mu, sigma = 0, 0.5 \n","# creating a noise with the same dimension as the dataset: \n","noise = np.random.normal(mu, sigma, X.shape)\n","\n","X = X + noise"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Building the model above from scratch in TensorFlow:\n","class Model:\n","    def __init__(self,number,l1,l2):\n","        self.l1=l1\n","        self.l2=l2\n","        self.number=number\n","        self.model = tf.keras.Sequential()\n","        self.model.add(tf.keras.layers.InputLayer(input_shape=(400)))\n","        #model.add(tf.keras.layers.Normalization(axis=-1))\n","        self.model.add(tf.keras.layers.Dense(units=self.l1,activation=\"relu\"))\n","        self.model.add(tf.keras.layers.Dense(units=self.l2,activation=\"relu\"))\n","        self.model.add(tf.keras.layers.Dense(units=26,activation=\"softmax\"))\n","        self.model.compile(optimizer=\"adam\",loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=\"accuracy\")\n","    \n","    def funct_fit(self):\n","        self.model.fit(X_train_scaled,y_train,epochs=50,batch_size=50)\n","        self.model.save(f\"{os.getcwd()}/Trial 1/model_{self.number}.h5\")   ### Saves the Model as a h5 file, can be loaded later.\n","#         try:\n","#             self.model.save(f\"{os.getcwd()}/Trial 1/model_{self.number}\", save_format=\"tf\")\n","#         except Exception:         \n","\n","# model=Model(1,64,256)\n","# model.funct_fit()\n","\n","# model=Model(2,96,384)\n","# model.funct_fit()\n","\n","# model=Model(3,128,512)\n","# model.funct_fit()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Deeper Model:\n","class Model:\n","    def __init__(self,number,l1,l2,l3,l4,l5):\n","        self.number=number\n","        self.model = tf.keras.Sequential()\n","        self.model.add(tf.keras.layers.InputLayer(input_shape=(400)))\n","        #model.add(tf.keras.layers.Normalization(axis=-1))\n","        self.model.add(tf.keras.layers.Dense(units=l1,activation=None))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ReLU())\n","        self.model.add(tf.keras.layers.Dropout(0.3))\n","        self.model.add(tf.keras.layers.Dense(units=l2,activation=None))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ReLU())\n","        self.model.add(tf.keras.layers.Dropout(0.3))\n","        self.model.add(tf.keras.layers.Dense(units=l3,activation=None))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ReLU())\n","        self.model.add(tf.keras.layers.Dropout(0.3))\n","        self.model.add(tf.keras.layers.Dense(units=l4,activation=None))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ReLU())\n","        self.model.add(tf.keras.layers.Dropout(0.3))\n","        self.model.add(tf.keras.layers.Dense(units=l5,activation=None))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ReLU())\n","        self.model.add(tf.keras.layers.Dropout(0.3))\n","        self.model.add(tf.keras.layers.Dense(units=26,activation=\"softmax\"))\n","        self.model.compile(optimizer=\"adam\",loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=\"accuracy\")\n","    \n","    def funct_fit(self):\n","        self.model.fit(X_train_scaled,y_train,validation_data=(X_test_scaled,y_test),epochs=50,batch_size=50)\n","        self.model.save(f\"{os.getcwd()}/model_{self.number}.h5\")\n","#         try:\n","#             self.model.save(f\"{os.getcwd()}/Trial 1/model_{self.number}\", save_format=\"tf\")\n","#         except Exception:\n","            \n","# model=Model(1,64,256,128,512,1024)\n","# model.funct_fit()\n","# model=Model(2,32,128,200,256,512)\n","# model.funct_fit()\n","# model=Model(3,100,300,600,900,1600)\n","# model.funct_fit()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["allmodels=[base_model]\n","\n","#### Loading other models:\n","def funct_load(number):\n","    for i in range(1,number+1):\n","        # load model from file\n","        model = load_model(f'Trial 1/model_{i}.h5')\n","        # add to list of members\n","        allmodels.append(model)\n","            \n","funct_load(3)    \n","allmodels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# [<keras.engine.functional.Functional at 0x7fcf70f15eb8>,\n","#  <keras.engine.sequential.Sequential at 0x7fcf5a26e550>,\n","#  <keras.engine.sequential.Sequential at 0x7fcf58844b38>,\n","#  <keras.engine.sequential.Sequential at 0x7fcf587b8b70>]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create stacked model input dataset as outputs from the ensemble\n","def stacked_dataset(members, X_test_scaled):\n","    stackX = None\n","    for model in allmodels:\n","        # make prediction\n","        yhat = model.predict(X_test_scaled, verbose=0)\n","        # stack predictions into [rows, members, class]\n","        if stackX is None:\n","            stackX = yhat\n","        else:\n","            stackX = np.dstack((stackX, yhat))\n","    # flatten predictions to [rows, members x class]\n","    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n","    return stackX"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit a model based on the outputs from the ensemble members\n","def fit_stacked_model(members, X_test_scaled, y_test,model_name):\n","    # create dataset using ensemble\n","    stackedX = stacked_dataset(members, X_test_scaled)\n","    # fit standalone model\n","    model = model_name\n","    model.fit(stackedX, y_test)\n","    return model\n"," \n","# Make a prediction with the stacked model\n","def stacked_prediction(members, model, X_test_scaled):\n","    # create dataset using ensemble\n","    stackedX = stacked_dataset(members, X_test_scaled)\n","    # make a prediction\n","    yhat = model.predict(stackedX)\n","    return yhat"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# To evaluate standalone models on test dataset:\n","for model in allmodels:\n","    y_hat=model.predict(X_test_scaled)\n","    y_hat=np.argmax(y_hat,axis=1)\n","    acc = accuracy_score(y_test, y_hat)\n","    print('Model Accuracy: %.3f' % acc)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Function to create csv file of classification report:\n","def funct_report_csv(y,y_hat):\n","    clf_rep = precision_recall_fscore_support(y, y_hat)\n","    out_dict = {\n","                 \"precision\" :clf_rep[0].round(2)\n","                ,\"recall\" : clf_rep[1].round(2)\n","                ,\"f1-score\" : clf_rep[2].round(2)\n","                ,\"support\" : clf_rep[3]\n","                }\n","    out_df = pd.DataFrame(out_dict)\n","    avg_tot = (out_df.apply(lambda x: round(x.mean(), 2) if x.name!=\"support\" else  round(x.sum(), 2)).to_frame().T)\n","    avg_tot.index = [\"avg/total\"]\n","    out_df = out_df.append(avg_tot)\n","    return out_df\n","\n","def plot(y,y_hat,model):\n","    #report=classification_report(y, y_hat,output_dict=True) # print classification report\n","    report=funct_report_csv(y, y_hat) # print classification report\n","    report.to_csv(f\"{os.getcwd}/CR_without_noise/Classification_Report_without_noise_{model}.csv\",index=False)\n","    cm=confusion_matrix(y,y_hat)\n","    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # For normalising the Matrix for better visualisation.\n","    plt.figure(figsize=(20,10))\n","    plt.rc(\"font\",size=10)\n","    sns.heatmap(cm,annot=True,fmt=\".2f\",cmap=\"viridis\")\n","    plt.savefig(f\"{os.getcwd()}/Plots_without_noise/{model}_without_noise.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Uses various machine learning models at LEVEL 1, to test for best accuracy:\n","def stacked_random_models_test(allmodels,X,y):\n","    sample={\"XGB\":XGBClassifier(),\"LGBM\":LGBMClassifier(),\"RandomForest\":RandomForestClassifier(),\"LR\":LogisticRegression(),\"CatBoost\":CatBoostClassifier()}\n","    for i,j in sample.items():\n","        model = fit_stacked_model(allmodels, X, y,j)\n","        yhat = stacked_prediction(allmodels, model, X_test_scaled)\n","        acc = accuracy_score(y, yhat)\n","        results[i]=round(acc,3)\n","    \n","    #### To store the results in the form of json that is prettified:\n","    with open('accuracy_results_stacked.json', 'w') as fp:\n","        json.dump(results, fp,  indent=4)        "]},{"cell_type":"markdown","metadata":{},"source":["# Stacked Ensemble Redone::"]},{"cell_type":"markdown","metadata":{},"source":["Preprocessing the data:\n","- This time the data was split into train and test (test_size=0.2)  {Two ways were tried, Random Sampling and Stratified Sampling}\n","- Standardised the test data on the basis of train data \n","- Saved the UnScaled train data and the Scaled data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Label Encoding, Removing Zero variance Features and Scaling the test data::\n","def initial(df):\n","    \n","    #### Label Encoding the Target Variable\n","\n","    X=df.drop([\"label\"],axis=1)\n","    y=df[\"label\"]\n","    if df.label.dtype==str:   ### Will apply label encoding if needed\n","        le=LabelEncoder()\n","        y=le.fit_transform(y)\n","        y=pd.Series(y)\n","\n","    #### Removing Features having zero variance.\n","\n","    Var=X[X.columns].std()\n","    col=Var[Var==0].index\n","    X=X.drop(col,axis=1)\n","    \n","    X, X_val, y, y_val = train_test_split(X, y,stratify=y,test_size=0.80, random_state=None)  ### For stratified sampling\n","    #X, X_val, y, y_val = train_test_split(X, y,stratify=y,test_size=0.80, random_state=None)  ### For random sampling\n","    \n","    ### Scaling the Data:\n","    sc=StandardScaler()\n","    X_sc=sc.fit_transform(X)\n","    X_val=sc.transform(X_val)  \n","    \n","    return X.reset_index(drop=True), pd.DataFrame(X_val,columns=X.columns), y.reset_index(drop=True), y_val.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### For the best model using EMG and NEMG Features\n","df_emg=pd.read_csv(\"EMG_ANOVA_200_features.csv\").rename({'0':'label'},axis=1)\n","df_nemg=pd.read_csv(\"NEMG_ANOVA_200_features.csv\").rename({\"0\":\"label\"},axis=1)\n","\n","temp=pd.DataFrame()\n","for i in range(26):\n","    n=df_nemg[df_nemg[\"label\"]==i].reset_index(drop=True)\n","    e=df_emg[df_emg[\"label\"]==i].reset_index(drop=True).drop([\"label\"],axis=1)[0:n.shape[0]]\n","    k=pd.concat([e,n],axis=1)\n","    temp=pd.concat([temp,k],axis=0)   "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X, X_val, y, y_val=initial(temp)   ### Remains Fixed\n","\n","## X,y ----> Training data\n","## X_val,y_val ----> Testing data   Which is being stored for training and evaluating the other level 0 models\n","X.shape,X_val.shape\n","# ((4736, 400), (18944, 400))\n","y.shape,y_val.shape\n","# ((4736,), (18944,))\n","\n","\n","train=pd.concat([X,y],axis=1)\n","test=pd.concat([X_val,y_val],axis=1)\n","\n","train.shape,test.shape\n","# ((4736, 401), (18944, 401))\n","\n","test.to_csv(\"Test_data_str.csv\",index=False)    ### Scaled Test Data\n","train.to_csv(\"Train_data_str.csv\",index=False)  ### Unscaled Train Data"]},{"cell_type":"markdown","metadata":{},"source":["Now Reading the saved Unscaled training data and used autokeras with max_iterations=28 to reach to the best model::"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Label Encoding and Removing Zero variance Features\n","def initial1(df):\n","    \n","    #### Label Encoding the Target Variable\n","\n","    X=df.drop([\"label\"],axis=1)\n","    y=df[\"label\"]\n","    if df.label.dtype==str:   ### Will apply label encoding if needed\n","        le=LabelEncoder()\n","        y=le.fit_transform(y)\n","        y=pd.Series(y)\n","\n","    #### Removing Features having zero variance.\n","\n","    Var=X[X.columns].std()\n","    col=Var[Var==0].index\n","    X=X.drop(col,axis=1)\n","    \n","    return X,y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def funct_autokeras_accuracy(X_train, X_test, y_train, y_test,max_trials,data):\n","    \n","    search = StructuredDataClassifier(max_trials=max_trials)\n","    \n","    # perform the search\n","    search.fit(x=X_train, y=y_train, verbose=0)\n","    \n","    # evaluate the model\n","    loss, acc = search.evaluate(X_test, y_test, verbose=0)\n","    accuracy[f\"{data}_{max_trials}\"]=round(acc, 3)    \n","    \n","    # get the best performing model\n","    model = search.export_model()   \n","    \n","    # save the best performing model to file\n","    try:\n","        model.save(f\"{data}_{max_trials}_strat\", save_format=\"tf\")\n","    except Exception:\n","        model.save(f\"{data}_{max_trials}_strat.h5\")\n","\n","\n","def funct_autokeras(max_trials,data,X,y):    ### data=\"EMG\"/\"NEMG\"/\"Both\" ### feature_count=NULL in case of \"Both\"\n","    X=X\n","    y=y\n","\n","    ### Splitting the data:\n","    X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.3, random_state=1)\n","    \n","    ### Scaling the Data:\n","    sc=StandardScaler()\n","    X_train_scaled=sc.fit_transform(X_train)\n","    X_test_scaled=sc.transform(X_test)    \n","    \n","    funct_autokeras_accuracy(X_train_scaled, X_test_scaled, y_train, y_test,max_trials,data)\n","\n","\n","accuracy={}\n","df=pd.read_csv(\"Train_data_random.csv\").rename({'0':'label'},axis=1)\n","X,y=initial1(df)   ### Remains Fixed\n","### Iterating through various values:\n","funct_autokeras(28,\"Both\",X,y) \n","    \n","with open('accuracy_results_stacked_28_strat.json', 'w') as fp:\n","    json.dump(accuracy, fp,  indent=4) "]},{"cell_type":"markdown","metadata":{},"source":["Creating the Architecture of Best Model on Tensorflow:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["base_model = load_model(\"Both_28_strat\", custom_objects=ak.CUSTOM_OBJECTS)\n","base_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Model:\n","    def __init__(self,number,l1,l2):\n","        self.number=number\n","        self.model = tf.keras.Sequential()\n","        self.model.add(tf.keras.layers.InputLayer(input_shape=(400)))\n","        self.model.add(tf.keras.layers.Normalization(axis=-1))\n","        self.model.add(tf.keras.layers.Dense(units=l1,activation=None))   ###\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ReLU())\n","        self.model.add(tf.keras.layers.Dropout(0.2))\n","        self.model.add(tf.keras.layers.Dense(units=l2,activation=None))  ###\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ReLU())\n","        self.model.add(tf.keras.layers.Dropout(0.2))\n","        #self.model.add(tf.keras.layers.Dropout(0.3))\n","        self.model.add(tf.keras.layers.Dense(units=26,activation=\"softmax\"))\n","        self.model.compile(optimizer=\"adam\",loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=\"accuracy\")\n","    \n","    def funct_fit(self,X_train,X_test,y_train,y_test,count,n_split):\n","        self.model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=50,batch_size=50)\n","        if count==n_split:\n","            self.model.save(f\"{os.getcwd()}/model_{self.number}.h5\")\n","        return self.model.history.history[\"accuracy\"],self.model.history.history[\"val_accuracy\"] \n","\n","def funct_avg(d):\n","    temp={}\n","    for i,j in d.items():\n","        temp[i]=np.mean(j)\n","        \n","    return temp\n","\n","### cross validation:\n","def funct_cv(m,n_split,X,y,l1,l2):\n","    dic_results={\"accuracy\":[],\"val_accuracy\":[]}\n","    n_split=n_split\n","    count=0\n","    for train_index,test_index in StratifiedKFold(n_split).split(X,y):\n","        X_train,X_test=X[train_index],X[test_index]\n","        y_train,y_test=y[train_index],y[test_index]\n","\n","        sc=StandardScaler()\n","        X_train_scaled=sc.fit_transform(X_train)\n","        X_test_scaled=sc.transform(X_test)  \n","\n","        model=Model(m,l1,l2)   ### creating the model object \n","        count=count+1\n","        acc,val_acc=model.funct_fit(X_train_scaled,X_test_scaled, y_train,y_test,count,n_split)        \n","\n","        dic_results[\"accuracy\"].append(max(acc))\n","        dic_results[\"val_accuracy\"].append(max(val_acc))\n","\n","    dic_results=funct_avg(dic_results)\n","\n","    final_result[m]=dic_results\n","\n","final_result={}\n","funct_cv(1,5,X.values,y.values,60,60)\n","funct_cv(2,5,X.values,y.values,90,90)\n","funct_cv(3,5,X.values,y.values,150,150)\n","\n","with open('accuracy_results_StratifiedCV.json', 'w') as fp:\n","    json.dump(final_result, fp,  indent=4)          "]},{"cell_type":"markdown","metadata":{},"source":["Using Base Models for final Prediction::"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["allmodels=[base_model]\n","\n","#### Loading other models:\n","def funct_load(number):\n","    for i in range(1,number+1):\n","        # load model from file\n","        model = load_model(f'model_{i}.h5')\n","        model.summary()\n","        # add to list of members\n","        allmodels.append(model)\n","            \n","funct_load(3)    \n","\n","df=pd.read_csv(\"/home/sudharshan/Sudharshan/Test_data_str.csv\").rename({'0':'label'},axis=1)\n","X_test,y_test=initial1(df)   ### Remains Fixed\n","\n","\n","# create stacked model input dataset as outputs from the individual ensemble models\n","def stacked_dataset(allmodels, X_test):\n","    stackX = None\n","    for model in allmodels:\n","        # make prediction\n","        yhat = model.predict(X_test, verbose=0)\n","        # stack predictions into [rows, members, class]\n","        if stackX is None:\n","            stackX = yhat\n","        else:\n","            stackX = np.dstack((stackX, yhat))\n","    # flatten predictions to [rows, members x class]\n","    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n","    return stackX\n","\n","stackedX = stacked_dataset(allmodels, X_test)\n","\n","# fit a model based on the outputs from the ensemble members\n","def fit_stacked_model(allmodels, X_test, y_test,model_name):\n","    # create dataset using ensemble\n","    stackedX = stacked_dataset(allmodels, X_test)\n","    # fit standalone model\n","    model = model_name\n","    model.fit(stackedX, y_test)\n","    return model\n"," \n","# make a prediction with the stacked model\n","def stacked_prediction(allmodels, model, X):\n","    # create dataset using ensemble\n","    stackedX = stacked_dataset(allmodels, X)\n","    # make a prediction\n","    yhat = model.predict(stackedX)\n","    return yhat\n","\n","\n","results={}\n","\n","\n","# evaluate standalone models on test dataset\n","for model in allmodels:\n","    y_hat=model.predict(X_test)\n","    y_hat=np.argmax(y_hat,axis=1)\n","    acc = accuracy_score(y_test, y_hat)\n","    print('Model Accuracy: %.6f' % acc)\n","\n","\n","def funct_report_csv(y,y_hat):\n","    clf_rep = precision_recall_fscore_support(y, y_hat)\n","    out_dict = {\n","                 \"precision\" :clf_rep[0].round(2)\n","                ,\"recall\" : clf_rep[1].round(2)\n","                ,\"f1-score\" : clf_rep[2].round(2)\n","                ,\"support\" : clf_rep[3]\n","                }\n","    out_df = pd.DataFrame(out_dict)\n","    avg_tot = (out_df.apply(lambda x: round(x.mean(), 2) if x.name!=\"support\" else  round(x.sum(), 2)).to_frame().T)\n","    avg_tot.index = [\"avg/total\"]\n","    out_df = out_df.append(avg_tot)\n","    return out_df\n","\n","def plot(y,y_hat,model):\n","    #report=classification_report(y, y_hat,output_dict=True) # print classification report\n","    report=funct_report_csv(y, y_hat) # print classification report\n","    report.to_csv(f\"Classification_Report_{model}.csv\",index=False)\n","    cm=confusion_matrix(y,y_hat)\n","    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # For normalising the Matrix for better visualisation.\n","    plt.figure(figsize=(20,10))\n","    plt.rc(\"font\",size=10)\n","    sns.heatmap(cm,annot=True,fmt=\".2f\",cmap=\"viridis\")\n","    plt.savefig(f\"Confusion_Matrix_{model}.png\")\n","\n","\n","def stacked_model_test(allmodels,X,y):\n","    sample={\"XGB\":XGBClassifier(),\"LGBM\":LGBMClassifier(),\"RandomForest\":RandomForestClassifier(),\"LR\":LogisticRegression(),\"CatBoost\":CatBoostClassifier(),\"Naive Bayes\":GaussianNB()}\n","    for i,j in sample.items():\n","        model = fit_stacked_model(allmodels, X, y,j)\n","        yhat = stacked_prediction(allmodels, model, X)\n","        acc = accuracy_score(y, yhat)\n","        \n","        ### Plotting the curves:\n","        plot(y,y_hat,i)\n","        \n","        results[i]=round(acc,5)\n","           \n","\n","stacked_model_test(allmodels,X_test,y_test)\n","#### To store the results in the form of json that is prettified:\n","with open('accuracy_results_final_stacked_random-sampling.json', 'w') as fp:\n","    json.dump(results, fp,  indent=4)     "]},{"cell_type":"markdown","metadata":{},"source":["# Stacked AutoEncoder:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### To define the Dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class AE:\n","  def __init__(self,n_inputs):\n","    # define encoder\n","    self.visible = Input(shape=(n_inputs,))\n","\n","    # encoder - level 1\n","    self.e1 = Dense(n_inputs*2)(self.visible)    \n","    self.e1 = BatchNormalization()(self.e1)     ### Please Note, BN is applied before linear activation functions.(Large values passed into Linear Activation\n","    self.e1 = ReLU()(self.e1)                   ### will become inflated and we dont want exploding gradients)\n","    \n","    # encoder - level 2\n","    self.e2 = Dense(n_inputs)(self.e1)\n","    self.e2 = BatchNormalization()(self.e2)\n","    self.e2 = ReLU()(self.e2)\n","    \n","    # bottleneck\n","    # n_bottleneck = n_inputs\n","    # self.bottleneck = Dense(n_bottleneck)(self.e2)\n","    \n","    n_bottleneck = round(float(n_inputs) / 2.0)\n","    self.bottleneck = Dense(n_bottleneck)(self.e2)\n","    \n","    # decoder - level 1\n","    self.d1 = Dense(n_inputs)(self.bottleneck)\n","    self.d1 = BatchNormalization()(self.d1)\n","    self.d1 = ReLU()(self.d1)\n","    \n","    # decoder - level 2\n","    self.d2 = Dense(n_inputs*2)(self.d1)\n","    self.d2 = BatchNormalization()(self.d2)\n","    self.d2 = ReLU()(self.d2)\n","\n","    # output layer\n","    self.output = Dense(n_inputs, activation='linear')(self.d2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def graph_plot(history):\n","  fig, ax = plt.subplots(1,2,figsize=(20,8))\n","  # plot loss\n","  ax[0].set_title(\"Loss Plot\")\n","  ax[0].plot(history.history['loss'], label='train')\n","  ax[0].plot(history.history['val_loss'], label='test')\n","  ax[0].legend()\n","\n","  # plot accuracy\n","  ax[1].set_title(\"Accuracy Plot\")\n","  ax[1].plot(history.history['accuracy'], label='train')\n","  ax[1].plot(history.history['val_accuracy'], label='test')\n","  ax[1].legend()\n","\n","  plt.show()\n","\n","def funct_AE(X_train,X_test,y_train,y_test):\n","  ### Defining the AE object to be passed into the Model:\n","  n_inputs=X_train.shape[1]\n","  ae=AE(n_inputs)  ### The n_inputs is the number of features supplied to the stacked autoencoder\n","\n","  # define autoencoder model\n","  model = Model(inputs=ae.visible, outputs=ae.output)\n","\n","  # compile autoencoder model\n","  model.compile(optimizer='adam', loss='mse',metrics='accuracy')\n","\n","  # plot the autoencoder\n","  # plot_model(model, 'autoencoder_no_compress.png', show_shapes=True)\n","\n","  # fit the autoencoder model to reconstruct input\n","  history = model.fit(X_train, X_train, epochs=100, batch_size=16, verbose=2, validation_data=(X_test,X_test))\n","\n","  graph_plot(history)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["funct_AE(X_train,X_test,y_train,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define an encoder model (without the decoder)\n","encoder = Model(inputs=ae.visible, outputs=ae.bottleneck)\n","plot_model(encoder, 'encoder_no_compress.png', show_shapes=True)\n","\n","# save the encoder to file\n","#encoder.save('encoder.h5')"]},{"cell_type":"markdown","metadata":{},"source":["# Multi Modal AutoEncoder:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Defining the Dataset from Google Drive:\n","emg=pd.read_csv(\"/content/drive/MyDrive/HAR Datasets/EMG_ANOVA_200_features.csv\").rename({\"0\":\"label\"},axis=1)\n","imu=pd.read_csv(\"/content/drive/MyDrive/HAR Datasets/NEMG_ANOVA_200_features.csv\").rename({\"0\":\"label\"},axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["temp_e=pd.DataFrame()\n","for i in range(26):\n","    e=emg[emg[\"label\"]==i].reset_index(drop=True)[0:len(imu[imu[\"label\"]==i])]\n","    temp_e=pd.concat([temp_e,e])\n","\n","X_train_emg, X_test_emg,_,_ = initial(temp_e)\n","X_train_imu, X_test_imu,_,_ = initial(imu)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### To store the accuracy and loss results:\n","results={}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#### For Plotting the Accuracy/Loss vs Epochs:\n","\n","def funct_plots(history):\n","  fig, ax = plt.subplots(1,2,figsize=(20,8))\n","  # plot loss\n","  ax[0].set_title(\"Loss Plot for EMG\")\n","  ax[0].plot(history.history['output_emg_loss'], label='train')\n","  ax[0].plot(history.history['val_output_emg_loss'], label='test')\n","  ax[0].legend()\n","\n","  # plot accuracy\n","  ax[1].set_title(\"Accuracy Plot for EMG\")\n","  ax[1].plot(history.history['output_emg_accuracy'], label='train')\n","  ax[1].plot(history.history['val_output_emg_accuracy'], label='test')\n","  ax[1].legend()\n","\n","  plt.show()\n","\n","  fig, ax = plt.subplots(1,2,figsize=(20,8))\n","  # plot loss\n","  ax[0].set_title(\"Loss Plot for IMU\")\n","  ax[0].plot(history.history['output_imu_loss'], label='train')\n","  ax[0].plot(history.history['val_output_imu_loss'], label='test')\n","  ax[0].legend()\n","\n","  # plot accuracy\n","  ax[1].set_title(\"Accuracy Plot for IMU\")\n","  ax[1].plot(history.history['output_imu_accuracy'], label='train')\n","  ax[1].plot(history.history['val_output_imu_accuracy'], label='test')\n","  ax[1].legend()\n","\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ### Deep Encoder:\n","# encoded2 = Dense(150, activation='relu')(Concat_X_X1)\n","# encoded1 = Dense(100, activation='relu')(encoded2)\n","# encoded0 = Dense(70, activation='relu')(encoded1)\n","\n","# ### Bottle Neck Layer\n","# n_bottleneck = 50\n","# bottleneck = Dense(n_bottleneck,activation='relu')(encoded0)\n","\n","# ### Deep Decoder:\n","# decoded0 = Dense(70, activation='relu')(Concat_X_X1)\n","# decoded1 = Dense(100, activation='relu')(encoded2)\n","# decoded2 = Dense(150, activation='relu')(encoded1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ### Combination 1:\n","# ### Initial Outer Encoder Layers:\n","# input_emg = Input(shape=(200),name='input_emg')\n","# e_o_emg = Dense(100,activation='relu',name='e_o_emg')(input_emg)\n","# e_o1_emg = Dense(50,activation='relu',name='e_o1_emg')(e_o_emg)\n","# latent_emg= Dense(30,activation='relu',name='latent_emg')(e_o1_emg)\n","\n","# input_imu = Input(shape=(200),name='input_imu')\n","# e_o_imu = Dense(100,activation='relu',name='e_o_imu')(input_imu)\n","# e_o1_imu = Dense(50,activation='relu',name='e_o1_imu')(e_o_imu)\n","# latent_imu= Dense(30,activation='relu',name='latent_imu')(e_o1_imu)\n","\n","# ### Concatenations of the outputs:\n","# Concat_emg_imu = Concatenate(name=\"concat\")([latent_emg, latent_imu])  \n","\n","# ### Final Outer Decoder Layers:\n","# d_o2_emg = Dense(50,activation='relu',name='d_o2_emg')(Concat_emg_imu)\n","# d_o1_emg= Dense(100,activation='relu',name='d_o1_emg')(d_o2_emg)\n","# output_emg = Dense(200,activation='linear',name='output_emg')(d_o1_emg)\n","\n","# d_o2_imu = Dense(50,activation='relu',name='d_o2_imu')(Concat_emg_imu)\n","# d_o1_imu = Dense(100,activation='relu',name='d_o1_imu')(d_o2_imu)\n","# output_imu = Dense(200,activation='linear',name='output_imu')(d_o1_imu)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Combination 1:\n","### Initial Outer Encoder Layers:\n","def model1():\n","  input_emg = Input(shape=(200),name='input_emg')\n","  e_o_emg = Dense(500,activation='relu',name='e_o1_emg')(input_emg)\n","  e_o_emg = Dense(300,activation='relu',name='e_o2_emg')(e_o_emg)\n","  e_o_emg = Dense(200,activation='relu',name='e_o3_emg')(e_o_emg)\n","  e_o1_emg = Dense(100,activation='relu',name='e_o4_emg')(e_o_emg)\n","  latent_emg= Dense(50,activation='relu',name='latent_emg')(e_o1_emg)\n","\n","  input_imu = Input(shape=(200),name='input_imu')\n","  e_o_imu = Dense(500,activation='relu',name='e_o1_imu')(input_imu)\n","  e_o_imu = Dense(300,activation='relu',name='e_o2_imu')(e_o_imu)\n","  e_o_imu = Dense(200,activation='relu',name='e_o3_imu')(e_o_imu)\n","  e_o1_imu = Dense(100,activation='relu',name='e_o4_imu')(e_o_imu)\n","  latent_imu= Dense(50,activation='relu',name='latent_imu')(e_o1_imu)\n","\n","  ### Concatenations of the outputs:\n","  Concat_emg_imu = Concatenate(name=\"concat\")([latent_emg, latent_imu])  \n","\n","  ### Final Outer Decoder Layers:\n","  d_o2_emg = Dense(100,activation='relu',name='d_o1_emg')(Concat_emg_imu)\n","  d_o2_emg= Dense(200,activation='relu',name='d_o2_emg')(d_o2_emg)\n","  d_o2_emg= Dense(300,activation='relu',name='d_o3_emg')(d_o2_emg)\n","  d_o2_emg= Dense(500,activation='relu',name='d_o4_emg')(d_o2_emg)\n","  output_emg = Dense(200,activation='linear',name='output_emg')(d_o2_emg)\n","\n","  d_o2_imu = Dense(100,activation='relu',name='d_o1_imu')(Concat_emg_imu)\n","  d_o1_imu = Dense(200,activation='relu',name='d_o2_imu')(d_o2_imu)\n","  d_o1_imu = Dense(300,activation='relu',name='d_o3_imu')(d_o1_imu)\n","  d_o1_imu = Dense(500,activation='relu',name='d_o4_imu')(d_o1_imu)\n","  output_imu = Dense(200,activation='linear',name='output_imu')(d_o1_imu)\n","\n","  mmae = Model([input_emg, input_imu], [output_emg, output_imu], name='multi_modal_autoencoder_1')\n","\n","  mmae.compile(optimizer=Adam(lr=0.001),metrics='accuracy',loss='mse')\n","\n","  history = mmae.fit([X_train_emg,X_train_imu], [X_train_emg,X_train_imu], epochs=100, batch_size=50,\n","                                   validation_data=([X_test_emg,X_test_imu],[X_test_emg,X_test_imu]))\n","  \n","  results[\"model1\"]={\"output_imu_accuracy\":max(history.history['output_imu_accuracy']),\n","                     \"val_output_imu_accuracy\":max(history.history['val_output_imu_accuracy']),\n","                     \"output_emg_accuracy\":max(history.history['output_emg_accuracy']),\n","                     \"val_output_emg_accuracy\":max(history.history['val_output_emg_accuracy'])}\n","\n","  funct_plots(history)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model1()\n","#### Check the png files res11 and res12 for the results."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Combination 1:\n","### Initial Outer Encoder Layers:\n","def model2():\n","  input_emg = Input(shape=(200),name='input_emg')\n","  e_o_emg = Dense(500,activation='relu',name='e_o1_emg')(input_emg)\n","  e_o_emg = Dense(300,activation='relu',name='e_o2_emg')(e_o_emg)\n","  e_o_emg = Dense(200,activation='relu',name='e_o3_emg')(e_o_emg)\n","  e_o1_emg = Dense(100,activation='relu',name='e_o4_emg')(e_o_emg)\n","  latent_emg= Dense(50,activation='relu',name='latent_emg')(e_o1_emg)\n","\n","  input_imu = Input(shape=(200),name='input_imu')\n","  e_o_imu = Dense(500,activation='relu',name='e_o1_imu')(input_imu)\n","  e_o_imu = Dense(300,activation='relu',name='e_o2_imu')(e_o_imu)\n","  e_o_imu = Dense(200,activation='relu',name='e_o3_imu')(e_o_imu)\n","  e_o1_imu = Dense(100,activation='relu',name='e_o4_imu')(e_o_imu)\n","  latent_imu= Dense(50,activation='relu',name='latent_imu')(e_o1_imu)\n","\n","  ### Concatenations of the outputs:\n","  Concat_emg_imu = Concatenate(name=\"concat\")([latent_emg, latent_imu])  \n","\n","  ### Deep Encoder:\n","  encoded2 = Dense(150, activation='relu',name='encoded2')(Concat_emg_imu)\n","  encoded1 = Dense(100, activation='relu',name='encoded1')(encoded2)\n","  encoded0 = Dense(70, activation='relu',name='encoded0')(encoded1)\n","\n","  ### Bottle Neck Layer\n","  n_bottleneck = 50\n","  bottleneck = Dense(n_bottleneck,activation='relu')(encoded0)\n","\n","  ### Deep Decoder:\n","  decoded0 = Dense(70, activation='relu',name='decoded0')(bottleneck)\n","  decoded1 = Dense(100, activation='relu',name='decoded1')(decoded0)\n","  decoded2 = Dense(150, activation='relu',name='decoded2')(decoded1)\n","\n","  ### Final Outer Decoder Layers:\n","  d_o2_emg = Dense(100,activation='relu',name='d_o1_emg')(decoded2)\n","  d_o2_emg= Dense(200,activation='relu',name='d_o2_emg')(d_o2_emg)\n","  d_o2_emg= Dense(300,activation='relu',name='d_o3_emg')(d_o2_emg)\n","  d_o2_emg= Dense(500,activation='relu',name='d_o4_emg')(d_o2_emg)\n","  output_emg = Dense(200,activation='linear',name='output_emg')(d_o2_emg)\n","\n","  d_o2_imu = Dense(100,activation='relu',name='d_o1_imu')(decoded2)\n","  d_o1_imu = Dense(200,activation='relu',name='d_o2_imu')(d_o2_imu)\n","  d_o1_imu = Dense(300,activation='relu',name='d_o3_imu')(d_o1_imu)\n","  d_o1_imu = Dense(500,activation='relu',name='d_o4_imu')(d_o1_imu)\n","  output_imu = Dense(200,activation='linear',name='output_imu')(d_o1_imu)\n","\n","  mmae = Model([input_emg, input_imu], [output_emg, output_imu], name='multi_modal_autoencoder_2')\n","\n","  mmae.compile(optimizer=Adam(lr=0.001),metrics='accuracy',loss='mse')\n","\n","  history = mmae.fit([X_train_emg,X_train_imu], [X_train_emg,X_train_imu], epochs=100, batch_size=50,\n","                                   validation_data=([X_test_emg,X_test_imu],[X_test_emg,X_test_imu]))\n","  \n","  results[\"model2\"]={\"output_imu_accuracy\":max(history.history['output_imu_accuracy']),\n","                     \"val_output_imu_accuracy\":max(history.history['val_output_imu_accuracy']),\n","                     \"output_emg_accuracy\":max(history.history['output_emg_accuracy']),\n","                     \"val_output_emg_accuracy\":max(history.history['val_output_emg_accuracy'])}\n","  \n","  funct_plots(history)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model2()\n","#### Check the png res21 and res22 for the results."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results\n","# {'model1': {'output_emg_accuracy': 0.5457287430763245,\n","#   'output_imu_accuracy': 0.31895512342453003,\n","#   'val_output_emg_accuracy': 0.5181587934494019,\n","#   'val_output_imu_accuracy': 0.3282657563686371},\n","#  'model2': {'output_emg_accuracy': 0.33518341183662415,\n","#   'output_imu_accuracy': 0.24499276280403137,\n","#   'val_output_emg_accuracy': 0.30757319927215576,\n","#   'val_output_imu_accuracy': 0.27561935782432556}}"]},{"cell_type":"markdown","metadata":{},"source":["# Attempt 2 for Stacked Model using Stratified Cross Validation:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def funct_avg(d):\n","    temp={}\n","    for i,j in d.items():\n","        temp[i]=np.mean(j)\n","        \n","    return temp"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Model:\n","    def __init__(self,number,l1,l2,l3):\n","        self.number=number\n","        self.model = tf.keras.Sequential()\n","        self.model.add(tf.keras.layers.InputLayer(input_shape=(400)))\n","        #model.add(tf.keras.layers.Normalization(axis=-1))\n","        self.model.add(tf.keras.layers.Dense(units=l1,activation=None))   ###\n","        #self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ReLU())\n","        self.model.add(tf.keras.layers.Dropout(0.2))\n","        self.model.add(tf.keras.layers.Dense(units=l2,activation=None))  ###\n","        #self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ReLU())\n","        self.model.add(tf.keras.layers.Dropout(0.2))\n","        self.model.add(tf.keras.layers.Dense(units=l3,activation=None))  ###\n","        #self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ReLU())\n","        self.model.add(tf.keras.layers.Dropout(0.2))\n","#         self.model.add(tf.keras.layers.Dense(units=l4,activation=None))\n","#         self.model.add(tf.keras.layers.BatchNormalization())\n","#         self.model.add(tf.keras.layers.ReLU())\n","#         self.model.add(tf.keras.layers.Dropout(0.5))\n","#         self.model.add(tf.keras.layers.Dense(units=l5,activation=None))\n","#         self.model.add(tf.keras.layers.BatchNormalization())\n","#         self.model.add(tf.keras.layers.ReLU())\n","        self.model.add(tf.keras.layers.Dropout(0.3))\n","        self.model.add(tf.keras.layers.Dense(units=26,activation=\"softmax\"))\n","        self.model.compile(optimizer=\"adam\",loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=\"accuracy\")\n","    \n","    def funct_fit(self,X_train,X_test,y_train,y_test,count,n_split):\n","        self.model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=50,batch_size=50)\n","        if count==n_split:\n","            self.model.save(f\"{os.getcwd()}/model_{self.number}.h5\")\n","        return self.model.history.history[\"accuracy\"],self.model.history.history[\"val_accuracy\"]          "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### cross validation:\n","def funct_cv(m,n_split,X,y,l1,l2,l3):\n","    dic_results={\"accuracy\":[],\"val_accuracy\":[]}\n","    n_split=n_split\n","    count=0\n","    for train_index,test_index in StratifiedKFold(n_split).split(X,y):\n","        X_train,X_test=X[train_index],X[test_index]\n","        y_train,y_test=y[train_index],y[test_index]\n","\n","        model=Model(m,l1,l2,l3)  ### To create the model object\n","        count=count+1   ### For saving the model\n","        acc,val_acc=model.funct_fit(X_train,X_test, y_train,y_test,count,n_split)        \n","\n","        dic_results[\"accuracy\"].append(max(acc))\n","        dic_results[\"val_accuracy\"].append(max(val_acc))\n","\n","    dic_results=funct_avg(dic_results)\n","\n","    final_result[m]=dic_results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from sklearn import datasets\n","# iris = datasets.load_iris()\n","# X = iris.data[:, :2]  # we only take the first two features.\n","# y = iris.target\n","final_result={}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"b3eff57f1b29dfdd788faddb90ebd9222db114d636e87b8fde724656933d5975"}}},"nbformat":4,"nbformat_minor":4}
