{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"##Basics\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport regex as re\nimport os\nimport string\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n##TSFresh\nimport tsfresh\nfrom tsfresh import extract_features\nfrom tsfresh.utilities.dataframe_functions import impute\n\n##ML scikit learn classes for data preprocessing:\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelEncoder\n\n##ML scikit learn classes for feature selection:\nfrom sklearn.feature_selection import chi2,mutual_info_classif  ### for chi2 and mutual info\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif  ### for ANOVA\nfrom sklearn.feature_selection import RFE  ### for RFE\n\n##ML scikit learn classes for model selection:  \nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n##ML scikit learn classes for evaluating model:\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report,accuracy_score,make_scorer,confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import learning_curve\n\n##ML scikit learn classes for creating Pipeline:\nfrom sklearn.pipeline import Pipeline\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:18:02.249578Z","iopub.execute_input":"2022-04-21T17:18:02.249943Z","iopub.status.idle":"2022-04-21T17:18:06.897422Z","shell.execute_reply.started":"2022-04-21T17:18:02.249858Z","shell.execute_reply":"2022-04-21T17:18:06.896125Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing:\n\n### Used Dataset:imu-dataset-original","metadata":{}},{"cell_type":"code","source":"## Iterate through the list of alphabets\n## Create a function that takes in letter for file path, i.e \"K\" \n## Select each file using file path\n## Store the csv file name with emg as emg variable(type:Dataframe), file name with other names as nemg(type:Dataframe)\n## Concatenate it with previous variables and return final eng,nemg\n## Pass it into the same function with different letter","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir=\"../input/imu-dataset-original/IMU dataset\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def funct_preprocessing(l,f,fn):\n    \n    emg=pd.DataFrame(columns=['timestamp', 'emg1', 'emg2', 'emg3', 'emg4', 'emg5', 'emg6', 'emg7',\n       'emg8'])\n    acc=pd.DataFrame()\n    gyro=pd.DataFrame()\n    \n    for i in sorted(os.listdir(os.path.join(dir,l))):\n        \n        if \"emg\" in i:\n            emg=pd.concat([emg,pd.read_csv(os.path.join(dir,l,i))],axis=0)\n            \n        elif \"acc\" in i or \"gyro\" in i:  \n        #### accelerometer,gyro,orientation have same feature names. Hence changing the column names\n            if \"acc\" in i:\n                temp=pd.read_csv(os.path.join(dir,l,i))\n                acc=pd.concat([acc,temp])\n\n            elif \"gyro\" in i:\n                temp=pd.read_csv(os.path.join(dir,l,i))\n                gyro=pd.concat([gyro,temp])\n\n    acc.columns=[\"timestamp\",\"x_acc\",\"y_acc\",\"z_acc\"]\n    acc.drop(\"timestamp\",axis=1,inplace=True)\n    gyro.columns=[\"timestamp\",\"x_gyro\",\"y_gyro\",\"z_gyro\"]\n        \n    nemg=pd.concat([acc,gyro],axis=1)\n    \n    emg[\"label\"]=l\n    nemg[\"label\"]=l\n    \n    f=pd.concat([f,emg],axis=0)\n    fn=pd.concat([fn,nemg],axis=0)\n    \n    return f,fn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f=pd.DataFrame()\nfn=pd.DataFrame()\nfor i in list(string.ascii_uppercase):\n    f,fn=funct_preprocessing(i,f,fn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f.to_csv(\"IMU_EMG.csv\",index=False)\nfn.to_csv(\"IMU_NonEMG_acc_and_gyro_only.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TSFresh with 50% Overlap:\n\n### for EMG Dataset: WS=200\n### for Non-EMG Dataset: WS=55\n\n### Used Dataset:imu-dataset-original","metadata":{}},{"cell_type":"code","source":"## Function to include time and id columns in the data:\ndef id_time(df,step):\n    \n    #Removing unwanted columns.\n    df.drop(columns=[i for i in df.columns if \"timestamp\" in i],inplace=True)\n        \n    #Sliding Window with 50% overlap:\n    w,t=step,(step//2)\n    r = np.arange(len(df))  # creating an array\n    s = r[::t]              #selecting elements with a step of t, i.e. half of SL\n    z = list(zip(s, s + w)) #Creating a list of tuples, each tuple holding the starting and ending row numbers.\n    g = lambda t: df.iloc[t[0]:t[1]]\n    j=pd.concat(map(g, z))\n    \n    j[\"id\"]=0\n    j[\"time\"]=0 \n    \n    l=np.arange(0,len(j),step=step)\n    l=np.append(l,len(j))\n    \n    j.reset_index(drop=True,inplace=True)\n    \n    #Sliding Window:    \n    for i in range(len(l)-1):\n        time = np.arange(len(j[l[i]:l[i+1]]))\n        t_id=np.full(len(time),i)\n        j.iloc[l[i]:l[i+1],list(j.columns).index(\"time\")]=time\n        j.iloc[l[i]:l[i+1],list(j.columns).index(\"id\")]=t_id\n                \n    return j\n\n## Function to extract time series features from the data:\ndef funct_tsfresh(df):\n    #Extracting Features\n    df_extracted = extract_features(df, column_id=\"id\", column_sort=\"time\")\n    \n    #Imputing the NaNs\n    df_extracted=impute(df_extracted)\n    \n    return df_extracted","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Function to complete the preprocessing (combining files) and feature extraction process\ndef funct_preprocessing(l,f,fn):\n    \n    emg=pd.DataFrame(columns=['timestamp', 'emg1', 'emg2', 'emg3', 'emg4', 'emg5', 'emg6', 'emg7',\n       'emg8'])\n    acc=pd.DataFrame()\n    gyro=pd.DataFrame()\n    euler=pd.DataFrame()\n    ori=pd.DataFrame()\n    \n    for i in sorted(os.listdir(os.path.join(dir,l))):\n        \n        if \"emg\" in i:\n            emg=pd.concat([emg,pd.read_csv(os.path.join(dir,l,i))],axis=0)\n            \n        elif \"acc\" in i or \"gyro\" in i or \"orientation\" in i:  \n        #### accelerometer,gyro,orientation have same feature names. Hence changing the column names\n            if \"acc\" in i:\n                temp=pd.read_csv(os.path.join(dir,l,i))\n                acc=pd.concat([acc,temp])\n\n            elif \"gyro\" in i:\n                temp=pd.read_csv(os.path.join(dir,l,i))\n                gyro=pd.concat([gyro,temp])\n                \n            elif \"Euler\" in i:\n                temp=pd.read_csv(os.path.join(dir,l,i))\n                euler=pd.concat([euler,temp])\n                \n            else:\n                temp=pd.read_csv(os.path.join(dir,l,i))\n                ori=pd.concat([ori,temp])\n\n    acc.columns=[\"timestamp\",\"x_acc\",\"y_acc\",\"z_acc\"]\n    acc.drop(\"timestamp\",axis=1,inplace=True)\n    gyro.columns=[\"timestamp\",\"x_gyro\",\"y_gyro\",\"z_gyro\"]\n    gyro.drop(\"timestamp\",axis=1,inplace=True)\n    ori.columns=[\"timestamp\",\"x_ori\",\"y_ori\",\"z_ori\",\"w\"]\n    ori.drop(\"timestamp\",axis=1,inplace=True)\n    \n    nemg=pd.concat([acc,gyro,ori,euler],axis=1)\n    \n    emg.reset_index(drop=True,inplace=True)\n    nemg.reset_index(drop=True,inplace=True)\n    \n    type_emg={\"emg1\":float,\n          \"emg2\":float,\n          \"emg3\":float,\n          \"emg4\":float,\n          \"emg5\":float,\n          \"emg6\":float,\n          \"emg7\":float,\n          \"emg8\":float\n    }\n    emg=emg.astype(type_emg)\n    \n###Now comes the TSFresh Part(Unsupervised)\n\n    emg=id_time(emg,200)\n    emg=funct_tsfresh(emg)\n    nemg=id_time(nemg,55)\n    nemg=funct_tsfresh(nemg)    \n    \n    emg[\"label\"]=l\n    nemg[\"label\"]=l\n    \n    f=pd.concat([f,emg],axis=0)\n    fn=pd.concat([fn,nemg],axis=0)\n    \n    return f,fn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To cross-validate the code:\n#fn.groupby([\"id\"]).agg(\"count\")  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To save time:\nstart=X   #### X is the custom value, like 18-21\nend=X+3\nf=pd.DataFrame()\nfn=pd.DataFrame()\nfor i in list(string.ascii_uppercase)[start:end]:\n    f,fn=funct_preprocessing(i,f,fn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f.to_csv(f\"IMU_EMG_AfterTSFresh_{start}-{end}.csv\",index=False)\nfn.to_csv(f\"IMU_NonEMG_AfterTSFresh_{start}-{end}.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection:","metadata":{}},{"cell_type":"markdown","source":"There are various Feature selection methods that can be used:\n1. ANOVA - FILTER Method of FS\n2. CHI2 - FILTER Method of FS\n3. RFE - WRAPPER Method of FS\n4. Mutual Info - FILTER Method of FS\n\nThere are other methods of FS like:\n1. Feature Importnace of individual models - EMBEDDED Method of FS\n2. Pearson's Correlation\n3. Spearman's Rank Correlation\n4. PCA - FILTER Method of FS","metadata":{}},{"cell_type":"markdown","source":"## For Combining individual zip files:\n## Used Dataset:after-tsfresh-imu","metadata":{}},{"cell_type":"code","source":"## Combining all the files:\ndir=\"../input/after-tsfresh-imu\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emg=pd.DataFrame()\nfor i in os.listdir(dir):\n    for j in os.listdir(os.path.join(dir,i)):\n        if \"_EMG_\" in j:\n            emg=pd.concat([emg,pd.read_csv(os.path.join(dir,i,j))],axis=0)\n            \nnemg=pd.DataFrame()\nfor i in os.listdir(dir):\n    for j in os.listdir(os.path.join(dir,i)):\n        if \"_NonEMG_\" in j:\n            nemg=pd.concat([nemg,pd.read_csv(os.path.join(dir,i,j))],axis=0)\n            ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emg.to_csv(\"Combined_EMG.csv\",index=False)\nnemg.to_csv(\"Combined_NEMG.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Created a dataset called imu-for-feature-selection","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"../input/imu-for-feature-selection/Combined_EMG.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:18:13.966869Z","iopub.execute_input":"2022-04-21T17:18:13.967370Z","iopub.status.idle":"2022-04-21T17:19:43.348749Z","shell.execute_reply.started":"2022-04-21T17:18:13.967339Z","shell.execute_reply":"2022-04-21T17:19:43.346844Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"imu=pd.read_csv(\"../input/imu-for-feature-selection/Combined_NEMG.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:20:42.888794Z","iopub.execute_input":"2022-04-21T17:20:42.889342Z","iopub.status.idle":"2022-04-21T17:23:02.521158Z","shell.execute_reply.started":"2022-04-21T17:20:42.889311Z","shell.execute_reply":"2022-04-21T17:23:02.519948Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"imu.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:23:02.523198Z","iopub.execute_input":"2022-04-21T17:23:02.523489Z","iopub.status.idle":"2022-04-21T17:23:02.530018Z","shell.execute_reply.started":"2022-04-21T17:23:02.523451Z","shell.execute_reply":"2022-04-21T17:23:02.529271Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:19:43.351649Z","iopub.execute_input":"2022-04-21T17:19:43.351955Z","iopub.status.idle":"2022-04-21T17:19:43.361230Z","shell.execute_reply.started":"2022-04-21T17:19:43.351918Z","shell.execute_reply":"2022-04-21T17:19:43.360046Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:19:43.363350Z","iopub.execute_input":"2022-04-21T17:19:43.363661Z","iopub.status.idle":"2022-04-21T17:19:43.423609Z","shell.execute_reply.started":"2022-04-21T17:19:43.363629Z","shell.execute_reply":"2022-04-21T17:19:43.422975Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def initial(df):\n    \n    #### Label Encoding the Target Variable\n\n    X=df.drop([\"label\"],axis=1)\n    y=df[\"label\"]\n    le=LabelEncoder()\n    y=le.fit_transform(y)\n    y= pd.Series(y)\n\n    #### Removing Features having zero variance.\n\n    Var=X[X.columns].std()\n    col=Var[Var==0].index\n    X=X.drop(col,axis=1)\n    \n    return X,y","metadata":{"execution":{"iopub.status.busy":"2022-04-20T06:15:58.098995Z","iopub.execute_input":"2022-04-20T06:15:58.099318Z","iopub.status.idle":"2022-04-20T06:15:58.106947Z","shell.execute_reply.started":"2022-04-20T06:15:58.099282Z","shell.execute_reply":"2022-04-20T06:15:58.105976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_curve(train_sizes, train_scores, test_scores):\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    plt.figure(figsize=(10,10))\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    plt.gca().invert_yaxis()\n    \n    # box-like grid\n    plt.grid()\n    \n    # plot the std deviation as a transparent range at each training set size\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    \n    # plot the average training and test score lines at each training set size\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    \n    # sizes the window for readability and displays the plot\n    # shows error from 0 to 1.1\n    plt.legend(loc=\"best\")\n    plt.ylim(-.1,1.1)\n    plt.show()\n       \n\ndef classification_report_with_accuracy_score(y_true, y_pred):\n\n    print(classification_report(y_true, y_pred)) # print classification report\n    cm=confusion_matrix(y_true,y_pred)\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # For normalising the MAtrix for better visualisation.\n    plt.figure(figsize=(20,10))\n    plt.rc(\"font\",size=10)\n    sns.heatmap(cm,annot=True,fmt=\".2f\",cmap=\"viridis\")\n    plt.show()\n    return accuracy_score(y_true, y_pred) # return accuracy score\n\ndef fun_best(X,y):\n    \n    X.rename({\"emg6__value_count__value_-1\":\"emg6__value_count__value_2\"},axis=1,inplace=True)\n    #To remove JSON characters from column names because LGBM fails to execute\n    \n    X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n    \n    models={\"XGB\":XGBClassifier(),\"LGBM\":LGBMClassifier(),\"GradientBoost\":GradientBoostingClassifier(),\"LDA\":LinearDiscriminantAnalysis(),\"RandomForest\":RandomForestClassifier()}\n    mean_score=[]\n    \n    for i,j in models.items():\n        try:\n            \n            model=j\n            score_model=cross_val_score(model,X,y,cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),scoring=make_scorer(classification_report_with_accuracy_score))\n            mean_score.append(score_model.mean())\n            train_sizes, train_scores, test_scores = learning_curve(model, X, y, n_jobs=-1, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n            draw_curve(train_sizes, train_scores, test_scores)\n        \n        except:\n            \n            model=j\n            score_model=cross_val_score(model,X,y,cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),scoring=make_scorer(classification_report_with_accuracy_score))\n            mean_score.append(score_model.mean())\n            train_sizes, train_scores, test_scores = learning_curve(model, X, y, n_jobs=-1, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n            draw_curve(train_sizes, train_scores, test_scores)            \n        \n    result=dict(zip(models.keys(),mean_score))\n   \n    return result","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:15:38.464547Z","iopub.execute_input":"2022-04-20T16:15:38.464917Z","iopub.status.idle":"2022-04-20T16:15:38.500216Z","shell.execute_reply.started":"2022-04-20T16:15:38.46482Z","shell.execute_reply":"2022-04-20T16:15:38.499575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def funct_FS(ds,fs_name,n):\n    \n    df=pd.read_csv(f\"../input/imu-for-feature-selection/Combined_{ds}.csv\")\n    \n    print(\"Read the Dataset\")\n    \n    X,y=initial(df)\n\n    print(\"Passed initial function\")\n    \n    fs_dic={\"MI\":SelectKBest(mutual_info_classif, k=n),\"CHI2\":SelectKBest(chi2, k=n),\"ANOVA\":SelectKBest(f_classif, k=n),\"RFE\":RFE(estimator=CatBoostClassifier(), n_features_to_select=n)}\n    \n    ### for Chi2 feature selection, data points must be strictly positive.\n    \n    if fs_name==\"CHI2\":\n        pipe = Pipeline([('scaler', MinMaxScaler()),\n                 ('selector', fs_dic[fs_name])])\n    \n    else:\n        pipe = Pipeline([('scaler', StandardScaler()),\n                 ('selector', fs_dic[fs_name])])\n    \n    print(\"Pipeline set up\")\n    \n    pipe.fit(X, y)\n    # Get columns to keep and create new dataframe with those only\n    cols = pipe.named_steps['selector'].get_support(indices=True) ### Note the format\n    X_fs= X.iloc[:,cols]\n\n    df_fs=pd.concat([X_fs,y],axis=1)\n\n    df_fs.to_csv(f\"{ds}_{fs_name}_{n}_features.csv\",index=False)\n    \n    print(\"Done Feature Selection\")\n    \n    result=fun_best(X_fs,y)\n    print(f\"Top {n} features using {fs_name} technique:\")\n    print(result)    ","metadata":{"execution":{"iopub.status.busy":"2022-04-20T06:16:07.884632Z","iopub.execute_input":"2022-04-20T06:16:07.885335Z","iopub.status.idle":"2022-04-20T06:16:07.89501Z","shell.execute_reply.started":"2022-04-20T06:16:07.885286Z","shell.execute_reply":"2022-04-20T06:16:07.894396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To check the Corresponding Letter -> Number Encoding\ntarget=pd.DataFrame(np.hstack([y.values.reshape((-1,1)),y_label.reshape((-1,1))]),columns=[\"label\",\"label_encoded\"])\ntarget.drop_duplicates().sort_values(\"label\").reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### For Trial:\n### Params are (\"EMG\" or \"NEMG\"/ Selection Method Name / Number of features)\nfunct_FS(\"EMG\",\"CHI2\",200) ","metadata":{"execution":{"iopub.status.busy":"2022-04-20T06:16:12.074172Z","iopub.execute_input":"2022-04-20T06:16:12.074486Z"},"trusted":true},"execution_count":null,"outputs":[]}]}