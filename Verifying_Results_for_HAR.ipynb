{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-ravi18/HAR/blob/main/Verifying_Results_for_HAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZlLqID4WwE5",
        "outputId": "653f3ea9-4cca-4cb3-ba1d-cfb2c40e351a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.1.1-cp39-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.9/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from catboost) (1.22.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from catboost) (5.13.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from catboost) (1.4.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->catboost) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (1.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (8.4.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (4.39.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (5.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly->catboost) (8.2.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.15.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.1.1\n"
          ]
        }
      ],
      "source": [
        "##Basics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import regex as re\n",
        "import os\n",
        "import string\n",
        "import time\n",
        "import warnings\n",
        "import json\n",
        "from os import path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "##ML scikit learn classes for data preprocessing:\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "##ML scikit learn classes for model selection:  \n",
        "from xgboost import XGBClassifier\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "##ML scikit learn classes for evaluating model:\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report,accuracy_score,make_scorer,confusion_matrix,precision_recall_fscore_support,roc_curve\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "## Deep Learning Libraries:\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model,Model\n",
        "from tensorflow.keras.layers import concatenate,Input,Dense,ReLU,BatchNormalization,Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_1Nykx7ZNEf"
      },
      "outputs": [],
      "source": [
        "## Setting the seed to allow reproducibility\n",
        "np.random.seed(31415)\n",
        "tf.random.set_seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3PTcNx-ZWQ7"
      },
      "outputs": [],
      "source": [
        "### Label Encoding, Removing Zero variance Features and Scaling the test data::\n",
        "def initial(df):\n",
        "    \n",
        "    #### Label Encoding the Target Variable\n",
        "\n",
        "    X=df.drop([\"label\"],axis=1)\n",
        "    y=df[\"label\"]\n",
        "    if df.label.dtype==str:   ### Will apply label encoding if needed\n",
        "        le=LabelEncoder()\n",
        "        y=le.fit_transform(y)\n",
        "        y=pd.Series(y)\n",
        "\n",
        "    #### Removing Features having zero variance.\n",
        "\n",
        "    Var=X[X.columns].std()\n",
        "    col=Var[Var==0].index\n",
        "    X=X.drop(col,axis=1)\n",
        "    \n",
        "    return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho4sQ0K2ZNGa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "5503610f-99e0-45fc-f821-5bcdec856d0c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d2d374564a6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### For the best model using EMG and NEMG Features (EMG+NEMG Combined)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# df_emg=pd.read_csv(\"/content/drive/MyDrive/HAR Datasets/EMG_ANOVA_200_features.csv\").rename({'0':'label'},axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_nemg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/HAR Datasets/Combined Results/NEMG_ANOVA_200_features.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# temp=pd.DataFrame()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/HAR Datasets/Combined Results/NEMG_ANOVA_200_features.csv'"
          ]
        }
      ],
      "source": [
        "### For the best model using EMG and NEMG Features (EMG+NEMG Combined)\n",
        "# df_emg=pd.read_csv(\"/content/drive/MyDrive/HAR Datasets/EMG_ANOVA_200_features.csv\").rename({'0':'label'},axis=1)\n",
        "df_nemg=pd.read_csv(\"/content/drive/MyDrive/HAR Datasets/Combined Results/NEMG_ANOVA_200_features.csv\").rename({\"0\":\"label\"},axis=1)\n",
        "\n",
        "# temp=pd.DataFrame()\n",
        "# for i in range(26):\n",
        "#     n=df_nemg[df_nemg[\"label\"]==i].reset_index(drop=True)\n",
        "#     e=df_emg[df_emg[\"label\"]==i].reset_index(drop=True).drop([\"label\"],axis=1)[0:n.shape[0]]\n",
        "#     k=pd.concat([e,n],axis=1)\n",
        "#     temp=pd.concat([temp,k],axis=0)   \n",
        "\n",
        "X,y=initial(df_nemg) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AxN4K9OZNJA"
      },
      "outputs": [],
      "source": [
        "X.shape,y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1iOfJHWBQog"
      },
      "outputs": [],
      "source": [
        "l1=list(df_nemg.columns[:-1])\n",
        "#l2=list(df_nemg.columns[:-1])\n",
        "#l1.extend(l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JehIchMlCE0c"
      },
      "outputs": [],
      "source": [
        "len(l1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fBzvluLZNK0"
      },
      "outputs": [],
      "source": [
        "## Splitting the data:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.15, random_state=2)\n",
        "sc=StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_train=pd.DataFrame(X_train,columns=l1)\n",
        "X_test=sc.transform(X_test)\n",
        "X_test=pd.DataFrame(X_test,columns=l1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tats0D4DhbVU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBEkq6LytRLh"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self,number,l1,l2,l3,l4):\n",
        "        self.number=number\n",
        "        self.model = tf.keras.Sequential()\n",
        "        self.model.add(tf.keras.layers.InputLayer(input_shape=(200)))\n",
        "        self.model.add(tf.keras.layers.Normalization(axis=-1))\n",
        "        self.model.add(tf.keras.layers.Dense(units=l1,activation=None))   ###\n",
        "        self.model.add(tf.keras.layers.BatchNormalization())\n",
        "        self.model.add(tf.keras.layers.ReLU())\n",
        "        self.model.add(tf.keras.layers.Dropout(0.2))\n",
        "        self.model.add(tf.keras.layers.Dense(units=l2,activation=None))  ###\n",
        "        self.model.add(tf.keras.layers.BatchNormalization())\n",
        "        self.model.add(tf.keras.layers.ReLU())\n",
        "        self.model.add(tf.keras.layers.Dropout(0.2))\n",
        "        self.model.add(tf.keras.layers.Dense(units=l3,activation=None))  ###\n",
        "        self.model.add(tf.keras.layers.BatchNormalization())\n",
        "        self.model.add(tf.keras.layers.ReLU())\n",
        "        self.model.add(tf.keras.layers.Dropout(0.2))\n",
        "        self.model.add(tf.keras.layers.Dense(units=l4,activation=None))  ###\n",
        "        self.model.add(tf.keras.layers.BatchNormalization())\n",
        "        self.model.add(tf.keras.layers.ReLU())\n",
        "        self.model.add(tf.keras.layers.Dropout(0.2))\n",
        "        #self.model.add(tf.keras.layers.Dropout(0.3))\n",
        "        self.model.add(tf.keras.layers.Dense(units=26,activation=\"softmax\"))\n",
        "        self.model.compile(optimizer=\"adam\",loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=\"accuracy\")\n",
        "    \n",
        "    def funct_fit(self,path,X_train,X_test,y_train,y_test,count,n_split):\n",
        "        self.model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=50,batch_size=50)\n",
        "        if count==n_split:\n",
        "            self.model.save(f\"{path}/model_{self.number}_deep.h5\")\n",
        "        return self.model.history.history[\"accuracy\"],self.model.history.history[\"val_accuracy\"],self.model.history.history[\"loss\"],self.model.history.history[\"val_loss\"] \n",
        "\n",
        "def funct_avg(d):\n",
        "    temp={}\n",
        "    for i,j in d.items():\n",
        "        temp[i]=np.mean(j)\n",
        "        \n",
        "    return temp\n",
        "\n",
        "def lc(path,m,acc,val_acc,loss,val_loss):\n",
        "  if os.path.exists(f\"{path}/LC\")==False:\n",
        "    os.mkdir(f\"{path}/LC\")\n",
        "  fig,ax=plt.subplots(1,2,figsize=(15,7))\n",
        "  ax[0].plot(acc,c= 'b',label=\"train_accuracy\")\n",
        "  ax[0].plot(val_acc,c= 'r',label=\"val_accuracy\")\n",
        "  ax[0].set_xlabel(\"Number of Epochs\")\n",
        "  ax[0].set_ylabel(\"Accuracy\")\n",
        "  ax[0].legend()\n",
        "\n",
        "  ax[1].plot(loss,c= 'b',label=\"train_loss\")\n",
        "  ax[1].plot(val_loss,c= 'r',label=\"val_loss\")\n",
        "  ax[1].set_xlabel(\"Number of Epochs\")\n",
        "  ax[1].set_ylabel(\"Loss\")\n",
        "  ax[1].legend()\n",
        "\n",
        "  fig.savefig(f'{path}/LC/Learning Curve_{m}.png')\n",
        "\n",
        "### cross validation:\n",
        "def funct_cv(path,m,n_split,X,y,l1,l2,l3,l4,final_result):\n",
        "    dic_results={\"accuracy\":[],\"val_accuracy\":[]}\n",
        "    n_split=n_split\n",
        "    count=0\n",
        "    for index,(train_index,test_index) in enumerate(StratifiedKFold(n_split).split(X,y)):\n",
        "        X_train,X_test=X[train_index],X[test_index]\n",
        "        y_train,y_test=y[train_index],y[test_index]\n",
        "\n",
        "        model=Model(m,l1,l2,l3,l4)   ### creating the model object \n",
        "        count=count+1\n",
        "        acc,val_acc,loss,val_loss=model.funct_fit(path,X_train,X_test, y_train,y_test,count,n_split) \n",
        "        if index==4:\n",
        "          lc(path,m,acc,val_acc,loss,val_loss)\n",
        "\n",
        "        dic_results[\"accuracy\"].append(np.mean(acc))\n",
        "        dic_results[\"val_accuracy\"].append(np.mean(val_acc))\n",
        "\n",
        "    dic_results=funct_avg(dic_results)\n",
        "\n",
        "    final_result[m]=dic_results\n",
        "    return final_result\n",
        "\n",
        "# final_result={}\n",
        "# funct_cv(1,5,X_train.values,y_train.values,140,120,80,70)\n",
        "# funct_cv(2,5,X_train.values,y_train.values,180,160,100,90)\n",
        "# funct_cv(3,5,X_train.values,y_train.values,320,250,200,150)\n",
        "\n",
        "# with open('/content/drive/MyDrive/HAR Results/IMU/accuracy_results_StratifiedCV_deep.json', 'w') as fp:\n",
        "#     json.dump(final_result, fp,  indent=4) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9_4sBNkxFYq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sY4EJwukFL7"
      },
      "outputs": [],
      "source": [
        "allmodels=[]\n",
        "\n",
        "#### Loading other models:\n",
        "def funct_load(path,number):\n",
        "    for i in range(1,number+1):\n",
        "        # load model from file\n",
        "        model = load_model(f'{path}/model_{i}_deep.h5')\n",
        "        with open(f'{path}/base_model_{i}_summary.txt', 'w') as f:  ### Getting the output of the model configuration.\n",
        "          with redirect_stdout(f):\n",
        "            model.summary()\n",
        "        # add to list of members\n",
        "        allmodels.append(model)\n",
        "            \n",
        "#funct_load(3)    \n",
        "\n",
        "# create stacked model input dataset as outputs from the individual ensemble models\n",
        "def stacked_dataset(allmodels, X_test):\n",
        "    stackX = None\n",
        "    for model in allmodels:\n",
        "        # make prediction\n",
        "        yhat = model.predict(X_test, verbose=0)\n",
        "        # stack predictions into [rows, members, class]\n",
        "        if stackX is None:\n",
        "            stackX = yhat\n",
        "        else:\n",
        "            stackX = np.dstack((stackX, yhat))\n",
        "    # flatten predictions to [rows, members x class]\n",
        "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "    return stackX\n",
        "\n",
        "#stackedX = stacked_dataset(allmodels, X_test)\n",
        "\n",
        "# results={}\n",
        "\n",
        "# evaluate standalone models on test dataset\n",
        "for model in allmodels:\n",
        "    y_hat=model.predict(X_test)\n",
        "    y_hat=np.argmax(y_hat,axis=1)\n",
        "    acc = accuracy_score(y_test, y_hat)\n",
        "    print('Model Accuracy: %.6f' % acc)\n",
        "\n",
        "\n",
        "def funct_report_csv(y,y_hat):\n",
        "    clf_rep = precision_recall_fscore_support(y, y_hat)\n",
        "    out_dict = {\n",
        "                 \"precision\" :clf_rep[0].round(2)\n",
        "                ,\"recall\" : clf_rep[1].round(2)\n",
        "                ,\"f1-score\" : clf_rep[2].round(2)\n",
        "                ,\"support\" : clf_rep[3]\n",
        "                }\n",
        "    out_df = pd.DataFrame(out_dict)\n",
        "    avg_tot = (out_df.apply(lambda x: round(x.mean(), 2) if x.name!=\"support\" else  round(x.sum(), 2)).to_frame().T)\n",
        "    avg_tot.index = [\"avg/total\"]\n",
        "    out_df = out_df.append(avg_tot)\n",
        "    return out_df\n",
        "\n",
        "def classification_report_with_accuracy_score(y, y_hat,model_name,path):\n",
        "    if os.path.exists(f\"{path}/CR\")==False:\n",
        "      os.mkdir(f\"{path}/CR\")\n",
        "    if os.path.exists(f\"{path}/CM\")==False:\n",
        "      os.mkdir(f\"{path}/CM\")\n",
        "    #report=classification_report(y, y_hat,output_dict=True) # print classification report\n",
        "    report=funct_report_csv(y, y_hat) # print classification report\n",
        "    report.to_csv(f\"{path}/CR/Classification_Report_{model_name}.csv\",index=False)\n",
        "\n",
        "    cm=confusion_matrix(y,y_hat)\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.rc(\"font\",size=10)\n",
        "    sns.heatmap(cm,annot=True,fmt=\".2f\",cmap=\"viridis\")\n",
        "    plt.savefig(f\"{path}/CM/Confusion_Matrix_{model_name}_simple.png\")\n",
        "\n",
        "    cm_1 = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # For normalising the Matrix for better visualisation.\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.rc(\"font\",size=10)\n",
        "    sns.heatmap(cm_1,annot=True,fmt=\".2f\",cmap=\"viridis\")\n",
        "    plt.savefig(f\"{path}/CM/Confusion_Matrix_{model_name}_axis=1.png\")\n",
        "    \n",
        "    return accuracy_score(y, y_hat) # return accuracy score\n",
        "\n",
        "    \n",
        "def stacked_model_test(path,allmodels,X,y):\n",
        "    # dictionary of all models\n",
        "    sample={\"Naive Bayes\":GaussianNB(),\"XGB\":XGBClassifier(),\"LGBM\":LGBMClassifier(),\"RandomForest\":RandomForestClassifier(),\"LR\":LogisticRegression(),\"CatBoost\":CatBoostClassifier(),\"Naive Bayes\":GaussianNB(),\"SVC\":SVC(),\"KNN_3\":KNeighborsClassifier(n_neighbors=3),\"KNN_5\":KNeighborsClassifier(n_neighbors=5)}\n",
        "    # create dataset using ensemble\n",
        "    stackedX = stacked_dataset(allmodels, X)\n",
        "    \n",
        "    for i,j in sample.items():\n",
        "        arg={\"model_name\":i,\"path\":path}\n",
        "        sc=make_scorer(classification_report_with_accuracy_score,**arg)\n",
        "        score_=cross_val_score(j,stackedX,y,cv=5,scoring=sc)\n",
        "        acc = score_.mean()\n",
        "       \n",
        "        results[i]=round(acc,5)\n",
        "           \n",
        "\n",
        "# stacked_model_test(allmodels,X_test,y_test)\n",
        "# #### To store the results in the form of json that is prettified:\n",
        "# with open('/content/drive/MyDrive/HAR Results/IMU/accuracy_results_final_stacked_random-sampling.json', 'w') as fp:\n",
        "#     json.dump(results, fp,  indent=4)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4kFueOlYQi-"
      },
      "outputs": [],
      "source": [
        "## Function for ROC and Shap FI:\n",
        "\n",
        "def roc(path,X_train,X_test,y_train,y_test,model_name,model):\n",
        "  if os.path.exists(f\"{path}/ROC\")==False:\n",
        "    os.mkdir(f\"{path}/ROC\")\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.title(\"ROC Curve and AUC\", fontsize=18)\n",
        "  plt.xlabel(\"False Positive Rate\", fontsize=16)\n",
        "  plt.ylabel(\"True Positive Rate\", fontsize=16)\n",
        "  visualizer = RadViz(size=(700,700))\n",
        "  model = wrap(model)\n",
        "  visualizer = ROCAUC(model)\n",
        "\n",
        "  visualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
        "  visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
        "  visualizer.show(outpath=f\"{path}/ROC/ROC_{model_name}.png\")\n",
        "\n",
        "def stacked_model_test(path,allmodels,X,y):\n",
        "    # dictionary of all models\n",
        "    sample={\"Naive Bayes\":GaussianNB(),\"XGB\":XGBClassifier(),\"LGBM\":LGBMClassifier(),\"RandomForest\":RandomForestClassifier(),\"LR\":LogisticRegression(),\"CatBoost\":CatBoostClassifier(),\"Naive Bayes\":GaussianNB(),\"SVC\":SVC(),\"KNN_3\":KNeighborsClassifier(n_neighbors=3),\"KNN_5\":KNeighborsClassifier(n_neighbors=5)}\n",
        "    # create dataset using ensemble\n",
        "    stackedX = stacked_dataset(allmodels, X)\n",
        "    \n",
        "    X_train,X_test,y_train,y_test=train_test_split(stackedX,y,stratify=y)\n",
        "    for i,j in sample.items():      \n",
        "      arg={\"model_name\":i,\"model\":j}\n",
        "      try:\n",
        "        roc(path,X_train,X_test,y_train,y_test,**arg)  \n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        continue\n",
        "\n",
        "#stacked_model_test(allmodels,X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME-uxu-qYh80"
      },
      "outputs": [],
      "source": [
        "## For Shap Visualisation and Top Features\n",
        "\n",
        "def shap_fe(path,df,X,y,model_name,model):\n",
        "  # compute SHAP values\n",
        "  model.fit(X,y)\n",
        "\n",
        "  if os.path.exists(f\"{path}/Shap\")==False:\n",
        "    os.mkdir(f\"{path}/Shap\")\n",
        "\n",
        "  explainer = shap.TreeExplainer(model)\n",
        "  shap_values = explainer.shap_values(X)\n",
        "\n",
        "  # explainer = shap.Explainer(model)\n",
        "  # shap_values = explainer(X)\n",
        "\n",
        "  shap.summary_plot(shap_values, X, class_names= [i for i in range(26)], feature_names = df.columns,show=False)\n",
        "  plt.savefig(f\"{path}/Shap/Shap_Feature_Plot_{model_name}.png\")\n",
        "\n",
        "  vals= np.abs(shap_values[1]).mean(0)\n",
        "  df_feature_importance=pd.DataFrame(np.concatenate([np.array(df.columns).reshape(-1,1),vals.reshape(-1,1)],axis=1),columns=[\"Feature\",\"Shap_Scores\"])\n",
        "  df_feature_importance = df_feature_importance.sort_values('Shap_Scores',ascending=False)\n",
        "  df_feature_importance.reset_index(drop=True,inplace=True)\n",
        "  df_feature_importance.to_csv(f\"{path}/Shap/Feature_scores_{model_name}.csv\")\n",
        "\n",
        "# sample={\"XGB\":XGBClassifier()} #,\"LGBM\":LGBMClassifier(),\"RandomForest\":RandomForestClassifier(),\"LR\":LogisticRegression(),\"CatBoost\":CatBoostClassifier(),\"Naive Bayes\":GaussianNB(),\"SVC\":SVC(),\"KNN_3\":KNeighborsClassifier(n_neighbors=3),\"KNN_5\":KNeighborsClassifier(n_neighbors=5)}  \n",
        "# for i,j in sample.items():\n",
        "#   try:\n",
        "#     shap_fe(df_nemg.iloc[:,:-1],X_train,y_train,i,j)\n",
        "#   except Exception as e:\n",
        "#     print(e)\n",
        "#     continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMHK87fzodQ5"
      },
      "outputs": [],
      "source": [
        "def master_fn(data,path):\n",
        "  if os.path.exists(f\"{path}\")==False:\n",
        "    os.mkdir(f\"{path}\")\n",
        "  ## Reading the data::\n",
        "  if data==\"EMG\":\n",
        "    df=pd.read_csv(\"/content/drive/MyDrive/HAR Datasets/EMG_ANOVA_200_features.csv\").rename({'0':'label'},axis=1)\n",
        "  elif data==\"IMU\":\n",
        "    df=pd.read_csv(\"/content/drive/MyDrive/HAR Datasets/Combined Results/NEMG_ANOVA_200_features.csv\").rename({\"0\":\"label\"},axis=1)\n",
        "  elif data==\"Combined\":\n",
        "    df_emg=pd.read_csv(\"/content/drive/MyDrive/HAR Datasets/EMG_ANOVA_200_features.csv\").rename({'0':'label'},axis=1)\n",
        "    df_nemg=pd.read_csv(\"/content/drive/MyDrive/HAR Datasets/Combined Results/NEMG_ANOVA_200_features.csv\").rename({\"0\":\"label\"},axis=1)\n",
        "    df=pd.DataFrame()\n",
        "    for i in range(26):\n",
        "        n=df_nemg[df_nemg[\"label\"]==i].reset_index(drop=True)\n",
        "        e=df_emg[df_emg[\"label\"]==i].reset_index(drop=True).drop([\"label\"],axis=1)[0:n.shape[0]]\n",
        "        k=pd.concat([e,n],axis=1)\n",
        "        df=pd.concat([df,k],axis=0)   \n",
        "\n",
        "  ## Label Encoding->Removing Zero Variance Features->Scaling the test data::\n",
        "  X,y=initial(df)\n",
        "\n",
        "  ## Splitting the data:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.15, random_state=2)\n",
        "  sc=StandardScaler()\n",
        "  X_train=sc.fit_transform(X_train)\n",
        "  X_train=pd.DataFrame(X_train,columns=list(df.columns[:-1]))\n",
        "  X_test=sc.transform(X_test)\n",
        "  X_test=pd.DataFrame(X_test,columns=list(df.columns[:-1]))\n",
        "\n",
        "  # ## Creating and training the models:\n",
        "  # final_result={}\n",
        "  # final_result=funct_cv(path,1,5,X_train.values,y_train.values,140,120,80,70,final_result)\n",
        "  # final_result=funct_cv(path,2,5,X_train.values,y_train.values,180,160,100,90,final_result)\n",
        "  # final_result=funct_cv(path,3,5,X_train.values,y_train.values,320,250,200,150,final_result)\n",
        "\n",
        "  # with open(f'{path}/accuracy_results_StratifiedCV_deep.json', 'w') as fp:\n",
        "  #     json.dump(final_result, fp,  indent=4) \n",
        "\n",
        "  ## Loading the models:\n",
        "  allmodels=[]\n",
        "  funct_load(path,3)\n",
        "\n",
        "  ## \n",
        "  stacked_model_test(path,allmodels,X_test,y_test)\n",
        "  ## To store the results in the form of json that is prettified:\n",
        "  with open(f'{path}/accuracy_results_final_stacked_random-sampling.json', 'w') as fp:\n",
        "      json.dump(results, fp,  indent=4)\n",
        "\n",
        "  ## ROC and Shap:\n",
        "  stacked_model_test(path,allmodels,X_test,y_test)    \n",
        "\n",
        "  sample={\"XGB\":XGBClassifier()} #,\"LGBM\":LGBMClassifier(),\"RandomForest\":RandomForestClassifier(),\"LR\":LogisticRegression(),\"CatBoost\":CatBoostClassifier(),\"Naive Bayes\":GaussianNB(),\"SVC\":SVC(),\"KNN_3\":KNeighborsClassifier(n_neighbors=3),\"KNN_5\":KNeighborsClassifier(n_neighbors=5)}  \n",
        "  for i,j in sample.items():\n",
        "    try:\n",
        "      shap_fe(path,df_nemg.iloc[:,:-1],X_train,y_train,i,j)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "KR4vqOVHQ0Mt",
        "outputId": "2378a05e-4463-4702-ce16-28299eaef92d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e29cee270abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmaster_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EMG\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/sample_emg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-56801f74af21>\u001b[0m in \u001b[0;36mmaster_fn\u001b[0;34m(data, path)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   \u001b[0mstacked_model_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m   \u001b[0;31m## To store the results in the form of json that is prettified:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/accuracy_results_final_stacked_random-sampling.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-fc3cdd8907c4>\u001b[0m in \u001b[0;36mstacked_model_test\u001b[0;34m(path, allmodels, X, y)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Naive Bayes\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"XGB\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"LGBM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"RandomForest\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"LR\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"CatBoost\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Naive Bayes\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SVC\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"KNN_3\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"KNN_5\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# create dataset using ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mstackedX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstackedX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-7d6565923f11>\u001b[0m in \u001b[0;36mstacked_dataset\u001b[0;34m(allmodels, X_test)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mstackX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstackX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# flatten predictions to [rows, members x class]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mstackX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstackX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstackX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstackX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstackX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "master_fn(\"EMG\",\"/content/drive/MyDrive/sample_emg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DHFkSpdSx2-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}